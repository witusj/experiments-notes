<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Understanding Kernels in Bayesian Optimization: A Step-by-Step Guide</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="bayesian_optimization_files/libs/clipboard/clipboard.min.js"></script>
<script src="bayesian_optimization_files/libs/quarto-html/quarto.js"></script>
<script src="bayesian_optimization_files/libs/quarto-html/popper.min.js"></script>
<script src="bayesian_optimization_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="bayesian_optimization_files/libs/quarto-html/anchor.min.js"></script>
<link href="bayesian_optimization_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="bayesian_optimization_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="bayesian_optimization_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="bayesian_optimization_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="bayesian_optimization_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Understanding Kernels in Bayesian Optimization: A Step-by-Step Guide</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="1d6663f0" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process <span class="im">import</span> GaussianProcessRegressor</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process.kernels <span class="im">import</span> RBF, WhiteKernel</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Define the "Unknown" Black-Box Function</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># (This is just an example function that looks somewhat like the one in the image)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> black_box_function(x):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The true function we want to optimize but don't know."""</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combining sines for complexity</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (np.sin(x <span class="op">/</span> <span class="fl">1.5</span>) <span class="op">*</span> <span class="dv">8</span> <span class="op">+</span> np.cos(x <span class="op">/</span> <span class="fl">3.5</span>) <span class="op">*</span> <span class="dv">5</span> <span class="op">+</span> np.sin(x <span class="op">/</span> <span class="dv">10</span>) <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">10</span> <span class="op">+</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Adding a sharp dip like in the image</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span><span class="dv">15</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> <span class="dv">22</span>) <span class="op">/</span> <span class="fl">1.5</span>)<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the domain for plotting</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">50</span>, <span class="dv">500</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> black_box_function(X_plot)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Iteration 1 Data ---</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Based on image: 6 previous points + 1 new point at x=6.0</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's pick some plausible points visually for the initial 6 green stars</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>X_train_iter1 <span class="op">=</span> np.array([<span class="fl">7.5</span>, <span class="fl">13.0</span>, <span class="fl">18.0</span>, <span class="fl">21.5</span>, <span class="fl">28.0</span>, <span class="fl">35.0</span>, <span class="fl">6.0</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>Y_train_iter1 <span class="op">=</span> black_box_function(X_train_iter1).ravel()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Iteration 2 Data ---</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Based on image: Adds a new point at x=50.0 to the previous 7 points</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>X_train_iter2 <span class="op">=</span> np.vstack((X_train_iter1, np.array([[<span class="fl">50.0</span>]])))</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>Y_train_iter2 <span class="op">=</span> black_box_function(X_train_iter2).ravel()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Setup Gaussian Process Regressor</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Kernel definition: RBF for smoothness + WhiteKernel for noise/tolerance</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> RBF(length_scale<span class="op">=</span><span class="fl">5.0</span>, length_scale_bounds<span class="op">=</span>(<span class="fl">1e-1</span>, <span class="fl">10.0</span>)) <span class="op">\</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>         <span class="op">+</span> WhiteKernel(noise_level<span class="op">=</span><span class="fl">0.5</span>, noise_level_bounds<span class="op">=</span>(<span class="fl">1e-5</span>, <span class="fl">1e+1</span>))</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kernel,</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>                              alpha<span class="op">=</span><span class="fl">0.0</span>, <span class="co"># Assuming no observation noise is added externally</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>                              n_restarts_optimizer<span class="op">=</span><span class="dv">10</span>, <span class="co"># Helps find better kernel parameters</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>                              normalize_y<span class="op">=</span><span class="va">True</span>) <span class="co"># Normalizing target values often helps</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to create the plot</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_gp_iteration(ax, iteration, X_train, Y_train, gp_model, X_plot, y_true):</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Plots a single iteration of the Bayesian Optimization process."""</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Separate old points (stars) and the latest point (dot)</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    X_old <span class="op">=</span> X_train[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    Y_old <span class="op">=</span> Y_train[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    X_new <span class="op">=</span> X_train[<span class="op">-</span><span class="dv">1</span>:]</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    Y_new <span class="op">=</span> Y_train[<span class="op">-</span><span class="dv">1</span>:]</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit the GP model to the current training data</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    gp_model.fit(X_train, Y_train)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predict using the fitted GP model</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    y_pred, std_dev <span class="op">=</span> gp_model.predict(X_plot, return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the true function</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_plot, y_true, <span class="st">'k-'</span>, label<span class="op">=</span><span class="st">'black-box function $f(x)$'</span>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the GP prediction (mean)</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_plot, y_pred, <span class="st">'r--'</span>, label<span class="op">=</span><span class="st">'approximated function $</span><span class="ch">\\</span><span class="st">hat</span><span class="sc">{f}</span><span class="st">(x)$'</span>)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the confidence interval (optional, but common in BO plots)</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ax.fill_between(X_plot.ravel(), y_pred - 1.96 * std_dev, y_pred + 1.96 * std_dev,</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    <span class="co">#                 alpha=0.2, color='red', label='95% confidence interval')</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot previously sampled points</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_old, Y_old, <span class="st">'g*'</span>, markersize<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'sampled points'</span>)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the newly sampled point</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_new, Y_new, <span class="st">'bo'</span>, markersize<span class="op">=</span><span class="dv">8</span>, label<span class="op">=</span><span class="st">'newly sampled point'</span>)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Formatting</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"x"</span>)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"f(x)"</span>)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"Iteration </span><span class="sc">{</span>iteration<span class="sc">}</span><span class="ch">\n</span><span class="ss">Sampled Points: #</span><span class="sc">{</span><span class="bu">len</span>(X_train)<span class="sc">}</span><span class="ss">, Newly Sampled Point: </span><span class="sc">{</span>X_new[<span class="dv">0</span>,<span class="dv">0</span>]<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="bu">min</span>(y_true)<span class="op">-</span><span class="dv">1</span>, <span class="bu">max</span>(y_true)<span class="op">+</span><span class="dv">1</span>) <span class="co"># Adjust Y limits</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(X_plot.<span class="bu">min</span>(), X_plot.<span class="bu">max</span>()) <span class="co"># Adjust X limits</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iteration <span class="op">==</span> <span class="dv">1</span>: <span class="co"># Only add legend to the first plot for clarity</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>        ax.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Create the plots for both iterations</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">6</span>)) <span class="co"># Create a figure with two subplots</span></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Iteration 1</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>plot_gp_iteration(axes[<span class="dv">0</span>], <span class="dv">1</span>, X_train_iter1, Y_train_iter1, gp, X_plot, y_true)</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Iteration 2</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Important: Use a *new* GP instance or ensure the state isn't carried over if not desired.</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Here, the same 'gp' instance is refitted, which is standard practice.</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>plot_gp_iteration(axes[<span class="dv">1</span>], <span class="dv">2</span>, X_train_iter2, Y_train_iter2, gp, X_plot, y_true)</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>plt.tight_layout() <span class="co"># Adjust subplot parameters for a tight layout</span></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning:

The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:445: ConvergenceWarning:

The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bayesian_optimization_files/figure-html/cell-2-output-2.png" width="1334" height="567" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In Bayesian Optimization, we use a <strong>Gaussian Process (GP)</strong> to create a probabilistic model (the “surrogate model” or “approximation,” like the red dotted line in our example) of the unknown black-box function (the black line). The <strong>kernel</strong> is the core component that defines the assumptions this model makes about the function.</p>
<p><strong>Step 1: The Goal - Modeling the Unknown</strong></p>
<p>Before diving into kernels, let’s remember <em>why</em> we need the GP surrogate model. We have a few samples <span class="math inline">\((x, y)\)</span> from our expensive-to-evaluate black-box function <span class="math inline">\(f(x)\)</span>. We want to:</p>
<ol type="1">
<li><strong>Approximate</strong> <span class="math inline">\(f(x)\)</span> over the entire input space, even where we haven’t sampled.</li>
<li>Quantify our <strong>uncertainty</strong> about this approximation. (How confident are we about the red line’s height at any given <span class="math inline">\(x\)</span>?)</li>
</ol>
<p>The GP achieves this by assuming that function values at “nearby” input points are related or correlated. The kernel tells us exactly <em>how</em> related they are.</p>
<p><strong>Step 2: The Kernel - A Measure of Similarity</strong></p>
<p>Think of the kernel, often written as <span class="math inline">\(k(x_1, x_2)\)</span>, as a function that measures the <strong>similarity</strong> or <strong>covariance</strong> between the function’s output values at two different input points, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</p>
<ul>
<li><strong>Input:</strong> Two points from the input domain (e.g., <span class="math inline">\(x_1 = 10\)</span>, <span class="math inline">\(x_2 = 12\)</span>).</li>
<li><strong>Output:</strong> A single number representing the expected relationship between <span class="math inline">\(f(x_1)\)</span> and <span class="math inline">\(f(x_2)\)</span>.
<ul>
<li>A <strong>high</strong> kernel value means <span class="math inline">\(f(x_1)\)</span> and <span class="math inline">\(f(x_2)\)</span> are expected to be <strong>strongly related</strong> (highly correlated). If we know <span class="math inline">\(f(x_1)\)</span>, we have a good idea about <span class="math inline">\(f(x_2)\)</span>. This usually happens when <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are “close”.</li>
<li>A <strong>low</strong> kernel value (close to zero) means <span class="math inline">\(f(x_1)\)</span> and <span class="math inline">\(f(x_2)\)</span> are expected to be <strong>weakly related</strong> (uncorrelated). Knowing <span class="math inline">\(f(x_1)\)</span> tells us little about <span class="math inline">\(f(x_2)\)</span>. This usually happens when <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are “far apart”.</li>
</ul></li>
</ul>
<p>The fundamental assumption encoded by the kernel is: <strong>The closer two input points <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are (according to the kernel’s definition of closeness), the more similar their output values <span class="math inline">\(f(x_1)\)</span> and <span class="math inline">\(f(x_2)\)</span> should be.</strong></p>
<p><strong>Step 3: How Kernels Work - The RBF Example</strong></p>
<p>Different mathematical functions can serve as kernels, each encoding different assumptions about the underlying function (e.g., smoothness, periodicity). The most widely used kernel is the <strong>Radial Basis Function (RBF)</strong> kernel, also known as the Squared Exponential or Gaussian kernel.</p>
<p>Its formula is: <span class="math inline">\(k_{RBF}(x_1, x_2) = \sigma^2 \exp\left( - \frac{\|x_1 - x_2\|^2}{2l^2} \right)\)</span></p>
<p>Let’s dissect this:</p>
<ul>
<li><span class="math inline">\(\|x_1 - x_2\|^2\)</span>: This is the squared <strong>Euclidean distance</strong> between the input points <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. For our 1D example, it’s simply <span class="math inline">\((x_1 - x_2)^2\)</span>. If <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> were 2D points <span class="math inline">\((x_{1a}, x_{1b})\)</span> and <span class="math inline">\((x_{2a}, x_{2b})\)</span>, this would be <span class="math inline">\((x_{1a}-x_{2a})^2 + (x_{1b}-x_{2b})^2\)</span>.</li>
<li><span class="math inline">\(l\)</span>: This is the <strong>length scale</strong>. It’s a crucial <strong>hyperparameter</strong> that controls how quickly the correlation decays as the distance between points increases.
<ul>
<li>A <strong>small</strong> length scale (<span class="math inline">\(l\)</span>) means the correlation drops off very quickly. Only <em>very</em> close points are considered related. This implies the function is expected to change rapidly or be “wiggly”.</li>
<li>A <strong>large</strong> length scale (<span class="math inline">\(l\)</span>) means the correlation drops off slowly. Points relatively far apart still influence each other. This implies the function is expected to be very “smooth” and change slowly.</li>
</ul></li>
<li><span class="math inline">\(\sigma^2\)</span>: This is the <strong>output variance</strong> (or signal variance). It’s another hyperparameter that controls the average expected deviation of the function values from their mean. It essentially scales the overall amplitude or vertical variation the model expects.</li>
<li><span class="math inline">\(\exp(\dots)\)</span>: The exponential function ensures the similarity is highest (equal to <span class="math inline">\(\sigma^2\)</span>) when <span class="math inline">\(x_1 = x_2\)</span> (distance is 0) and decays smoothly towards 0 as the distance grows.</li>
</ul>
<p><em>Python Code: Visualizing the RBF Kernel’s Similarity Decay</em></p>
<p>The code below shows how the RBF kernel value (similarity) changes as one point (<span class="math inline">\(x_2\)</span>) moves away from a fixed point (<span class="math inline">\(x_1=0\)</span>) for different length scales.</p>
<div id="92838ed6" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rbf_kernel(x1, x2, length_scale, variance):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">  Calculates the Radial Basis Function (RBF) kernel value between two points.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    x1: First input point (scalar or numpy array).</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    x2: Second input point (scalar or numpy array).</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">    length_scale: The length scale parameter (l). Controls smoothness.</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">    variance: The variance parameter (sigma^2). Controls amplitude.</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">  Returns:</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co">    The kernel value (similarity).</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate squared Euclidean distance</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># np.sum works for both 1D and multi-dimensional inputs</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  distance_sq <span class="op">=</span> np.<span class="bu">sum</span>((np.asarray(x1) <span class="op">-</span> np.asarray(x2))<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate RBF kernel value</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> variance <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> distance_sq <span class="op">/</span> length_scale<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Simulation Setup ---</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fix one point, e.g., x1 = 0</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>x1_fixed <span class="op">=</span> np.array([<span class="fl">0.0</span>])</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a range of other points (x2) to compare against x1</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>x2_values <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">200</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># Use reshape for consistency</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define kernel parameters</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>variance <span class="op">=</span> <span class="fl">1.0</span> <span class="co"># Sigma^2 (often fixed at 1 initially, can be optimized later)</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Define different length scales to compare</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>length_scales <span class="op">=</span> {</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Short (l=0.5)"</span>: <span class="fl">0.5</span>,</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Medium (l=1.0)"</span>: <span class="fl">1.0</span>,</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Long (l=3.0)"</span>: <span class="fl">3.0</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Calculate Kernel Values ---</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, l <span class="kw">in</span> length_scales.items():</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate similarity of each x2 with the fixed x1</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>  similarities <span class="op">=</span> [rbf_kernel(x1_fixed, x2, length_scale<span class="op">=</span>l, variance<span class="op">=</span>variance) <span class="cf">for</span> x2 <span class="kw">in</span> x2_values]</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>  plt.plot(x2_values, similarities, label<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Plotting ---</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'RBF Kernel Value (Similarity) vs. Input Point $x_2$ (relative to $x_1=0$)'</span>)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x_2$'</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="ss">f'$k(x_1=0, x_2)$ with $\sigma^2=</span><span class="sc">{</span>variance<span class="sc">}</span><span class="ss">$'</span>)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>plt.axvline(x1_fixed[<span class="dv">0</span>], color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.0</span>, label<span class="op">=</span><span class="ss">f'$x_1=0$'</span>) <span class="co"># Mark x1</span></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.05</span>, <span class="fl">1.05</span>) <span class="co"># Kernel value is between 0 and variance (here 1)</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>&lt;&gt;:50: SyntaxWarning:

invalid escape sequence '\s'

&lt;&gt;:50: SyntaxWarning:

invalid escape sequence '\s'

/var/folders/gf/gtt1mww524x0q33rqlwsmjw80000gn/T/ipykernel_84668/855785160.py:50: SyntaxWarning:

invalid escape sequence '\s'
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bayesian_optimization_files/figure-html/cell-3-output-2.png" width="816" height="524" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Step 4: Building the Model - The Covariance Matrix</strong></p>
<p>The GP uses the chosen kernel function <span class="math inline">\(k(x_i, x_j)\)</span> to build a <strong>covariance matrix</strong> (<span class="math inline">\(K\)</span>) using the set of already observed input points <span class="math inline">\(X_{train} = \{x_1, ..., x_n\}\)</span>.</p>
<p><span class="math inline">\(K = \begin{pmatrix}
k(x_1, x_1) &amp; k(x_1, x_2) &amp; \dots &amp; k(x_1, x_n) \\
k(x_2, x_1) &amp; k(x_2, x_2) &amp; \dots &amp; k(x_2, x_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
k(x_n, x_1) &amp; k(x_n, x_2) &amp; \dots &amp; k(x_n, x_n)
\end{pmatrix}\)</span></p>
<p>Each entry <span class="math inline">\(K_{ij} = k(x_i, x_j)\)</span> represents the assumed covariance (relatedness) between the function values <span class="math inline">\(f(x_i)\)</span> and <span class="math inline">\(f(x_j)\)</span>. This matrix, along with the observed output values <span class="math inline">\(Y_{train} = \{y_1, ..., y_n\}\)</span>, forms the core of the GP model.</p>
<p>When predicting the function value at a <em>new</em>, unobserved point <span class="math inline">\(x_*\)</span>, the GP uses:</p>
<ol type="1">
<li><p>The kernel evaluations between <span class="math inline">\(x_*\)</span> and all training points: <span class="math inline">\([k(x_*, x_1), k(x_*, x_2), ..., k(x_*, x_n)]\)</span>.</p></li>
<li><p>The kernel evaluation of <span class="math inline">\(x_*\)</span> with itself: <span class="math inline">\(k(x_*, x_*)\)</span>.</p></li>
<li><p>The covariance matrix <span class="math inline">\(K\)</span> of the training points.</p></li>
<li><p>The observed values <span class="math inline">\(Y_{train}\)</span>.</p></li>
</ol>
<p>Through matrix operations involving these components, the GP calculates the predicted mean value <span class="math inline">\(\hat{f}(x_*)\)</span> (the height of the red dotted line) and the variance (which gives the uncertainty or confidence interval) at <span class="math inline">\(x_*\)</span>. The prediction <span class="math inline">\(\hat{f}(x_*)\)</span> is effectively a weighted average of the observed <span class="math inline">\(Y_{train}\)</span> values, where the weights are determined by the kernel similarities.</p>
<p><strong>Step 5: Application to Our Example - Shaping the Red Line</strong></p>
<p>In the first Python code example we looked at, the kernel was defined as:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># From previous code:</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> RBF(length_scale<span class="op">=</span><span class="fl">5.0</span>, ...) <span class="op">+</span> WhiteKernel(noise_level<span class="op">=</span><span class="fl">0.5</span>, ...)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p><strong><code>RBF(length_scale=5.0, ...)</code></strong>: This specifies the RBF kernel. The initial <code>length_scale=5.0</code> sets the assumption about the function’s smoothness. A value of 5.0 suggests that points within roughly 5 units of each other have significantly correlated outputs. This value influences how smooth or wiggly the resulting red dotted line is. (Note: <code>scikit-learn</code>’s <code>GaussianProcessRegressor</code> often optimizes these hyperparameters like <code>length_scale</code> during the <code>.fit()</code> process unless told not to).</p></li>
<li><p><strong><code>1.0 * ...</code></strong>: This is the output variance <span class="math inline">\(\sigma^2\)</span>. It scales the amplitude.</p></li>
<li><p><strong><code>+ WhiteKernel(...)</code></strong>: This kernel adds a small amount to the diagonal of the covariance matrix (<span class="math inline">\(k(x_i, x_i)\)</span>). It represents observation noise – the assumption that our measured <span class="math inline">\(y_i\)</span> might not be <em>exactly</em> <span class="math inline">\(f(x_i)\)</span> but rather <span class="math inline">\(f(x_i) + \epsilon_i\)</span>, where <span class="math inline">\(\epsilon_i\)</span> is some noise. This helps stabilize the calculations and makes the model less sensitive to individual data points.</p></li>
</ul>
<p><em>Python Code: Impact of Length Scale on the GP Fit</em></p>
<p>This code demonstrates how changing <em>only</em> the <code>length_scale</code> in the RBF kernel alters the shape of the fitted GP model (the red dotted line) using a few sample points.</p>
<div id="a8209b25" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process <span class="im">import</span> GaussianProcessRegressor</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process.kernels <span class="im">import</span> RBF, WhiteKernel</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Sample Data ---</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Use a few points similar to the Iteration 1 scenario for illustration</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.array([<span class="fl">6.0</span>, <span class="fl">7.5</span>, <span class="fl">13.0</span>, <span class="fl">18.0</span>, <span class="fl">21.5</span>, <span class="fl">28.0</span>, <span class="fl">35.0</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple target function just for this illustration</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># (The GP doesn't know this function, it only sees X_train, Y_train)</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_target_func(x):</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sin(x<span class="op">/</span><span class="fl">3.0</span>)<span class="op">*</span><span class="dv">5</span> <span class="op">+</span> np.cos(x<span class="op">/</span><span class="fl">1.5</span>)<span class="op">*</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">7</span> <span class="op">-</span> <span class="dv">5</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> <span class="dv">22</span>) <span class="op">/</span> <span class="fl">2.5</span>)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> simple_target_func(X_train).ravel()</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the domain for plotting the GP prediction</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">50</span>, <span class="dv">300</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Kernels with different length scales ---</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># We also include a WhiteKernel for numerical stability/noise modelling</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>kernels_to_test <span class="op">=</span> {</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Short (l=1.0)"</span>: <span class="fl">1.0</span> <span class="op">*</span> RBF(length_scale<span class="op">=</span><span class="fl">1.0</span>, length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="op">+</span> WhiteKernel(noise_level<span class="op">=</span><span class="fl">0.1</span>, noise_level_bounds<span class="op">=</span><span class="st">"fixed"</span>),</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Medium (l=5.0)"</span>: <span class="fl">1.0</span> <span class="op">*</span> RBF(length_scale<span class="op">=</span><span class="fl">5.0</span>, length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="op">+</span> WhiteKernel(noise_level<span class="op">=</span><span class="fl">0.1</span>, noise_level_bounds<span class="op">=</span><span class="st">"fixed"</span>),</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Long (l=15.0)"</span>: <span class="fl">1.0</span> <span class="op">*</span> RBF(length_scale<span class="op">=</span><span class="fl">15.0</span>, length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="op">+</span> WhiteKernel(noise_level<span class="op">=</span><span class="fl">0.1</span>, noise_level_bounds<span class="op">=</span><span class="st">"fixed"</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: "fixed" prevents the GP from optimizing the hyperparameters, so we see the direct effect.</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Plotting ---</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">7</span>))</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training data points</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>plt.plot(X_train, Y_train, <span class="st">'g*'</span>, markersize<span class="op">=</span><span class="dv">12</span>, label<span class="op">=</span><span class="st">'Sampled Points (Training Data)'</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the true underlying function for reference (GP doesn't see this)</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(X_plot, simple_target_func(X_plot), 'k-', alpha=0.3, linewidth=1.5, label='True Underlying Function (Unknown to GP)')</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit GP and plot prediction for each kernel</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, kernel <span class="kw">in</span> kernels_to_test.items():</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Instantiate the Gaussian Process Regressor</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># alpha=1e-10 adds a tiny value to the diagonal for numerical stability</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kernel, alpha<span class="op">=</span><span class="fl">1e-10</span>,</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>                                  optimizer<span class="op">=</span><span class="va">None</span>, <span class="co"># Explicitly disable optimizer</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>                                  normalize_y<span class="op">=</span><span class="va">False</span>) <span class="co"># Keep simple for illustration</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit the GP model to the training data</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    gp.fit(X_train, Y_train)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predict mean and standard deviation over the plotting domain</span></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    y_pred, std_dev <span class="op">=</span> gp.predict(X_plot, return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the GP mean prediction (the surrogate model)</span></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_plot, y_pred, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'GP Mean Fit (</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the uncertainty (e.g., +/- 1 standard deviation)</span></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>    plt.fill_between(X_plot.ravel(), y_pred <span class="op">-</span> std_dev, y_pred <span class="op">+</span> std_dev, alpha<span class="op">=</span><span class="fl">0.15</span>, label<span class="op">=</span><span class="ss">f'_nolabel_'</span>) <span class="co"># Underscore hides from legend</span></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Impact of RBF Kernel Length Scale ($l$) on Gaussian Process Fit'</span>)</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Input $x$'</span>)</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Output $f(x)$'</span>)</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="bu">min</span>(Y_train)<span class="op">-</span><span class="dv">5</span>, <span class="bu">max</span>(Y_train)<span class="op">+</span><span class="dv">5</span>) <span class="co"># Adjust y-limits based on data</span></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bayesian_optimization_files/figure-html/cell-4-output-1.png" width="974" height="600" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Summary</strong></p>
<p>The kernel is the component of a Gaussian Process that encodes our prior beliefs about the function we are modeling, specifically:</p>
<ol type="1">
<li><strong>Similarity:</strong> How related are the function outputs for given inputs?</li>
<li><strong>Smoothness:</strong> How rapidly do we expect the function to change? (Controlled mainly by the length scale <span class="math inline">\(l\)</span>).</li>
<li><strong>Amplitude:</strong> What is the expected range of function values? (Controlled by the output variance <span class="math inline">\(\sigma^2\)</span>).</li>
</ol>
<p>Choosing a kernel and its hyperparameters (or letting the GP optimize them) fundamentally shapes the surrogate model (the red dotted line) and, consequently, influences how Bayesian Optimization explores the search space.</p>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>