---
title: Optuna optimization
jupyter: python3
---

We consider a scheduling problem where the schedule is represented by a vector $\mathbf{x} = (x_0, x_1, \ldots, x_{T-1})^T$. This vector comprises $T$ components, where $x_j$ denotes the non-negative allocation (e.g., number of patients or tasks) to time slot $j$, for $j = 0, \ldots, T-1$. A fundamental constraint is that the total allocation across all time slots must equal a fixed constant $N$: $$ \sum_{j=0}^{T-1} x_j = N $$ We require $x_j \ge 0$ for all $j = 0, \ldots, T-1$. Consequently, a valid schedule $\mathbf{x}$ belongs to the feasible set $\mathcal{F} = \{ \mathbf{z} \in \mathbb{D}^{T} \mid \sum_{j=0}^{T-1} z_j = N, z_j \ge 0 \text{ for all } j\}$, where $\mathbb{D}$ is typically the set of non-negative integers ($\mathbb{Z}_{\ge 0}$) or non-negative real numbers ($\mathbb{R}_{\ge 0}$). The definitions below use $\mathbb{R}_{\ge 0}^{T}$.

We define a neighborhood structure for local search based on perturbation vectors derived from a set of $T$ basis change vectors, $v_i \in \mathbb{R}^{T}$, for $i = 0, \ldots, T-1$. These basis vectors represent elementary shifts of allocation between time slots:

-   $v_0 = (-1, 0, \ldots, 0, 1)^T$ (Shift unit *from* slot 0 *to* slot $T-1$)

-   $v_1 = (1, -1, 0, \ldots, 0)^T$ (Shift unit *from* slot 1 *to* slot 0)

-   $v_i = (0, \ldots, 0, \underbrace{1}_{\text{pos } i-1}, \underbrace{-1}_{\text{pos } i}, 0, \ldots, 0)^T$ for $i = 2, \ldots, T-1$ (Shift unit *from* slot $i$ *to* slot $i-1$)

A key property of these basis vectors is that the sum of components for each vector is zero: $\sum_{j=0}^{T-1} v_{ij} = 0$ for all $i=0, \ldots, T-1$.

Perturbations are constructed using a binary selection vector $\mathbf{U} = (u_0, u_1, \ldots, u_{T-1})^T$, where $u_i \in \{0, 1\}$. Each $u_i$ indicates whether the basis change $v_i$ is included in the perturbation. The resulting perturbation vector $\mathbf{r}(\mathbf{U}) \in \mathbb{R}^{T}$ is the linear combination: $$ \mathbf{r}(\mathbf{U}) := \sum_{i=0}^{T-1} u_i v_i $$

Since each $v_i$ sums to zero, any perturbation $\mathbf{r}(\mathbf{U})$ also sums to zero: $\sum_{j=0}^{T-1} r_j(\mathbf{U}) = 0$. This ensures that applying such a perturbation to a valid schedule $\mathbf{x}$ preserves the total allocation $N$.

Two specific selection vectors result in a zero perturbation:

1.  If $\mathbf{U} = \mathbf{0} = (0, \ldots, 0)^T$, then $\mathbf{r}(\mathbf{0}) = \mathbf{0}$.

2.  If $\mathbf{U} = \mathbf{1} = (1, \ldots, 1)^T$, then $\mathbf{r}(\mathbf{1}) = \sum_{i=0}^{T-1} v_i = \mathbf{0}$, as demonstrated by summing the components of the basis vectors.

The neighborhood of a schedule $\mathbf{x} \in \mathcal{F}$, denoted by $\mathcal{N}(\mathbf{x})$, comprises all distinct, feasible schedules $\mathbf{x}'$ reachable by applying a non-zero perturbation $\mathbf{r}(\mathbf{U})$: $$ \mathcal{N}(\mathbf{x}) := \{ \mathbf{x}' \mid \mathbf{x}' = \mathbf{x} + \mathbf{r}(\mathbf{U}), \mathbf{U} \in \{0,1\}^T, \mathbf{r}(\mathbf{U}) \neq \mathbf{0}, \text{ and } x'_j \ge 0 \text{ for all } j = 0, \ldots, T-1 \} $$

Note that because $\sum r_j(\mathbf{U}) = 0$, any $\mathbf{x}'$ generated from $\mathbf{x} \in \mathcal{F}$ automatically satisfies $\sum x'_j = N$. The condition $\mathbf{r}(\mathbf{U}) \neq \mathbf{0}$ explicitly excludes the transformations resulting from $\mathbf{U}=\mathbf{0}$ and $\mathbf{U}=\mathbf{1}$.

There are $2^T$ possible selection vectors $\mathbf{U}$. Since $\mathbf{U}=\mathbf{0}$ and $\mathbf{U}=\mathbf{1}$ both yield $\mathbf{r}(\mathbf{U}) = \mathbf{0}$ (assuming $T \ge 1$ so $\mathbf{0} \neq \mathbf{1}$), there are $2^T - 2$ distinct selection vectors that generate non-zero perturbations. This establishes an upper bound on the number of candidate neighbors generated by unique non-zero perturbations: $$ |\mathcal{N}(\mathbf{x})| \le 2^T - 2 $$

The actual size of the neighborhood may be smaller than this bound due to the non-negativity constraint ($\mathbf{x}'_j \ge 0$) rendering some potential neighbors infeasible.

The local search aims to iteratively improve the schedule based on an objective function $C(\mathbf{x})$. Given the following constants:

-   $\mathbf{x}$: The current feasible schedule vector ($\mathbf{x} \in \mathcal{F}$).

-   $N$: The total number of patients/tasks to be scheduled.

-   $T$: The number of time slots.

-   $d$: The duration of each time slot.

-   $s$: The service time distribution.

-   $q$: The no-show probability.

-   $w$: The objective function weight ($w \in [0, 1]$).

-   $EWT(\mathbf{x})$: Expected Waiting Time for schedule $\mathbf{x}$.

-   $ESP(\mathbf{x})$: Expected Staff Penalty (e.g., idle/overtime) for schedule $\mathbf{x}$.

and the objective function: $$ C(\mathbf{x}) = w \cdot EWT(\mathbf{x}) + (1-w) \cdot ESP(\mathbf{x}) $$

, a neighborhood search step seeks to find a neighbor $\mathbf{x}' \in \mathcal{N}(\mathbf{x})$ such that $C(\mathbf{x}') < C(\mathbf{x})$. This involves evaluating candidate schedules generated by $\mathbf{x} + \mathbf{r}(\mathbf{U})$ for the $2^T - 2$ selection vectors $\mathbf{U}$ (where $\mathbf{U} \neq \mathbf{0}$ and $\mathbf{U} \neq \mathbf{1}$), checking their feasibility (non-negativity), and comparing their objective function values to $C(\mathbf{x})$.

In case such an improvement is found, the algorithm updates the current schedule to $\mathbf{x}'$ and continues the search. If no better neighbor is found, the algorithm terminates, returning the best schedule found during the search. Because of the multi-modular nature of the objective function, a local optimum must also be the global optimum.

## Setup

Load all necessary packages and initialize the constants

```{python}
import optuna
import logging # Optional: To see pruned trial messages if desired
import sys
import math 
import numpy as np
import time
from scipy.optimize import minimize
from itertools import combinations
from typing import List, Dict, Tuple, Callable, Optional, Union, Any, Iterable # Added type hints
import multiprocessing as mp

# Optional: Configure logging to see messages about pruned trials
# optuna.logging.get_logger("optuna").addHandler(logging.StreamHandler(sys.stdout))
```

```{python}
from functions import compute_convolutions, bailey_welch_schedule, get_v_star

N = 12 # Number of patients
T = 8 # Number of intervals
d = 5 # Length of each interval
max_s = 20 # Maximum service time
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.1 # Weight for the waiting time in objective function
l = 10
v_star = get_v_star(T)
print("v_star: \n", v_star)

# Create service time distribution
def generate_weighted_list(max_s: int, l: float, i: int) -> Optional[np.ndarray]:
    """
    Generates a service time probability distribution using optimization.

    This function creates a discrete probability distribution over max_s possible
    service times (from 1 to max_s). It uses optimization (SLSQP) to find a
    distribution whose weighted average service time is as close as possible
    to a target value 'l', subject to the constraint that the probabilities
    sum to 1 and each probability is between 0 and 1.

    After finding the distribution, it sorts the probabilities: the first 'i'
    probabilities (corresponding to service times 1 to i) are sorted in
    ascending order, and the remaining probabilities (service times i+1 to max_s)
    are sorted in descending order.

    Note:
        - Requires NumPy and SciPy libraries (specifically scipy.optimize.minimize).

    Args:
        max_s (int): Maximum service time parameter (number of probability bins).
                     Must be a positive integer.
        l (float): The target weighted average service time for the distribution.
                   Must be between 1 and max_s, inclusive.
        i (int): The index determining the sorting split point. Probabilities
                 for service times 1 to 'i' are sorted ascendingly, and
                 probabilities for service times 'i+1' to 'max_s' are sorted
                 descendingly. Must be between 1 and max_s-1 for meaningful sorting.

    Returns:
        numpy.ndarray: An array of size max_s+1. The first element (index 0) is 0.
                       Elements from index 1 to max_s represent the calculated
                       and sorted probability distribution, summing to 1.
                       Returns None if optimization fails or inputs are invalid.
    """

    # --- Input Validation ---
    if not isinstance(max_s, int) or max_s <= 0:
        print(f"Error: max_s must be a positive integer, but got {max_s}")
        return None
    if not isinstance(l, (int, float)) or not (1 <= l <= max_s):
        print(f"Error: Target average 'l' ({l}) must be between 1 and max_s ({max_s}).")
        return None
    if not isinstance(i, int) or not (0 < i < max_s):
        print(f"Error: Sorting index 'i' ({i}) must be between 1 and max_s-1 ({max_s-1}).")
        # If clamping is desired instead of error:
        # print(f"Warning: Index 'i' ({i}) is outside the valid range (1 to {max_s-1}). Clamping i.")
        # i = max(1, min(i, max_s - 1))
        return None # Strict check based on docstring requirement

    # --- Inner helper function for optimization ---
    def objective(x: np.ndarray) -> float:
        """Objective function: Squared difference between weighted average and target l."""
        # x represents probabilities P(1) to P(max_s)
        service_times = np.arange(1, max_s + 1)
        weighted_avg = np.dot(service_times, x) # Equivalent to sum(k * P(k) for k=1 to max_s)
        return (weighted_avg - l) ** 2

    # --- Constraints for optimization ---
    # Constraint 1: The sum of the probabilities must be 1
    constraints = ({
        'type': 'eq',
        'fun': lambda x: np.sum(x) - 1.0 # Ensure float comparison
    })

    # Bounds: Each probability value x[k] must be between 0 and 1
    # Creates a list of max_s tuples, e.g., [(0, 1), (0, 1), ..., (0, 1)]
    bounds = [(0, 1)] * max_s

    # Initial guess: Use Dirichlet distribution to get a random distribution that sums to 1.
    # Provides a starting point for the optimizer. np.ones(max_s) gives equal weights initially.
    initial_guess = np.random.dirichlet(np.ones(max_s))

    # --- Perform Optimization ---
    try:
        result = minimize(
            objective,
            initial_guess,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints,
            # options={'disp': False} # Set True for detailed optimizer output
        )

        # Check if optimization was successful
        if not result.success:
            print(f"Warning: Optimization failed! Message: {result.message}")
            # Optionally print result object for more details: print(result)
            return None # Indicate failure

        # The optimized probabilities (P(1) to P(max_s))
        optimized_probs = result.x

        # --- Post-process: Correct potential floating point inaccuracies ---
        # Ensure probabilities are non-negative and sum *exactly* to 1
        optimized_probs[optimized_probs < 0] = 0 # Clamp small negatives to 0
        current_sum = np.sum(optimized_probs)
        if not np.isclose(current_sum, 1.0):
            if current_sum > 0: # Avoid division by zero
                 optimized_probs /= current_sum # Normalize to sum to 1
            else:
                 print("Warning: Optimization resulted in zero sum probabilities after clamping negatives.")
                 # Handle this case - maybe return uniform distribution or None
                 return None # Or return uniform: np.ones(max_s) / max_s

    except Exception as e:
        print(f"An error occurred during optimization: {e}")
        return None

    # --- Reorder the probabilities based on the index 'i' ---
    # Split the probabilities P(1)...P(i) and P(i+1)...P(max_s)
    # Note: Python slicing is exclusive of the end index, array indexing is 0-based.
    # result.x[0] corresponds to P(1), result.x[i-1] to P(i).
    # result.x[i] corresponds to P(i+1), result.x[max_s-1] to P(max_s).

    first_part_probs = optimized_probs[:i]   # Probabilities P(1) to P(i)
    second_part_probs = optimized_probs[i:]  # Probabilities P(i+1) to P(max_s)

    # Sort the first part ascending, the second part descending
    sorted_first_part = np.sort(first_part_probs)
    sorted_second_part = np.sort(second_part_probs)[::-1] # [::-1] reverses

    # --- Create final output array ---
    # Array of size max_s + 1, initialized to zeros. Index 0 unused.
    values = np.zeros(max_s + 1)

    # Assign the sorted probabilities back into the correct slots (index 1 onwards)
    values[1 : i + 1] = sorted_first_part      # Assign P(1)...P(i)
    values[i + 1 : max_s + 1] = sorted_second_part # Assign P(i+1)...P(max_s)

    # Final check on sum after potential normalization/sorting
    if not np.isclose(np.sum(values[1:]), 1.0):
         print(f"Warning: Final distribution sum is {np.sum(values[1:])}, not 1.0. Check logic.")

    # Return the final array with the sorted probability distribution
    return values

i = 5  # First 5 highest values in ascending order, rest in descending order
s = generate_weighted_list(max_s, l, i)
print("Service time distribution: ", s)
print("Sum: ", np.sum(s))  # This should be 1
print("Weighted service time:", np.dot(np.arange(len(s)), s))  # This should be close to l
x_star = bailey_welch_schedule(T, d, N, s)
print(f"Initial schedule: {x_star}")
convolutions = compute_convolutions(s, N, q)
```

In order to compute the optimal solution with real cost, we can use the local search algorithm. The following code implements this approach. It uses the `local_search` function to compute the optimal solution.

```{python}
from functions import local_search

# Computing optimal solution with real cost
start = time.time()
test_x = local_search(x_star, d, convolutions, w, v_star, T, echo=True)
end = time.time()
print(f"Time taken for local search: {end - start:.2f} seconds")
```


```{python}
from vns_logic import variable_neighborhood_search

# This guard is ESSENTIAL for multiprocessing in notebooks/scripts
if __name__ == "__main__":
    # --- Configure Logging ---
    log_file = 'vns_run.log'
    logging.basicConfig(
        level=logging.INFO,  # Log INFO level and above (INFO, WARNING, ERROR, CRITICAL)
        format='%(asctime)s - %(levelname)s - %(processName)s - %(message)s',
        filename=log_file,
        filemode='w'  # 'w' clears the file each time, 'a' appends
    )
    # --- Crucial for 'spawn' start method (Windows, macOS, Quarto/Jupyter) ---
    mp.freeze_support() # Needed for frozen executables, good practice

    logging.info("Script execution started.")

    print("Starting VNS from Quarto...")
    start_time = time.time()

    # Call the imported function
    best_solution, best_cost = variable_neighborhood_search(
        x_init=x_star,
        d=d,
        convolutions=convolutions,
        w=w,
        v_star=v_star,
        echo=True # Set to False for less output
    )

    end_time = time.time()
    print("\n--- VNS Result ---")
    print(f"Best solution found: {best_solution}")
    print(f"Best cost found: {best_cost:.4f}")
    print(f"Total execution time: {end_time - start_time:.2f} seconds")
```

Now we will try a larger instance.

```{python}
N = 22 # Number of patients
T = 20 # Number of intervals
d = 5 # Length of each interval
max_s = 20 # Maximum service time
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.1 # Weight for the waiting time in objective function
l = 10
v_star = get_v_star(T)
print("v_star: \n", v_star)

i = 5  # First 5 highest values in ascending order, rest in descending order
s = generate_weighted_list(max_s, l, i)
print(s)
print("Sum:", np.sum(s[1:]))  # This should be 1
print("Weighted service time:", np.dot(np.arange(len(s)), s))  # This should be close to l
x_star = bailey_welch_schedule(T, d, N, s)
print(f"Initial schedule: {x_star}")
convolutions = compute_convolutions(s, N, q)
```


```{python}
# Computing optimal solution with real cost
start = time.time()
test_x = local_search(x_star, d, convolutions, w, v_star, T, echo=True)
end = time.time()
print(f"Time taken for local search: {end - start:.2f} seconds")
```

```{python}
# This guard is ESSENTIAL for multiprocessing in notebooks/scripts
if __name__ == "__main__":
    # --- Configure Logging ---
    log_file = 'vns_run.log'
    logging.basicConfig(
        level=logging.INFO,  # Log INFO level and above (INFO, WARNING, ERROR, CRITICAL)
        format='%(asctime)s - %(levelname)s - %(processName)s - %(message)s',
        filename=log_file,
        filemode='w'  # 'w' clears the file each time, 'a' appends
    )

    # --- Crucial for 'spawn' start method (Windows, macOS, Quarto/Jupyter) ---
    mp.freeze_support() # Needed for frozen executables, good practice

    logging.info("Script execution started.")

    print("Starting VNS from Quarto...")
    start_time = time.time()

    # Call the imported function
    best_solution, best_cost = variable_neighborhood_search(
        x_init=x_star,
        d=d,
        convolutions=convolutions,
        w=w,
        v_star=v_star,
        echo=True # Set to False for less output
    )

    end_time = time.time()
    print("\n--- VNS Result ---")
    print(f"Best solution found: {best_solution}")
    print(f"Best cost found: {best_cost:.4f}")
    print(f"Total execution time: {end_time - start_time:.2f} seconds")
```

