---
title: Optuna optimization
jupyter: python3
---

We consider a scheduling problem where the schedule is represented by a vector $\mathbf{x} = (x_0, x_1, \ldots, x_{T-1})^T$. This vector comprises $T$ components, where $x_j$ denotes the non-negative allocation (e.g., number of patients or tasks) to time slot $j$, for $j = 0, \ldots, T-1$. A fundamental constraint is that the total allocation across all time slots must equal a fixed constant $N$: $$ \sum_{j=0}^{T-1} x_j = N $$ We require $x_j \ge 0$ for all $j = 0, \ldots, T-1$. Consequently, a valid schedule $\mathbf{x}$ belongs to the feasible set $\mathcal{F} = \{ \mathbf{z} \in \mathbb{D}^{T} \mid \sum_{j=0}^{T-1} z_j = N, z_j \ge 0 \text{ for all } j\}$, where $\mathbb{D}$ is typically the set of non-negative integers ($\mathbb{Z}_{\ge 0}$) or non-negative real numbers ($\mathbb{R}_{\ge 0}$). The definitions below use $\mathbb{R}_{\ge 0}^{T}$.

We define a neighborhood structure for local search based on perturbation vectors derived from a set of $T$ basis change vectors, $v_i \in \mathbb{R}^{T}$, for $i = 0, \ldots, T-1$. These basis vectors represent elementary shifts of allocation between time slots:

-   $v_0 = (-1, 0, \ldots, 0, 1)^T$ (Shift unit *from* slot 0 *to* slot $T-1$)

-   $v_1 = (1, -1, 0, \ldots, 0)^T$ (Shift unit *from* slot 1 *to* slot 0)

-   $v_i = (0, \ldots, 0, \underbrace{1}_{\text{pos } i-1}, \underbrace{-1}_{\text{pos } i}, 0, \ldots, 0)^T$ for $i = 2, \ldots, T-1$ (Shift unit *from* slot $i$ *to* slot $i-1$)

A key property of these basis vectors is that the sum of components for each vector is zero: $\sum_{j=0}^{T-1} v_{ij} = 0$ for all $i=0, \ldots, T-1$.

Perturbations are constructed using a binary selection vector $\mathbf{U} = (u_0, u_1, \ldots, u_{T-1})^T$, where $u_i \in \{0, 1\}$. Each $u_i$ indicates whether the basis change $v_i$ is included in the perturbation. The resulting perturbation vector $\mathbf{r}(\mathbf{U}) \in \mathbb{R}^{T}$ is the linear combination: $$ \mathbf{r}(\mathbf{U}) := \sum_{i=0}^{T-1} u_i v_i $$

Since each $v_i$ sums to zero, any perturbation $\mathbf{r}(\mathbf{U})$ also sums to zero: $\sum_{j=0}^{T-1} r_j(\mathbf{U}) = 0$. This ensures that applying such a perturbation to a valid schedule $\mathbf{x}$ preserves the total allocation $N$.

Two specific selection vectors result in a zero perturbation:

1.  If $\mathbf{U} = \mathbf{0} = (0, \ldots, 0)^T$, then $\mathbf{r}(\mathbf{0}) = \mathbf{0}$.

2.  If $\mathbf{U} = \mathbf{1} = (1, \ldots, 1)^T$, then $\mathbf{r}(\mathbf{1}) = \sum_{i=0}^{T-1} v_i = \mathbf{0}$, as demonstrated by summing the components of the basis vectors.

The neighborhood of a schedule $\mathbf{x} \in \mathcal{F}$, denoted by $\mathcal{N}(\mathbf{x})$, comprises all distinct, feasible schedules $\mathbf{x}'$ reachable by applying a non-zero perturbation $\mathbf{r}(\mathbf{U})$: $$ \mathcal{N}(\mathbf{x}) := \{ \mathbf{x}' \mid \mathbf{x}' = \mathbf{x} + \mathbf{r}(\mathbf{U}), \mathbf{U} \in \{0,1\}^T, \mathbf{r}(\mathbf{U}) \neq \mathbf{0}, \text{ and } x'_j \ge 0 \text{ for all } j = 0, \ldots, T-1 \} $$

Note that because $\sum r_j(\mathbf{U}) = 0$, any $\mathbf{x}'$ generated from $\mathbf{x} \in \mathcal{F}$ automatically satisfies $\sum x'_j = N$. The condition $\mathbf{r}(\mathbf{U}) \neq \mathbf{0}$ explicitly excludes the transformations resulting from $\mathbf{U}=\mathbf{0}$ and $\mathbf{U}=\mathbf{1}$.

There are $2^T$ possible selection vectors $\mathbf{U}$. Since $\mathbf{U}=\mathbf{0}$ and $\mathbf{U}=\mathbf{1}$ both yield $\mathbf{r}(\mathbf{U}) = \mathbf{0}$ (assuming $T \ge 1$ so $\mathbf{0} \neq \mathbf{1}$), there are $2^T - 2$ distinct selection vectors that generate non-zero perturbations. This establishes an upper bound on the number of candidate neighbors generated by unique non-zero perturbations: $$ |\mathcal{N}(\mathbf{x})| \le 2^T - 2 $$

The actual size of the neighborhood may be smaller than this bound due to the non-negativity constraint ($\mathbf{x}'_j \ge 0$) rendering some potential neighbors infeasible.

The local search aims to iteratively improve the schedule based on an objective function $C(\mathbf{x})$. Given the following constants:

-   $\mathbf{x}$: The current feasible schedule vector ($\mathbf{x} \in \mathcal{F}$).

-   $N$: The total number of patients/tasks to be scheduled.

-   $T$: The number of time slots.

-   $d$: The duration of each time slot.

-   $s$: The service time distribution.

-   $q$: The no-show probability.

-   $w$: The objective function weight ($w \in [0, 1]$).

-   $EWT(\mathbf{x})$: Expected Waiting Time for schedule $\mathbf{x}$.

-   $ESP(\mathbf{x})$: Expected Staff Penalty (e.g., idle/overtime) for schedule $\mathbf{x}$.

and the objective function: $$ C(\mathbf{x}) = w \cdot EWT(\mathbf{x}) + (1-w) \cdot ESP(\mathbf{x}) $$

, a neighborhood search step seeks to find a neighbor $\mathbf{x}' \in \mathcal{N}(\mathbf{x})$ such that $C(\mathbf{x}') < C(\mathbf{x})$. This involves evaluating candidate schedules generated by $\mathbf{x} + \mathbf{r}(\mathbf{U})$ for the $2^T - 2$ selection vectors $\mathbf{U}$ (where $\mathbf{U} \neq \mathbf{0}$ and $\mathbf{U} \neq \mathbf{1}$), checking their feasibility (non-negativity), and comparing their objective function values to $C(\mathbf{x})$.

In case such an improvement is found, the algorithm updates the current schedule to $\mathbf{x}'$ and continues the search. If no better neighbor is found, the algorithm terminates, returning the best schedule found during the search. Because of the multi-modular nature of the objective function, a local optimum must also be the global optimum.

## Setup

Load all necessary packages and initialize the constants

```{python}
import optuna
import logging # Optional: To see pruned trial messages if desired
import sys
import math 
import numpy as np
import time
from scipy.optimize import minimize
from itertools import combinations
from typing import List, Dict, Tuple, Callable, Optional, Union, Any, Iterable # Added type hints
import multiprocessing as mp

# Optional: Configure logging to see messages about pruned trials
# optuna.logging.get_logger("optuna").addHandler(logging.StreamHandler(sys.stdout))
```

```{python}
from functions import compute_convolutions, bailey_welch_schedule, get_v_star

N = 12 # Number of patients
T = 8 # Number of intervals
d = 5 # Length of each interval
max_s = 20 # Maximum service time
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.1 # Weight for the waiting time in objective function
l = 10
v_star = get_v_star(T)
print("v_star: \n", v_star)

# Create service time distribution
def generate_weighted_list(max_s: int, l: float, i: int) -> Optional[np.ndarray]:
    """
    Generates a service time probability distribution using optimization.

    This function creates a discrete probability distribution over max_s possible
    service times (from 1 to max_s). It uses optimization (SLSQP) to find a
    distribution whose weighted average service time is as close as possible
    to a target value 'l', subject to the constraint that the probabilities
    sum to 1 and each probability is between 0 and 1.

    After finding the distribution, it sorts the probabilities: the first 'i'
    probabilities (corresponding to service times 1 to i) are sorted in
    ascending order, and the remaining probabilities (service times i+1 to max_s)
    are sorted in descending order.

    Note:
        - Requires NumPy and SciPy libraries (specifically scipy.optimize.minimize).

    Args:
        max_s (int): Maximum service time parameter (number of probability bins).
                     Must be a positive integer.
        l (float): The target weighted average service time for the distribution.
                   Must be between 1 and max_s, inclusive.
        i (int): The index determining the sorting split point. Probabilities
                 for service times 1 to 'i' are sorted ascendingly, and
                 probabilities for service times 'i+1' to 'max_s' are sorted
                 descendingly. Must be between 1 and max_s-1 for meaningful sorting.

    Returns:
        numpy.ndarray: An array of size max_s+1. The first element (index 0) is 0.
                       Elements from index 1 to max_s represent the calculated
                       and sorted probability distribution, summing to 1.
                       Returns None if optimization fails or inputs are invalid.
    """

    # --- Input Validation ---
    if not isinstance(max_s, int) or max_s <= 0:
        print(f"Error: max_s must be a positive integer, but got {max_s}")
        return None
    if not isinstance(l, (int, float)) or not (1 <= l <= max_s):
        print(f"Error: Target average 'l' ({l}) must be between 1 and max_s ({max_s}).")
        return None
    if not isinstance(i, int) or not (0 < i < max_s):
        print(f"Error: Sorting index 'i' ({i}) must be between 1 and max_s-1 ({max_s-1}).")
        # If clamping is desired instead of error:
        # print(f"Warning: Index 'i' ({i}) is outside the valid range (1 to {max_s-1}). Clamping i.")
        # i = max(1, min(i, max_s - 1))
        return None # Strict check based on docstring requirement

    # --- Inner helper function for optimization ---
    def objective(x: np.ndarray) -> float:
        """Objective function: Squared difference between weighted average and target l."""
        # x represents probabilities P(1) to P(max_s)
        service_times = np.arange(1, max_s + 1)
        weighted_avg = np.dot(service_times, x) # Equivalent to sum(k * P(k) for k=1 to max_s)
        return (weighted_avg - l) ** 2

    # --- Constraints for optimization ---
    # Constraint 1: The sum of the probabilities must be 1
    constraints = ({
        'type': 'eq',
        'fun': lambda x: np.sum(x) - 1.0 # Ensure float comparison
    })

    # Bounds: Each probability value x[k] must be between 0 and 1
    # Creates a list of max_s tuples, e.g., [(0, 1), (0, 1), ..., (0, 1)]
    bounds = [(0, 1)] * max_s

    # Initial guess: Use Dirichlet distribution to get a random distribution that sums to 1.
    # Provides a starting point for the optimizer. np.ones(max_s) gives equal weights initially.
    initial_guess = np.random.dirichlet(np.ones(max_s))

    # --- Perform Optimization ---
    try:
        result = minimize(
            objective,
            initial_guess,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints,
            # options={'disp': False} # Set True for detailed optimizer output
        )

        # Check if optimization was successful
        if not result.success:
            print(f"Warning: Optimization failed! Message: {result.message}")
            # Optionally print result object for more details: print(result)
            return None # Indicate failure

        # The optimized probabilities (P(1) to P(max_s))
        optimized_probs = result.x

        # --- Post-process: Correct potential floating point inaccuracies ---
        # Ensure probabilities are non-negative and sum *exactly* to 1
        optimized_probs[optimized_probs < 0] = 0 # Clamp small negatives to 0
        current_sum = np.sum(optimized_probs)
        if not np.isclose(current_sum, 1.0):
            if current_sum > 0: # Avoid division by zero
                 optimized_probs /= current_sum # Normalize to sum to 1
            else:
                 print("Warning: Optimization resulted in zero sum probabilities after clamping negatives.")
                 # Handle this case - maybe return uniform distribution or None
                 return None # Or return uniform: np.ones(max_s) / max_s

    except Exception as e:
        print(f"An error occurred during optimization: {e}")
        return None

    # --- Reorder the probabilities based on the index 'i' ---
    # Split the probabilities P(1)...P(i) and P(i+1)...P(max_s)
    # Note: Python slicing is exclusive of the end index, array indexing is 0-based.
    # result.x[0] corresponds to P(1), result.x[i-1] to P(i).
    # result.x[i] corresponds to P(i+1), result.x[max_s-1] to P(max_s).

    first_part_probs = optimized_probs[:i]   # Probabilities P(1) to P(i)
    second_part_probs = optimized_probs[i:]  # Probabilities P(i+1) to P(max_s)

    # Sort the first part ascending, the second part descending
    sorted_first_part = np.sort(first_part_probs)
    sorted_second_part = np.sort(second_part_probs)[::-1] # [::-1] reverses

    # --- Create final output array ---
    # Array of size max_s + 1, initialized to zeros. Index 0 unused.
    values = np.zeros(max_s + 1)

    # Assign the sorted probabilities back into the correct slots (index 1 onwards)
    values[1 : i + 1] = sorted_first_part      # Assign P(1)...P(i)
    values[i + 1 : max_s + 1] = sorted_second_part # Assign P(i+1)...P(max_s)

    # Final check on sum after potential normalization/sorting
    if not np.isclose(np.sum(values[1:]), 1.0):
         print(f"Warning: Final distribution sum is {np.sum(values[1:])}, not 1.0. Check logic.")

    # Return the final array with the sorted probability distribution
    return values

i = 5  # First 5 highest values in ascending order, rest in descending order
s = generate_weighted_list(max_s, l, i)
print("Service time distribution: ", s)
print("Sum: ", np.sum(s))  # This should be 1
print("Weighted service time:", np.dot(np.arange(len(s)), s))  # This should be close to l
x_star = bailey_welch_schedule(T, d, N, s)
print(f"Initial schedule: {x_star}")
convolutions = compute_convolutions(s, N, q)
```

The simplest application of applying Bayes Optimization to the Bailey-Welch schedule is to use the initial schedule as the starting point and try to find the next optimal schedule. The following code implements this approach. It uses the `optuna` library to perform the optimization.

```{python}
from functions import calculate_objective_serv_time_lookup

# ---------------------------------------------------------------------------
# --- Configuration ---
# ---------------------------------------------------------------------------

x_star = np.array(x_star)
# Derive T (list length) and N (target sum) from x_star
LIST_LENGTH = len(x_star)
TARGET_SUM = sum(x_star)

# --- Sanity check x_star (optional but recommended) ---
if any(x < 0 for x in x_star):
    raise ValueError(f"Starting schedule x_star contains negative values: {x_star}")
if LIST_LENGTH == 0:
     raise ValueError("Starting schedule x_star cannot be empty.")

# Other fixed parameters for the evaluation function
DURATION_THRESHOLD = d # Example: Time slot duration
WEIGHT = w # Example: Weight for ewt in the final objective (0 <= w <= 1)
# ---------------------------------------------------------------------------

# 1. Define function to create the objective
def create_objective(
    T: int,
    N: int,
    d: int,
    convolutions: Dict[int, np.ndarray],
    w: float
) -> Callable[[optuna.trial.Trial], float]:
    """
    Creates the objective function for Optuna, capturing T, N, d, convolutions, and w.
    (Code is identical to the previous version)
    """
    if not 0 <= w <= 1:
         raise ValueError(f"Weight w must be between 0 and 1, but got {w}")
    if 1 not in convolutions and N > 0: # Check key 1 only if N > 0
         # If N=0, TARGET_SUM is 0, schedule must be all zeros, convolution[1] isn't strictly needed.
         raise ValueError("Convolutions dictionary must contain the key '1' when N > 0.")

    def objective(trial: optuna.trial.Trial) -> float:
        X = [trial.suggest_int(f"x_{i}", 0, N) for i in range(T)]
        current_sum = sum(X)
        if current_sum != N:
            message = f"Constraint sum(X) == {N} violated (sum={current_sum}, X={X})"
            raise optuna.exceptions.TrialPruned(message)
        try:
            ewt, esp = calculate_objective_serv_time_lookup(schedule=X, d=d, convolutions=convolutions)
        except ValueError as e:
             print(f"Error during evaluation for schedule {X}: {e}")
             raise optuna.exceptions.TrialPruned(f"Evaluation failed: {e}") # Prune on eval error
        objective_value = w * ewt + (1 - w) * esp
        return objective_value
    return objective

# 2. Create the specific objective function instance using derived T and N
objective_func = create_objective(
    LIST_LENGTH,         # Derived T
    TARGET_SUM,          # Derived N
    DURATION_THRESHOLD,
    convolutions,   # Generated based on derived N
    WEIGHT
)

# 3. Create an Optuna study object
study = optuna.create_study(direction="minimize")

# 4. **** Enqueue the starting trial ****
# Convert x_star list into the parameter dictionary format
initial_params = {f"x_{i}": x_star[i] for i in range(LIST_LENGTH)}
print(f"\nEnqueuing starting schedule: {x_star} (Params: {initial_params})")
study.enqueue_trial(initial_params)

# 5. Run the optimization
# n_trials specifies the number of *new* trials Optuna runs *after* the enqueued ones.
n_trials_to_run = 99 # If you want 500 total evaluations, run 499 new ones after the enqueued trial.
print(f"\nStarting optimization...")
print(f"Derived Parameters: T={LIST_LENGTH}, N={TARGET_SUM}")
print(f"Fixed Parameters: d={DURATION_THRESHOLD}, w={WEIGHT}")
print(f"Attempting {n_trials_to_run} new trials (plus 1 enqueued trial)...")
start_time = time.time()
try:
    # Optuna will first run the enqueued trial(s), then sample n_trials_to_run more.
    study.optimize(objective_func, n_trials=n_trials_to_run)
except Exception as e:
    print(f"\nAn error occurred during optimization: {e}")
    # import traceback
    # traceback.print_exc()
end_time = time.time()
print(f"Optimization finished in {end_time - start_time:.2f} seconds.")


# 6. Get the results and report trial states
print(f"\n--- Results ---")
# The total number of trials will be n_trials_to_run + number of enqueued trials
print(f"Total number of trials run: {len(study.trials)}")

completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]
failed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]

print(f"  Successfully completed trials: {len(completed_trials)}")
print(f"  Pruned trials: {len(pruned_trials)}")
print(f"  Failed trials: {len(failed_trials)}")

# Check if any trial completed successfully (including the enqueued one)
if completed_trials:
    best_trial = study.best_trial
    best_parameters_optuna = best_trial.params
    best_value_optuna = best_trial.value

    # Reconstruct the list X from the best parameters dictionary
    best_X = [best_parameters_optuna.get(f'x_{i}', 0) for i in range(LIST_LENGTH)] # Use .get for safety

    print(f"\nBest trial found (among completed):")
    print(f"  Trial Number: {best_trial.number} {'(Enqueued Initial Schedule)' if best_trial.number == 0 else ''}") # Check if best was trial 0
    print(f"  Objective Value (w*ewt + (1-w)*esp): {best_value_optuna:.4f}")
    print(f"  Parameters (Schedule X): {best_X}")
    print(f"  Constraint check (Sum(X)): {sum(best_X)} (should be {TARGET_SUM})")

    # Optional: Recalculate ewt and esp for the best schedule
    try:
        # Ensure using the correct convolutions dict
        best_ewt, best_esp = calculate_objective_serv_time_lookup(best_X, DURATION_THRESHOLD, convolutions)
        print(f"  Components:")
        print(f"    - EWT (Expected Wait Time): {best_ewt:.4f}")
        print(f"    - ESP (Expected Spillover): {best_esp:.4f}")
        recalculated_value = WEIGHT * best_ewt + (1 - WEIGHT) * best_esp
        print(f"    - Recalculated Objective: {recalculated_value:.4f} (should match Best Value)")
    except Exception as e:
        print(f"  Could not recalculate EWT/ESP for the best trial: {e}")

else:
    # This case is less likely now, as the enqueued trial should run unless IT fails evaluation.
    print("\nNo trials completed successfully (including the enqueued starting schedule).")
    if failed_trials:
         print(f"  Check the error messages for Trial 0 (the enqueued trial) and others.")
    if pruned_trials and pruned_trials[0].number == 0:
         print(f"  The enqueued trial was pruned. Reason: {pruned_trials[0].intermediate_values}") # Or check logs
    print("Consider:")
    print(f"  - Verifying the 'calculate_objective_serv_time_lookup' works correctly for x_star.")
    print(f"  - Ensuring 'convolutions_data' is generated correctly for N={TARGET_SUM}.")
    print(f"  - Increasing n_trials if the search space is complex.")
```

From the output it is clear that the algorithm although working properly, is not able to find a better solution than the initial one. This is because in it's search it has to check a large number of unfeasable solutions. A lot of trials have to be discarded because they violate the constraint of $\mathbf{x} = N$.

To circumvent this, we can use the perturbation vectors $v_i$ to create a new schedule $\mathbf{x}^*$ that is close to the initial schedule $\mathbf{x}$, but not equal to it. This new schedule will be used as the starting point for the optimization. The number of unfeasable solutions now is limited to the cases that a new trial contains negative values.

Effectively the model now learns to manipulate the first derivative of the objective function. The following code implements this approach. Again we are not exploring all solutions, just the neighborhood of the initial solution. The code is similar to the previous one, but now we are using the perturbation vectors to create a new schedule $\mathbf{x}'$.

```{python}
# --- Optuna Configuration Variables ---
x_star = np.array(x_star)
duration_threshold = d # Use 'd' from initialization
weight = w             # Use 'w' from initialization
conv_dict = convolutions # Use 'convolutions' from initialization
num_u_variables = v_star.shape[0]

# ---------------------------------------------------------------------------
# Optuna Objective Factory Function
# ---------------------------------------------------------------------------
def create_objective(
    initial_x_star: np.ndarray,
    modification_v_star: np.ndarray,
    objective_weight: float,
    slot_duration: int,
    conv_dict: Dict[int, np.ndarray]
) -> Callable[[optuna.trial.Trial], float]:
    """ Creates the objective function for Optuna, capturing problem parameters. """

    internal_num_u = modification_v_star.shape[0]

    def objective(trial: optuna.trial.Trial) -> float:
        """ The function Optuna minimizes. """
        # 1. Suggest u
        U = np.array([trial.suggest_int(f"u_{i}", 0, 1) for i in range(internal_num_u)])
        u_column = U.reshape(-1, 1)
        # 2. Calculate r_u
        r_u = modification_v_star * u_column
        # 3. Sum r_u columns
        r_u_col_sum = np.sum(r_u, axis=0)
        # 4. Calculate neighbor
        neighbor_schedule = initial_x_star + r_u_col_sum
        # 5. Constraint Check
        if np.any(neighbor_schedule < 0):
            message = f"Constraint violation: Resulting schedule < 0: {neighbor_schedule} (u={U})"
            raise optuna.exceptions.TrialPruned(message)
        # 6. Evaluate
        try:
            ewt, esp = calculate_objective_serv_time_lookup(
                schedule=neighbor_schedule,
                d=slot_duration,
                convolutions=conv_dict
            )
            # Optional tracking
            # trial.set_user_attr("EWT", ewt)
            # trial.set_user_attr("ESP", esp)
        except ValueError as e:
            print(f"Error during evaluation for schedule {neighbor_schedule} (u={U}): {e}")
            raise optuna.exceptions.TrialPruned(f"Evaluation failed: {e}")
        # 7. Calculate final objective
        objective_value = objective_weight * ewt + (1 - objective_weight) * esp
        return objective_value

    return objective # Return the nested function

# ---------------------------------------------------------------------------
# Create Objective Instance & Run Optuna Study
# ---------------------------------------------------------------------------

# 2. Create the specific objective function instance
print("\n--- Starting Optuna Setup ---")
print("Creating Optuna objective function instance...")
objective_func = create_objective(
    initial_x_star=x_star,             # Use generated x_star (NumPy array)
    modification_v_star=v_star,        # Use generated v_star
    objective_weight=weight,           # Use defined w
    slot_duration=duration_threshold,  # Use defined d
    conv_dict=conv_dict                # Use generated convolutions
)
print("Objective function created.")

# 3. Create Optuna study
study = optuna.create_study(direction="minimize", study_name="Schedule_Opt_BW_Init")
print(f"Optuna study '{study.study_name}' created.")

# 4. Enqueue starting trial (U=0) - Use correct length: num_u_variables
initial_params = {f"u_{i}": 0 for i in range(num_u_variables)}
print(f"\nEnqueuing starting trial (u=all zeros):")
print(f"  Initial x_star evaluated: {x_star}")
print(f"  Enqueued Params (len={len(initial_params)} based on v_star rows): {initial_params}")
study.enqueue_trial(initial_params)

# 5. Run Optimization
n_trials_to_run = 99 # Example: 99 new trials + 1 enqueued = 100 total
print(f"\nStarting optimization run...")
print(f"  Attempting {n_trials_to_run} new trials (plus {len(study.trials)} enqueued)...")

start_time = time.time()
try:
    study.optimize(objective_func, n_trials=n_trials_to_run)
except KeyboardInterrupt:
    print("\nOptimization stopped manually.")
except Exception as e:
    print(f"\nAn unexpected error occurred during optimization: {e}")
    # import traceback
    # traceback.print_exc() # Uncomment for detailed error
finally:
    end_time = time.time()
    print(f"Optimization process finished in {end_time - start_time:.2f} seconds.")

# 6. Get and Display Results
print(f"\n--- Optimization Results ---")
print(f"Total number of trials in study: {len(study.trials)}")

completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]
failed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]

print(f"  Successfully completed trials: {len(completed_trials)}")
print(f"  Pruned trials (Constraint/Eval Fail): {len(pruned_trials)}")
print(f"  Failed trials (Other Errors): {len(failed_trials)}")

if completed_trials:
    best_trial = study.best_trial
    best_params_u = best_trial.params
    best_value_optuna = best_trial.value

    # Reconstruct best 'u' array using num_u_variables
    best_u_list = [best_params_u.get(f'u_{i}', 0) for i in range(num_u_variables)]
    best_u_np = np.array(best_u_list)

    # Calculate the best neighbor schedule
    best_u_column = best_u_np.reshape(-1, 1)
    best_r_u = v_star * best_u_column
    best_r_u_col_sum = np.sum(best_r_u, axis=0)
    best_neighbor_schedule = x_star + best_r_u_col_sum

    print(f"\nBest trial found (Trial #{best_trial.number}):")
    # Check if the best trial was the enqueued one
    if best_trial.number == 0:
        print("  (This was the initial enqueued trial)")
    print(f"  Objective Value: {best_value_optuna:.6f}")
    print(f"  Best Z parameters (len={len(best_u_list)}): {best_u_list}")
    print(f"  Corresponding Best Schedule: {best_neighbor_schedule.astype(int)}") # Show as int
    print(f"  Sum of Best Schedule: {np.sum(best_neighbor_schedule)}")

    # Optional: Recalculate EWT/ESP for verification
    try:
        best_ewt, best_esp = calculate_objective_serv_time_lookup(
            schedule=best_neighbor_schedule,
            d=duration_threshold,
            convolutions=conv_dict
        )
        recalculated_value = weight * best_ewt + (1 - weight) * best_esp
        print(f"  Verification Components:")
        print(f"    - EWT: {best_ewt:.6f}")
        print(f"    - ESP: {best_esp:.6f}")
        print(f"    - Recalculated Objective: {recalculated_value:.6f}")
    except Exception as e:
        print(f"  Could not recalculate EWT/ESP for the best trial: {e}")

elif pruned_trials or failed_trials:
    print("\nNo trials completed successfully.")
    if study.trials: # Check if list is not empty
         initial_trial = study.trials[0]
         print(f"  Status of Trial 0 (Enqueued): {initial_trial.state}")
         if initial_trial.state == optuna.trial.TrialState.PRUNED:
              print(f"  Trial 0 pruned, check logs for constraint violation or evaluation errors.")
         elif initial_trial.state == optuna.trial.TrialState.FAIL:
              print(f"  Trial 0 failed, check error messages during optimization run.")
    print("  Consider checking function implementations and constraints.")
else:
     print("\nNo trials were run or recorded in the study.")

print("\n--- End of Script ---")
```

```{python}
params = [f"u_{i}" for i in range(num_u_variables)]
fig = optuna.visualization.plot_parallel_coordinate(study, params)
fig
```


The results seem to indicate that the experiments are generating far less waste in the search space. Of the `{python} len(study.trials)` trials `{python}len(completed_trials)` were completed. The algorithm ran for `{python} end_time - start_time` seconds and was able to find a better solution than the initial one.

In order to compute the optimal solution with real cost, we can use the local search algorithm. The following code implements this approach. It uses the `local_search` function to compute the optimal solution.

```{python}
from functions import local_search

# Computing optimal solution with real cost
start = time.time()
test_x = local_search(x_star, d, convolutions, w, v_star, T, echo=True)
end = time.time()
print(f"Time taken for local search: {end - start:.2f} seconds")
```


As we can see the Bayesion Optimization had not yet found the global optimum. The local search algorithm is able to find the optimal solution in a much smaller amount of time. 

Now we will try a larger instance and use Bayesian Optimization to search for the optimal solution. We will let the Bayesian Optimization algorithm find and update the optimal solution. If it can't find a better solution it will break and return the best solution. The following code implements this approach.

```{python}
N = 22 # Number of patients
T = 20 # Number of intervals
d = 5 # Length of each interval
max_s = 20 # Maximum service time
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.1 # Weight for the waiting time in objective function
l = 10
v_star = get_v_star(T)
print("v_star: \n", v_star)

i = 5  # First 5 highest values in ascending order, rest in descending order
s = generate_weighted_list(max_s, l, i)
print(s)
print("Sum:", np.sum(s[1:]))  # This should be 1
print("Weighted service time:", np.dot(np.arange(len(s)), s))  # This should be close to l
x_star = bailey_welch_schedule(T, d, N, s)
print(f"Initial schedule: {x_star}")
convolutions = compute_convolutions(s, N, q)
```


```{python}
# --- Optuna Configuration Variables ---
x_star = np.array(x_star)
duration_threshold = d # Use 'd' from initialization
weight = w             # Use 'w' from initialization
conv_dict = convolutions # Use 'convolutions' from initialization
num_u_variables = v_star.shape[0]

# ---------------------------------------------------------------------------
# Optuna Objective Factory Function
# ---------------------------------------------------------------------------
def create_objective(
    initial_x_star: np.ndarray,
    modification_v_star: np.ndarray,
    objective_weight: float,
    slot_duration: int,
    conv_dict: Dict[int, np.ndarray]
) -> Callable[[optuna.trial.Trial], float]:
    """ Creates the objective function for Optuna, capturing problem parameters. """

    internal_num_u = modification_v_star.shape[0]

    def objective(trial: optuna.trial.Trial) -> float:
        """ The function Optuna minimizes. """
        # 1. Suggest u
        U = np.array([trial.suggest_int(f"u_{i}", 0, 1) for i in range(internal_num_u)])
        u_column = U.reshape(-1, 1)
        # 2. Calculate r_u
        r_u = modification_v_star * u_column
        # 3. Sum r_u columns
        r_u_col_sum = np.sum(r_u, axis=0)
        # 4. Calculate neighbor
        neighbor_schedule = initial_x_star + r_u_col_sum
        # 5. Constraint Check
        if np.any(neighbor_schedule < 0):
            message = f"Constraint violation: Resulting schedule < 0: {neighbor_schedule} (u={U})"
            raise optuna.exceptions.TrialPruned(message)
        # 6. Evaluate
        try:
            ewt, esp = calculate_objective_serv_time_lookup(
                schedule=neighbor_schedule,
                d=slot_duration,
                convolutions=conv_dict
            )
            # Optional tracking
            # trial.set_user_attr("EWT", ewt)
            # trial.set_user_attr("ESP", esp)
        except ValueError as e:
            print(f"Error during evaluation for schedule {neighbor_schedule} (u={U}): {e}")
            raise optuna.exceptions.TrialPruned(f"Evaluation failed: {e}")
        # 7. Calculate final objective
        objective_value = objective_weight * ewt + (1 - objective_weight) * esp
        return objective_value

    return objective # Return the nested function

# ---------------------------------------------------------------------------
# --- Iterative Optimization Loop ---
# ---------------------------------------------------------------------------
start = time.time()
max_iterations = 10     # Safety limit for iterations
tolerance = 1e-8        # Minimum improvement to continue
n_trials_per_iteration = 99 # Number of *new* trials per Optuna run

iteration = 0
improvement_found = True # Start the loop
current_x_star = x_star.copy() # Start with the BW schedule
current_best_value = float('inf') # Initialize high, will be set by initial eval

history = [] # Optional: Store (iteration, value, schedule)

# --- Initial Evaluation (Get value for the starting x_star) ---
print("\n--- Evaluating Initial Schedule ---")
try:
    initial_study = optuna.create_study(direction="minimize", study_name="Initial_Eval")
    # Use the factory to create the objective for the initial schedule
    initial_objective_func = create_objective(
        initial_x_star=current_x_star,      # Use the initial BW schedule
        modification_v_star=v_star,
        objective_weight=weight,
        slot_duration=duration_threshold,
        conv_dict=conv_dict
    )
    # Enqueue the trial representing the initial schedule itself (U=0)
    initial_params = {f"u_{i}": 0 for i in range(num_u_variables)}
    initial_study.enqueue_trial(initial_params)
    # Run only the single enqueued trial
    initial_study.optimize(initial_objective_func, n_trials=1)

    if initial_study.best_trial and initial_study.best_trial.state == optuna.trial.TrialState.COMPLETE:
         current_best_value = initial_study.best_trial.value
         print(f"Initial Objective Value: {current_best_value:.6f}")
         # Store initial state in history
         history.append((iteration, current_best_value, current_x_star.copy()))
    else:
         print("Error: Could not evaluate initial schedule successfully.")
         print(f"  Initial trial state: {initial_study.trials[0].state if initial_study.trials else 'N/A'}")
         improvement_found = False # Stop if initial eval fails
except Exception as e:
     print(f"An unexpected error occurred during initial evaluation: {e}")
     improvement_found = False # Stop

# --- Iterative Optimization Loop ---
while improvement_found and iteration < max_iterations:
    iteration += 1
    print(f"\n--- Optimization Iteration {iteration} ---")
    print(f"Starting with schedule: {current_x_star.astype(int)}")
    print(f"Current best value to beat: {current_best_value:.6f}")

    improvement_found_this_iter = False # Reset flag for this iteration

    # 1. Create new study and objective using the current best schedule
    study = optuna.create_study(direction="minimize", study_name=f"Sched_Opt_Iter_{iteration}")
    objective_func = create_objective(
        initial_x_star=current_x_star, # Use current best as starting point
        modification_v_star=v_star,
        objective_weight=weight,
        slot_duration=duration_threshold,
        conv_dict=conv_dict
    )

    # 2. Enqueue trial for the current schedule (U=0) to ensure it's evaluated
    params_to_enqueue = {f"u_{i}": 0 for i in range(num_u_variables)}
    study.enqueue_trial(params_to_enqueue)
    print(f"Enqueued current schedule evaluation (Trial 0 for this iteration).")

    # 3. Run Optuna optimization for this iteration
    print(f"Running Optuna search ({n_trials_per_iteration} new trials)...")
    start_time_iter = time.time()
    try:
        # n_trials is the number of NEW trials after the enqueued one
        study.optimize(objective_func, n_trials=n_trials_per_iteration)
    except Exception as e:
        print(f"Error during Optuna run in iteration {iteration}: {e}")
        # Consider whether to break or try to continue depending on error
        break # Stop loop on error
    finally:
        end_time_iter = time.time()
        print(f"Iteration {iteration} Optuna run finished in {end_time_iter - start_time_iter:.2f} seconds.")
        n_trials_per_iteration += 30 # Increment for next iteration

    # 4. Process results of this iteration
    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
    print(f"Iteration {iteration} results: {len(completed_trials)} completed trials out of {len(study.trials)} total.")

    if completed_trials:
        best_trial_iter = study.best_trial # Best among completed trials in *this* study
        best_value_iter = best_trial_iter.value

        print(f"  Best value found in iteration {iteration}: {best_value_iter:.6f} (Trial #{best_trial_iter.number})")

        # 5. Check for improvement compared to the start of the iteration
        if best_value_iter < current_best_value - tolerance:
            # Check if the best trial is actually different from the enqueued one
            if best_trial_iter.number > 0 or len(study.trials) == 1: # Improved or only initial trial ran/completed
                 print(f"  Improvement found! ({current_best_value:.6f} -> {best_value_iter:.6f})")
                 improvement_found_this_iter = True

                 # Update the overall best value
                 current_best_value = best_value_iter

                 # Calculate the schedule that yielded this improvement
                 best_params_u = best_trial_iter.params
                 best_u_list = [best_params_u.get(f'u_{i}', 0) for i in range(num_u_variables)]
                 best_u_np = np.array(best_u_list)
                 best_u_column = best_u_np.reshape(-1, 1)
                 best_r_u = v_star * best_u_column
                 best_r_u_col_sum = np.sum(best_r_u, axis=0)

                 # !!! Update current_x_star for the NEXT iteration !!!
                 # Apply the modification found to the schedule that started this iteration
                 current_x_star = current_x_star + best_r_u_col_sum
                 print(f"  Updated current_x_star for next iteration: {current_x_star.astype(int)}")

                 # Optional: Store history
                 history.append((iteration, current_best_value, current_x_star.copy()))
            else:
                 # Best trial was the enqueued one (value might be slightly different due to float precision but not < tolerance)
                 print(f"  Best trial was the enqueued starting point. No improvement among new trials.")
                 improvement_found_this_iter = False
        else:
            # No significant improvement found
            print(f"  No significant improvement found over current best value ({current_best_value:.6f}).")
            improvement_found_this_iter = False

    else:
        # No trials completed successfully in this iteration
        print("  No trials completed successfully in this iteration. Stopping.")
        improvement_found_this_iter = False

    # Update master loop flag
    improvement_found = improvement_found_this_iter

# --- End of Loop ---
print("\n--- Iterative Optimization Finished ---")
if iteration >= max_iterations:
    print(f"Stopping: Reached maximum iteration limit ({max_iterations}).")
elif not improvement_found:
     print(f"Stopping: No further improvement found in iteration {iteration}.")
else:
     print("Stopping: Loop finished for other reasons.")


# --- Final Results ---
print("\n--- Final Optimal Result ---")
if history: # Check if any results were recorded
    final_iter, final_value, final_schedule = history[-1] # Get last recorded state
    print(f"Found after iteration: {final_iter}")
    print(f"Final Objective Value: {final_value:.6f}")
    print(f"Final Optimal Schedule: {final_schedule.astype(int)}")
    print(f"Sum of Final Schedule: {np.sum(final_schedule)}")

    # Compare with the very initial schedule
    print("\nComparison with Initial Schedule:")
    print(f"  Initial Schedule (Iter 0): {history[0][2].astype(int)}")
    print(f"  Initial Objective Value:   {history[0][1]:.6f}")
else:
     print("No results found (Initial evaluation might have failed or no iterations ran).")

# Optional: Print full history
# print("\n--- Optimization History ---")
# for iter_num, value, sched in history:
#    print(f"Iter {iter_num}: Value={value:.6f}, Schedule={sched.astype(int)}")
end = time.time()
print(f"Total time taken for optimization: {end - start:.2f} seconds.")
print("\n--- End of Script ---")
```

```{python}
# Computing optimal solution with real cost
start = time.time()
test_x = local_search(x_star, d, convolutions, w, v_star, T, echo=True)
end = time.time()
print(f"Time taken for local search: {end - start:.2f} seconds")
```

