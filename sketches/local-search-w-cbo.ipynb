{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Combinatorial Bayesian Optimization Experiments\n",
        "editor: visual\n",
        "bibliography: references.bib\n",
        "---"
      ],
      "id": "2ef83555"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective\n",
        "\n",
        "The objective of this experiment is to evaluate and compare the performance of two distinct Combinatorial Bayesian Optimization (CBO) strategies for an outpatient appointment scheduling problem. We investigate:\n",
        "\n",
        "1.  CBO utilizing Expected Improvement (EI) as the acquisition function.\n",
        "2.  CBO utilizing Lower Confidence Bound (LCB) as the acquisition function with a fixed kappa ($\\kappa$) value.\n",
        "\n",
        "We aim to determine which strategy is most effective in identifying an optimal or near-optimal schedule, as measured by the objective function value, leveraging dictionary-based embeddings for the high-dimensional combinatorial space [@deshwal_bayesian_2023].\n",
        "\n",
        "## Background\n",
        "\n",
        "We consider an outpatient appointment scheduling problem as described by @kaandorp_optimal_2007 where the schedule is represented by a vector $\\mathbf{x} = (x_0, x_1, \\ldots, x_{T-1})^T$. This vector comprises $T$ components, where $x_j$ denotes the non-negative allocation (number of patients) to time slot $j$, for $j = 0, \\ldots, T-1$. A fundamental constraint is that the total allocation across all time slots must equal a fixed constant $N$: $$\\sum_{j=0}^{T-1} x_j = N$$ We require $x_j \\ge 0$ for all $j = 0, \\ldots, T-1$. Consequently, a valid schedule $\\mathbf{x}$ belongs to the feasible set $\\mathcal{F} = \\{ \\mathbf{z} \\in \\mathbb{D}^{T} \\mid \\sum_{j=0}^{T-1} z_j = N, z_j \\ge 0 \\text{ for all } j\\}$, where $\\mathbb{D}$ is the set of non-negative integers ($\\mathbb{Z}_{\\ge 0}$).\n",
        "\n",
        "@kaandorp_optimal_2007 define a neighborhood structure for local search based on perturbation vectors derived from a set of $T$ basis change vectors, $v_i \\in \\mathbb{D}^{T}$, for $i = 0, \\ldots, T-1$. These basis vectors represent elementary shifts of allocation between time slots:\n",
        "\n",
        "-   $v_0 = (-1, 0, \\ldots, 0, 1)$ (Shift unit *from* slot 0 *to* slot $T-1$)\n",
        "-   $v_1 = (1, -1, 0, \\ldots, 0)$ (Shift unit *from* slot 1 *to* slot 0)\n",
        "-   $v_i = (0, \\ldots, 0, \\underbrace{1}_{\\text{pos } i-1}, \\underbrace{-1}_{\\text{pos } i}, 0, \\ldots, 0)$ for $i = 2, \\ldots, T-1$ (Shift unit *from* slot $i$ *to* slot $i-1$)\n",
        "\n",
        "A key property of these basis vectors is that the sum of components for each vector is zero: $\\sum_{j=0}^{T-1} v_{ij} = 0$ for all $i=0, \\ldots, T-1$.\n",
        "\n",
        "Perturbations are constructed using a binary selection vector $\\mathbf{U} = (u_0, u_1, \\ldots, u_{T-1})$, where $u_i \\in \\{0, 1\\}$. Each $u_i$ indicates whether the basis change $v_i$ is included in the perturbation. The resulting perturbation vector $\\mathbf{r}(\\mathbf{U}) \\in \\mathbb{D}^{T}$ is the linear combination: $$\\mathbf{r}(\\mathbf{U}) := \\sum_{i=0}^{T-1} u_i v_i$$\n",
        "\n",
        "Since each $v_i$ sums to zero, any perturbation $\\mathbf{r}(\\mathbf{U})$ also sums to zero: $\\sum_{j=0}^{T-1} r_j(\\mathbf{U}) = 0$. This ensures that applying such a perturbation to a valid schedule $\\mathbf{x}$ preserves the total allocation $N$.\n",
        "\n",
        "The neighborhood of a schedule $\\mathbf{x} \\in \\mathcal{F}$, denoted by $\\mathcal{N}(\\mathbf{x})$, comprises all distinct, feasible schedules $\\mathbf{x}'$ reachable by applying a non-zero perturbation $\\mathbf{r}(\\mathbf{U})$ (@kaandorp_optimal_2007, use a slightly different but related neighborhood definition based on combinations of these basis vectors).\n",
        "\n",
        "The objective function to be minimized is a weighted sum of Expected Waiting Time (EWT) and Expected Staff Penalty (ESP), as defined by @kaandorp_optimal_2007: $$C(\\mathbf{x}) = w \\cdot EWT(\\mathbf{x}) + (1-w) \\cdot ESP(\\mathbf{x})$$ @kaandorp_optimal_2007 prove that this objective function is multimodular, which guarantees that a local search algorithm using their defined neighborhood converges to the global optimum.\n",
        "\n",
        "However, evaluating $C(\\mathbf{x})$ can be computationally expensive, especially for large $N$ and $T$. Furthermore, the search space defined by the binary vectors $\\mathbf{U}$ is high-dimensional ($2^T - 2$ possibilities, excluding $\\mathbf{0}$ and $\\mathbf{1}$). Bayesian Optimization (BO) is a suitable framework for optimizing such expensive black-box functions. Standard BO methods often struggle with high-dimensional combinatorial spaces. @deshwal_bayesian_2023 propose a method using dictionary-based embeddings (Hamming Embedding via Dictionaries - HED) to map the high-dimensional binary space of $\\mathbf{U}$ vectors into a lower-dimensional continuous space, where standard Gaussian Process (GP) models can be effectively applied. This experiment applies the HED approach within a BO framework to solve the scheduling problem formulated by @kaandorp_optimal_2007.\n",
        "\n",
        "## Hypothesis\n",
        "\n",
        "We hypothesize that:\n",
        "\n",
        "1.  Both CBO strategies, leveraging the HED embedding [@deshwal_bayesian_2023], will be capable of finding schedules superior to the initial schedule derived from the Bailey-Welch method (\\@).\n",
        "2.  CBO strategies employing Lower Confidence Bound (LCB) may exhibit superior performance or faster convergence compared to Expected Improvement (EI), due to the explicit exploration-exploitation trade-off inherent in LCB.\n",
        "\n",
        "## Methodology\n",
        "\n",
        "### Tools and Materials\n",
        "\n",
        "-   Programming Language: Python 3\n",
        "-   Core Libraries: NumPy, SciPy\n",
        "-   Machine Learning: Scikit-learn (for `GaussianProcessRegressor`, `MinMaxScaler`)\n",
        "-   Data Structures: Standard Python lists and dictionaries, NumPy arrays.\n",
        "-   Imported functions: `bailey_welch_schedule`, `get_v_star`, `compute_convolutions`, `calculate_objective_serv_time_lookup` (implementing the logic from @bailey1952study, assumed to be in an external `functions.py` file).\n",
        "\n",
        "### Experimental Design\n",
        "\n",
        "Three distinct Bayesian optimization experiments are conducted, applying the HED embedding approach [@deshwal_bayesian_2023] to the scheduling problem:\n",
        "\n",
        "1.  **Experiment 1: Expected Improvement (EI)**\n",
        "    -   Acquisition Function: Expected Improvement.\n",
        "    -   Objective: Minimize $C(\\mathbf{x})$ by iteratively selecting candidate vectors $\\mathbf{U}$ (via their embeddings) that maximize the EI.\n",
        "2.  **Experiment 2: Lower Confidence Bound (LCB) - Fixed Kappa**\n",
        "    -   Acquisition Function: Lower Confidence Bound.\n",
        "    -   Objective: Minimize $C(\\mathbf{x})$ using a fixed `kappa` ($\\kappa$) value in the LCB acquisition function applied to the GP model over the embedded space.\n",
        "\n",
        "For all experiments, Hamming Distance Embedding (HED) with a \"diverse random\" dictionary construction strategy [@deshwal_bayesian_2023] is employed to map the binary perturbation vectors $\\mathbf{U}$ to a continuous embedding space. A Gaussian Process (GP) model with Automatic Relevance Determination (ARD) kernels models the (negative) objective function in this embedded space.\n",
        "\n",
        "### Variables\n",
        "\n",
        "-   **Independent Variables**:\n",
        "    -   Type of acquisition function (EI, LCB).\n",
        "    -   The specific binary perturbation vector $\\mathbf{U}$ selected in each iteration (chosen via optimizing the acquisition function over the embedded space).\n",
        "-   **Dependent Variables**:\n",
        "    -   The objective function value $C(\\mathbf{x}')$ for the resulting schedule $\\mathbf{x}' = \\mathbf{x} + \\mathbf{r}(\\mathbf{U})$ (calculated using the method from @kaandorp_optimal_2007).\n",
        "    -   The best objective function value found throughout the optimization process.\n",
        "\n",
        "### Data Collection\n",
        "\n",
        "Data, comprising evaluated pairs $(\\mathbf{U}, C(\\mathbf{x}'))$, is collected iteratively:\n",
        "\n",
        "-   An initial set of `N_INITIAL` randomly generated $\\mathbf{U}$ vectors is evaluated.\n",
        "-   In each of the subsequent `N_ITERATIONS`, `BATCH_SIZE_q` new $\\mathbf{U}$ vectors are selected by optimizing the respective acquisition function over `NUM_CANDIDATES_Acqf` randomly generated candidate vectors in the original binary space (evaluated via their embeddings). These newly selected vectors are then evaluated, and the results are added to the dataset.\n",
        "\n",
        "### Sample Size and Selection\n",
        "\n",
        "-   **N_INITIAL**: 20 (number of initial random evaluations)\n",
        "-   **N_ITERATIONS**: 20 (number of Bayesian optimization iterations)\n",
        "-   **BATCH_SIZE_q**: 5 (number of candidates selected and evaluated per iteration)\n",
        "-   **NUM_CANDIDATES_Acqf**: $T \\times 1024 = 20 \\times 1024 = 20480$ (number of random candidates generated for optimizing the acquisition function in each iteration)\n",
        "-   **m**: 128 (dimensionality of the HED embedding space, following @deshwal_bayesian_2023)\n",
        "\n",
        "The selection of new points for evaluation is guided by the respective acquisition function (EI or LCB) optimized over the embedded space representation of candidate $\\mathbf{U}$ vectors.\n",
        "\n",
        "### Experimental Procedure\n",
        "\n",
        "#### 1. Setup\n",
        "\n",
        "Import necessary libraries and configure warning filters.\n"
      ],
      "id": "90ce7d18"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Core Libraries\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import warnings\n",
        "from scipy.optimize import minimize\n",
        "from typing import List, Dict, Tuple, Callable, Optional, Union, Any, Iterable\n",
        "\n",
        "# Scikit-learn for GP, Scaling, and potentially acquisition functions\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel, WhiteKernel\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# SciPy for statistics (needed for Expected Improvement calculation)\n",
        "from scipy.stats import norm\n",
        "\n",
        "from functions import bailey_welch_schedule, get_v_star, compute_convolutions, calculate_objective_serv_time_lookup\n",
        "\n",
        "# Filter warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning) # GP fitting might not always converge perfectly"
      ],
      "id": "3cf4d1d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Constants\n",
        "\n",
        "Definition of problem parameters and initial configuration.\n"
      ],
      "id": "ac10ffd7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Problem Definition ---\n",
        "\n",
        "# Fixed Data (Use your actual data)\n",
        "N = 32 # Total number of patients\n",
        "T = 30 # Dimension of the binary vector U\n",
        "d = 10 # Length of each interval\n",
        "max_s = 30 # Maximum service time\n",
        "q = 0.20 # Probability of a scheduled patient not showing up\n",
        "w = 0.1 # Weight for the waiting time in objective function\n",
        "l = 14\n",
        "v_star = get_v_star(T) # Get the V* matrix(T x T)\n",
        "# Create service time distribution\n",
        "def generate_weighted_list(max_s: int, l: float, i: int) -> Optional[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates a service time probability distribution using optimization.\n",
        "\n",
        "    This function creates a discrete probability distribution over max_s possible\n",
        "    service times (from 1 to max_s). It uses optimization (SLSQP) to find a\n",
        "    distribution whose weighted average service time is as close as possible\n",
        "    to a target value 'l', subject to the constraint that the probabilities\n",
        "    sum to 1 and each probability is between 0 and 1.\n",
        "\n",
        "    After finding the distribution, it sorts the probabilities: the first 'i'\n",
        "    probabilities (corresponding to service times 1 to i) are sorted in\n",
        "    ascending order, and the remaining probabilities (service times i+1 to max_s)\n",
        "    are sorted in descending order.\n",
        "\n",
        "    Note:\n",
        "        - Requires NumPy and SciPy libraries (specifically scipy.optimize.minimize).\n",
        "\n",
        "    Args:\n",
        "        max_s (int): Maximum service time parameter (number of probability bins).\n",
        "                     Must be a positive integer.\n",
        "        l (float): The target weighted average service time for the distribution.\n",
        "                   Must be between 1 and max_s, inclusive.\n",
        "        i (int): The index determining the sorting split point. Probabilities\n",
        "                 for service times 1 to 'i' are sorted ascendingly, and\n",
        "                 probabilities for service times 'i+1' to 'max_s' are sorted\n",
        "                 descendingly. Must be between 1 and max_s-1 for meaningful sorting.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: An array of size max_s+1. The first element (index 0) is 0.\n",
        "                       Elements from index 1 to max_s represent the calculated\n",
        "                       and sorted probability distribution, summing to 1.\n",
        "                       Returns None if optimization fails or inputs are invalid.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(max_s, int) or max_s <= 0:\n",
        "        print(f\"Error: max_s must be a positive integer, but got {max_s}\")\n",
        "        return None\n",
        "    if not isinstance(l, (int, float)) or not (1 <= l <= max_s):\n",
        "        print(f\"Error: Target average 'l' ({l}) must be between 1 and max_s ({max_s}).\")\n",
        "        return None\n",
        "    if not isinstance(i, int) or not (0 < i < max_s):\n",
        "        print(f\"Error: Sorting index 'i' ({i}) must be between 1 and max_s-1 ({max_s-1}).\")\n",
        "        # If clamping is desired instead of error:\n",
        "        # print(f\"Warning: Index 'i' ({i}) is outside the valid range (1 to {max_s-1}). Clamping i.\")\n",
        "        # i = max(1, min(i, max_s - 1))\n",
        "        return None # Strict check based on docstring requirement\n",
        "\n",
        "    # --- Inner helper function for optimization ---\n",
        "    def objective(x: np.ndarray) -> float:\n",
        "        \"\"\"Objective function: Squared difference between weighted average and target l.\"\"\"\n",
        "        # x represents probabilities P(1) to P(max_s)\n",
        "        service_times = np.arange(1, max_s + 1)\n",
        "        weighted_avg = np.dot(service_times, x) # Equivalent to sum(k * P(k) for k=1 to max_s)\n",
        "        return (weighted_avg - l) ** 2\n",
        "\n",
        "    # --- Constraints for optimization ---\n",
        "    # Constraint 1: The sum of the probabilities must be 1\n",
        "    constraints = ({\n",
        "        'type': 'eq',\n",
        "        'fun': lambda x: np.sum(x) - 1.0 # Ensure float comparison\n",
        "    })\n",
        "\n",
        "    # Bounds: Each probability value x[k] must be between 0 and 1\n",
        "    # Creates a list of max_s tuples, e.g., [(0, 1), (0, 1), ..., (0, 1)]\n",
        "    bounds = [(0, 1)] * max_s\n",
        "\n",
        "    # Initial guess: Use Dirichlet distribution to get a random distribution that sums to 1.\n",
        "    # Provides a starting point for the optimizer. np.ones(max_s) gives equal weights initially.\n",
        "    initial_guess = np.random.dirichlet(np.ones(max_s))\n",
        "\n",
        "    # --- Perform Optimization ---\n",
        "    try:\n",
        "        result = minimize(\n",
        "            objective,\n",
        "            initial_guess,\n",
        "            method='SLSQP',\n",
        "            bounds=bounds,\n",
        "            constraints=constraints,\n",
        "            # options={'disp': False} # Set True for detailed optimizer output\n",
        "        )\n",
        "\n",
        "        # Check if optimization was successful\n",
        "        if not result.success:\n",
        "            print(f\"Warning: Optimization failed! Message: {result.message}\")\n",
        "            # Optionally print result object for more details: print(result)\n",
        "            return None # Indicate failure\n",
        "\n",
        "        # The optimized probabilities (P(1) to P(max_s))\n",
        "        optimized_probs = result.x\n",
        "\n",
        "        # --- Post-process: Correct potential floating point inaccuracies ---\n",
        "        # Ensure probabilities are non-negative and sum *exactly* to 1\n",
        "        optimized_probs[optimized_probs < 0] = 0 # Clamp small negatives to 0\n",
        "        current_sum = np.sum(optimized_probs)\n",
        "        if not np.isclose(current_sum, 1.0):\n",
        "            if current_sum > 0: # Avoid division by zero\n",
        "                 optimized_probs /= current_sum # Normalize to sum to 1\n",
        "            else:\n",
        "                 print(\"Warning: Optimization resulted in zero sum probabilities after clamping negatives.\")\n",
        "                 # Handle this case - maybe return uniform distribution or None\n",
        "                 return None # Or return uniform: np.ones(max_s) / max_s\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during optimization: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- Reorder the probabilities based on the index 'i' ---\n",
        "    # Split the probabilities P(1)...P(i) and P(i+1)...P(max_s)\n",
        "    # Note: Python slicing is exclusive of the end index, array indexing is 0-based.\n",
        "    # result.x[0] corresponds to P(1), result.x[i-1] to P(i).\n",
        "    # result.x[i] corresponds to P(i+1), result.x[max_s-1] to P(max_s).\n",
        "\n",
        "    first_part_probs = optimized_probs[:i]   # Probabilities P(1) to P(i)\n",
        "    second_part_probs = optimized_probs[i:]  # Probabilities P(i+1) to P(max_s)\n",
        "\n",
        "    # Sort the first part ascending, the second part descending\n",
        "    sorted_first_part = np.sort(first_part_probs)\n",
        "    sorted_second_part = np.sort(second_part_probs)[::-1] # [::-1] reverses\n",
        "\n",
        "    # --- Create final output array ---\n",
        "    # Array of size max_s + 1, initialized to zeros. Index 0 unused.\n",
        "    values = np.zeros(max_s + 1)\n",
        "\n",
        "    # Assign the sorted probabilities back into the correct slots (index 1 onwards)\n",
        "    values[1 : i + 1] = sorted_first_part      # Assign P(1)...P(i)\n",
        "    values[i + 1 : max_s + 1] = sorted_second_part # Assign P(i+1)...P(max_s)\n",
        "\n",
        "    # Final check on sum after potential normalization/sorting\n",
        "    if not np.isclose(np.sum(values[1:]), 1.0):\n",
        "         print(f\"Warning: Final distribution sum is {np.sum(values[1:])}, not 1.0. Check logic.\")\n",
        "\n",
        "    # Return the final array with the sorted probability distribution\n",
        "    return values\n",
        "\n",
        "i = 10  # First 5 highest values in ascending order, rest in descending order\n",
        "s = generate_weighted_list(max_s, l, i)\n",
        "print(f\"Average generated service time: {np.dot(np.arange(len(s)), s)}\")\n",
        "convolutions = compute_convolutions(s, N, q)\n",
        "X = np.array(bailey_welch_schedule(T, d, N, s))\n",
        "print(f\"Initial schedule: {X}\")\n",
        "# Objective Function Calculation\n",
        "LARGE_PENALTY = 1e10 # Penalty for infeasible solutions\n",
        "ewt, esp = calculate_objective_serv_time_lookup(X, d, convolutions)\n",
        "initial_objective_value = w * ewt + (1 - w) * esp\n",
        "print(f\"Initial objective value: {initial_objective_value}\")"
      ],
      "id": "056a7035",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Common Functions (Objective Evaluation and HED)\n",
        "\n",
        "Objective evaluation implements $C(\\mathbf{x})$ from @kaandorp_optimal_2007. HED implementation follows @deshwal_bayesian_2023.\n"
      ],
      "id": "dcf28306"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_objective(U_np, X_vec, v_star, convolutions, d, w):\n",
        "    \"\"\"\n",
        "    Target function: Evaluates objective for a single binary numpy array U.\n",
        "    Returns a float.\n",
        "    \"\"\"\n",
        "    # Input validation (same as before)\n",
        "    if not isinstance(U_np, np.ndarray):\n",
        "        raise TypeError(\"Input U must be a numpy array\")\n",
        "    if U_np.ndim != 1:\n",
        "         raise ValueError(\"Input U must be 1-dimensional\")\n",
        "    if U_np.shape[0] != v_star.shape[0]:\n",
        "         raise ValueError(f\"Dimension mismatch: U length {U_np.shape[0]} != V* rows {v_star.shape[0]}.\")\n",
        "    if X_vec.shape[0] != v_star.shape[1]:\n",
        "         raise ValueError(\"Dimension mismatch: X length must match V* columns.\")\n",
        "    if not np.all((U_np == 0) | (U_np == 1)):\n",
        "         raise ValueError(\"Input U must be binary (0s and 1s).\")\n",
        "\n",
        "    # Calculate Y based on selected rows of V_star\n",
        "    V_sum = np.sum(v_star[U_np == 1, :], axis=0)\n",
        "    Y = X_vec + V_sum\n",
        "\n",
        "    # Check feasibility and calculate objective\n",
        "    if np.all(Y >= 0):\n",
        "        ewt, esp = calculate_objective_serv_time_lookup(Y, d, convolutions)\n",
        "        objective_value = w * ewt + (1 - w) * esp\n",
        "        return objective_value\n",
        "    else:\n",
        "        # Infeasible solution\n",
        "        return LARGE_PENALTY\n",
        "\n",
        "# --- HED Implementation ---\n",
        "\n",
        "def hamming_distance(u1, u2):\n",
        "    \"\"\"Calculates Hamming distance between two binary numpy arrays.\"\"\"\n",
        "    return np.sum(u1 != u2)\n",
        "\n",
        "def generate_diverse_random_dictionary(T, m):\n",
        "    \"\"\"Generates the random dictionary A for HED.\"\"\"\n",
        "    dictionary_A = np.zeros((m, T), dtype=int)\n",
        "    for i in range(m):\n",
        "        # Sample theta for density of 1s in this dictionary vector\n",
        "        theta = np.random.uniform(0, 1)\n",
        "        row = (np.random.rand(T) < theta).astype(int)\n",
        "        dictionary_A[i, :] = row\n",
        "    return dictionary_A\n",
        "\n",
        "def _generate_binary_hadamard_matrix_recursive(dim):\n",
        "    \"\"\"\n",
        "    Generates a binary (0/1) Hadamard-like matrix of size dim x dim.\n",
        "    'dim' must be a power of 2.\n",
        "    This uses the Sylvester's construction H_2n = [[H_n, H_n], [H_n, 1-H_n]]\n",
        "    starting with H_1 = [[1]].\n",
        "    \"\"\"\n",
        "    if not (dim > 0 and (dim & (dim - 1) == 0)): # Checks if dim is a power of 2\n",
        "        raise ValueError(\"Dimension must be a power of 2.\")\n",
        "\n",
        "    if dim == 1:\n",
        "        return np.array([[1]], dtype=int)\n",
        "    else:\n",
        "        h_prev = _generate_binary_hadamard_matrix_recursive(dim // 2)\n",
        "        h_top = np.hstack((h_prev, h_prev))\n",
        "        h_bottom = np.hstack((h_prev, 1 - h_prev)) # 1-H_n for binary\n",
        "        return np.vstack((h_top, h_bottom))\n",
        "\n",
        "def generate_wavelet_dictionary(T, m):\n",
        "    \"\"\"\n",
        "    Generates a dictionary A of size m x T using the subsampled binary wavelet approach.\n",
        "\n",
        "    Args:\n",
        "        T (int): The dimensionality of the input space (number of columns in dictionary).\n",
        "        m (int): The desired number of dictionary elements (number of rows).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: An m x T integer numpy array representing the dictionary.\n",
        "    \"\"\"\n",
        "    if T <= 0:\n",
        "        raise ValueError(\"T (dimensionality) must be positive.\")\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"m (dictionary size) must be positive.\")\n",
        "\n",
        "    # 1. Determine the smallest power of 2 >= T for the full wavelet matrix\n",
        "    if T == 1:\n",
        "        n_wavelet = 1\n",
        "    elif (T > 0 and (T & (T - 1) == 0)): # T is already a power of 2\n",
        "        n_wavelet = T\n",
        "    else:\n",
        "        n_wavelet = 2**math.ceil(math.log2(T))\n",
        "\n",
        "    # 2. Generate the full n_wavelet x n_wavelet binary Hadamard matrix\n",
        "    # print(f\"Generating full wavelet matrix of size: {n_wavelet}x{n_wavelet}\")\n",
        "    full_wavelet_matrix = _generate_binary_hadamard_matrix_recursive(n_wavelet)\n",
        "\n",
        "    # 3. Subsample T columns if n_wavelet > T\n",
        "    if n_wavelet > T:\n",
        "        # print(f\"Subsampling {T} columns from {n_wavelet} columns.\")\n",
        "        col_indices = np.random.choice(n_wavelet, size=T, replace=False)\n",
        "        col_indices.sort() # Optional: for deterministic testing if seed is set\n",
        "        wavelet_matrix_T_cols = full_wavelet_matrix[:, col_indices]\n",
        "    else:\n",
        "        wavelet_matrix_T_cols = full_wavelet_matrix # n_wavelet == T\n",
        "\n",
        "    # 4. Subsample m rows\n",
        "    num_available_rows = wavelet_matrix_T_cols.shape[0]\n",
        "    if m > num_available_rows:\n",
        "        print(f\"Warning: Requested dictionary size m ({m}) is greater than \"\n",
        "              f\"available unique wavelet rows ({num_available_rows}). \"\n",
        "              f\"Using all available rows and repeating if necessary, or consider reducing m.\")\n",
        "        # For simplicity, if m > num_available_rows, we'll sample with replacement\n",
        "        # or you could choose to error, or return fewer rows.\n",
        "        # The paper implies m should be less than or equal to the rows of B_d.\n",
        "        # If sampling with replacement is needed:\n",
        "        row_indices = np.random.choice(num_available_rows, size=m, replace=True)\n",
        "        # If strictly no replacement and m > num_available_rows, one might error or cap m\n",
        "        # row_indices = np.random.choice(num_available_rows, size=min(m, num_available_rows), replace=False)\n",
        "        # if m > num_available_rows:\n",
        "        #     # Handle the case where more rows are needed than available unique ones\n",
        "        #     # This might involve repeating rows or another strategy\n",
        "        #     pass\n",
        "    else:\n",
        "        # print(f\"Subsampling {m} rows from {num_available_rows} available rows.\")\n",
        "        row_indices = np.random.choice(num_available_rows, size=m, replace=False)\n",
        "\n",
        "    row_indices.sort() # Optional: for deterministic testing if seed is set\n",
        "    dictionary_A = wavelet_matrix_T_cols[row_indices, :]\n",
        "\n",
        "    return dictionary_A\n",
        "\n",
        "def embed_vector(U_np, dictionary_A):\n",
        "    \"\"\"Embeds a single binary vector U using HED.\"\"\"\n",
        "    m = dictionary_A.shape[0]\n",
        "    embedding_phi = np.zeros(m, dtype=float) # Use float for GP\n",
        "    for i in range(m):\n",
        "        embedding_phi[i] = hamming_distance(U_np, dictionary_A[i, :])\n",
        "    return embedding_phi\n",
        "\n",
        "def embed_batch(U_batch_np, dictionary_A):\n",
        "    \"\"\"Embeds a batch of binary vectors U.\"\"\"\n",
        "    # Input U_batch_np is expected to be a NumPy array\n",
        "    m = dictionary_A.shape[0]\n",
        "    if U_batch_np.ndim == 1: # Handle single vector case\n",
        "        U_batch_np = U_batch_np.reshape(1, -1)\n",
        "\n",
        "    batch_size = U_batch_np.shape[0]\n",
        "    embeddings_np = np.zeros((batch_size, m), dtype=float) # Use float for GP\n",
        "\n",
        "    for j in range(batch_size):\n",
        "        embeddings_np[j, :] = embed_vector(U_batch_np[j, :], dictionary_A)\n",
        "\n",
        "    # Return NumPy array directly\n",
        "    return embeddings_np\n"
      ],
      "id": "2894c5e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Experiment 1: CBO with Expected Improvement (EI)\n",
        "\n",
        "Applies the methodology from @deshwal_bayesian_2023 using EI.\n"
      ],
      "id": "2c5b4f50"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- get_fitted_model function remains the same ---\n",
        "def get_fitted_model(train_X_embedded_scaled, train_Y, m):\n",
        "    # ... (implementation is unchanged) ...\n",
        "    if train_Y.ndim > 1 and train_Y.shape[1] == 1: train_Y = train_Y.ravel()\n",
        "    kernel = ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \\\n",
        "             Matern(length_scale=np.ones(m), length_scale_bounds=(1e-2, 1e2), nu=2.5) + \\\n",
        "             WhiteKernel(noise_level=1e-10, # Small value for numerical stability\n",
        "                         noise_level_bounds=\"fixed\") # Bounds for noise optimization\n",
        "    gp_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-10, n_restarts_optimizer=10, random_state=42)\n",
        "    gp_model.fit(train_X_embedded_scaled, train_Y)\n",
        "    return gp_model\n",
        "\n",
        "def lower_confidence_bound(mu, sigma, kappa=2.576):\n",
        "    \"\"\"\n",
        "    Computes the Lower Confidence Bound (LCB) acquisition function.\n",
        "    Assumes maximization of this value guides the search (since mu is neg objective).\n",
        "    Higher LCB means lower predicted objective or lower penalty for uncertainty.\n",
        "\n",
        "    mu, sigma: Predicted mean and standard deviation (NumPy arrays).\n",
        "    kappa: Controls the balance between exploitation (high mu -> low original objective)\n",
        "           and exploration (low sigma).\n",
        "    \"\"\"\n",
        "    # Ensure sigma is non-negative\n",
        "    sigma = np.maximum(sigma, 0)\n",
        "    return mu - kappa * sigma # <<< Sign flipped from UCB\n",
        "\n",
        "def optimize_acqf_discrete_via_embedding(gp_model, scaler, dictionary_A, T, q, num_candidates, kappa):\n",
        "    \"\"\"\n",
        "    Optimizes LCB acquisition function by sampling random binary candidates,\n",
        "    embedding, SCALING, predicting with GP, and calculating LCB.\n",
        "    Selects the top q candidates based on LCB.\n",
        "    Returns candidates as a numpy array (q x T).\n",
        "    \"\"\"\n",
        "    m = dictionary_A.shape[0]\n",
        "\n",
        "    # 1. Generate Random Binary Candidates\n",
        "    candidate_u_vectors_np = np.random.randint(0, 2, size=(num_candidates, T))\n",
        "\n",
        "    # 2. Embed the Candidates\n",
        "    embedded_candidates_np = embed_batch(candidate_u_vectors_np, dictionary_A)\n",
        "\n",
        "    # 3. Scale the Embedded Candidates\n",
        "    embedded_candidates_scaled_np = scaler.transform(embedded_candidates_np)\n",
        "\n",
        "    # 4. Predict Mean and Std Dev using the GP Model\n",
        "    mu, std = gp_model.predict(embedded_candidates_scaled_np, return_std=True)\n",
        "\n",
        "    # 5. Calculate Acquisition Function (Lower Confidence Bound) <<< CHANGED HERE\n",
        "    acq_values = lower_confidence_bound(mu, std, kappa=kappa) # Use LCB\n",
        "\n",
        "    # 6. Select Top Candidates (based on highest LCB) <<< COMMENT UPDATED\n",
        "    # We maximize LCB = mu - kappa*sigma, where mu is neg_objective\n",
        "    top_indices = np.argsort(acq_values)[-q:]\n",
        "    top_indices = top_indices[::-1] # Ensure descending order of LCB\n",
        "\n",
        "    return candidate_u_vectors_np[top_indices, :]\n",
        "\n",
        "# --- Local Search with Embedded BO ---\n",
        "start_time = time.time()\n",
        "MAX_LOCAL_SEARCH_ITERATIONS = 10 # Max iterations for the outer local search loop\n",
        "CONVERGENCE_TOLERANCE = 1e-5    # If improvement is less than this, consider converged\n",
        "NO_IMPROVEMENT_STREAK_LIMIT = 3 # Stop if no improvement for this many LS iterations\n",
        "\n",
        "# BO Parameters (can be tuned)\n",
        "KAPPA = 2.576\n",
        "N_INITIAL_BO = 50\n",
        "N_ITERATIONS_BO = 20 # Max BO iterations *if no improvement is found earlier*\n",
        "BATCH_SIZE_q = 5     # Number of candidates BO proposes at once\n",
        "m = math.ceil(T/2) if T > 0 else 1\n",
        "NUM_CANDIDATES_Acqf = (T * 2 * 1024) if T > 0 else 100 # Ensure Acqf has candidates even if T=0 (though U will be empty)\n",
        "\n",
        "current_X_ls = np.copy(X) # Start with the initial X\n",
        "best_overall_f_ls = float('inf')\n",
        "best_overall_U_ls = None\n",
        "best_overall_Y_ls = None\n",
        "no_improvement_streak = 0\n",
        "\n",
        "print(f\"Starting Local Search with initial X: {current_X_ls}\")\n",
        "\n",
        "for ls_iter in range(MAX_LOCAL_SEARCH_ITERATIONS):\n",
        "    print(f\"\\n\\n{'='*10} LOCAL SEARCH ITERATION {ls_iter + 1}/{MAX_LOCAL_SEARCH_ITERATIONS} {'='*10}\")\n",
        "    print(f\"Current X for this LS iteration: {current_X_ls}\")\n",
        "    current_best_f_in_ls_iter = float('inf') # Best objective for the *current* X\n",
        "\n",
        "    # --- BO Loop (inner loop) ---\n",
        "    evaluated_U_np_list_bo = []\n",
        "    evaluated_f_vals_bo = []\n",
        "    train_Y_list_bo = [] # For negated objectives (GP maximizes)\n",
        "\n",
        "    # 1. Initialization for BO (using current_X_ls)\n",
        "    #    We can add some diverse initial candidates or just let BO explore\n",
        "    #    For simplicity, we'll generate a few random ones if initial_candidates is specific to global start\n",
        "    bo_initial_points = [np.random.randint(0, 2, size=T) for _ in range(N_INITIAL_BO)]\n",
        "    bo_initial_points.insert(0, np.zeros(T, dtype=int)) # Add U=zeros as the first point\n",
        "    # Ensure at least one non-empty schedule if possible\n",
        "    if T > 0 and not any(np.any(u) for u in bo_initial_points):\n",
        "        idx_to_set = np.random.randint(T)\n",
        "        bo_initial_points[0][idx_to_set] = 1\n",
        "\n",
        "\n",
        "    for U_init in bo_initial_points: # Use a small set of diverse points for each BO run\n",
        "        f_val = evaluate_objective(U_init, current_X_ls, v_star, convolutions, d, w)\n",
        "        neg_f_val = -f_val\n",
        "        evaluated_U_np_list_bo.append(U_init)\n",
        "        evaluated_f_vals_bo.append(f_val)\n",
        "        train_Y_list_bo.append(neg_f_val)\n",
        "\n",
        "    if evaluated_f_vals_bo:\n",
        "        best_f_this_bo_run = min(evaluated_f_vals_bo)\n",
        "        best_U_this_bo_run_idx = np.argmin(evaluated_f_vals_bo)\n",
        "        best_U_this_bo_run = evaluated_U_np_list_bo[best_U_this_bo_run_idx]\n",
        "    else: # Should not happen if N_INITIAL_BO > 0\n",
        "        best_f_this_bo_run = float('inf')\n",
        "        best_U_this_bo_run = np.zeros(T, dtype=int) # Placeholder\n",
        "\n",
        "    print(f\"BO Initial best objective for current X: {best_f_this_bo_run:.4f}\")\n",
        "\n",
        "    improvement_found_in_bo = False\n",
        "\n",
        "    for bo_iter in range(N_ITERATIONS_BO):\n",
        "        print(f\"\\n--- BO Iteration {bo_iter + 1}/{N_ITERATIONS_BO} (LS Iter {ls_iter + 1}) ---\")\n",
        "        start_time_bo = time.time()\n",
        "\n",
        "        # a. Generate dictionary A\n",
        "        current_dictionary_A = generate_diverse_random_dictionary(T, m) # Or wavelet\n",
        "\n",
        "        # b. Embed ALL evaluated U vectors for this BO run\n",
        "        if not evaluated_U_np_list_bo: continue\n",
        "        evaluated_U_np_array_bo = np.array(evaluated_U_np_list_bo)\n",
        "        embedded_train_X_bo = embed_batch(evaluated_U_np_array_bo, current_dictionary_A)\n",
        "\n",
        "        # c. Scale\n",
        "        scaler_bo = MinMaxScaler()\n",
        "        if embedded_train_X_bo.shape[0] > 0:\n",
        "            embedded_train_X_scaled_bo = scaler_bo.fit_transform(embedded_train_X_bo)\n",
        "        else:\n",
        "            embedded_train_X_scaled_bo = embedded_train_X_bo\n",
        "\n",
        "        train_Y_for_fit_bo = np.array(train_Y_list_bo)\n",
        "\n",
        "        # d. Fit GP Model\n",
        "        if embedded_train_X_scaled_bo.shape[0] > 0 and \\\n",
        "           train_Y_for_fit_bo.shape[0] == embedded_train_X_scaled_bo.shape[0]:\n",
        "            try:\n",
        "                gp_model_bo = get_fitted_model(embedded_train_X_scaled_bo, train_Y_for_fit_bo, m)\n",
        "            except ValueError as e:\n",
        "                print(f\"Warning: GP fitting error: {e}. Skipping BO iteration.\")\n",
        "                continue\n",
        "        else:\n",
        "            print(\"Warning: Not enough data or mismatch for GP. Skipping BO iteration.\")\n",
        "            continue\n",
        "\n",
        "        # f. Optimize Acquisition Function (LCB)\n",
        "        next_U_candidates_np_bo = optimize_acqf_discrete_via_embedding(\n",
        "            gp_model=gp_model_bo,\n",
        "            scaler=scaler_bo,\n",
        "            dictionary_A=current_dictionary_A,\n",
        "            T=T,\n",
        "            q=BATCH_SIZE_q,\n",
        "            num_candidates=NUM_CANDIDATES_Acqf,\n",
        "            kappa=KAPPA\n",
        "        )\n",
        "        if next_U_candidates_np_bo.shape[0] == 0:\n",
        "            print(\"Acquisition function optimization yielded no new candidates. Stopping BO for this LS iteration.\")\n",
        "            break\n",
        "\n",
        "\n",
        "        # g. Evaluate Objective for new candidates\n",
        "        newly_evaluated_U_bo = []\n",
        "        newly_evaluated_f_bo = []\n",
        "        newly_evaluated_neg_f_bo = []\n",
        "\n",
        "        for i in range(next_U_candidates_np_bo.shape[0]):\n",
        "            next_U_bo = next_U_candidates_np_bo[i, :]\n",
        "            # Check if already evaluated in *this specific BO run*\n",
        "            already_evaluated_this_bo = any(np.array_equal(next_U_bo, u) for u in evaluated_U_np_list_bo)\n",
        "            if already_evaluated_this_bo:\n",
        "                print(f\"  Candidate {i} was already evaluated in this BO run. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            next_f_bo = evaluate_objective(next_U_bo, current_X_ls, v_star, convolutions, d, w)\n",
        "            next_neg_f_bo = -next_f_bo\n",
        "            temp_Y_bo = current_X_ls + np.sum(v_star[next_U_bo == 1, :], axis=0) # Calculate Y for this candidate\n",
        "            print(f\"  BO Candidate {i}: Obj = {next_f_bo:.4f} (with current_X_ls)\") # Y: {temp_Y_bo}\n",
        "\n",
        "            newly_evaluated_U_bo.append(next_U_bo)\n",
        "            newly_evaluated_f_bo.append(next_f_bo)\n",
        "            newly_evaluated_neg_f_bo.append(next_neg_f_bo)\n",
        "\n",
        "            if next_f_bo < best_f_this_bo_run:\n",
        "                best_f_this_bo_run = next_f_bo\n",
        "                best_U_this_bo_run = next_U_bo # Update best U for this BO run\n",
        "                improvement_found_in_bo = True\n",
        "                print(f\"    New best for this BO run: Obj={best_f_this_bo_run:.4f}, U={best_U_this_bo_run}\")\n",
        "                # --- LOCAL SEARCH DECISION POINT ---\n",
        "                # If this new objective is better than the current_best_f_in_ls_iter\n",
        "                # (which is the objective associated with current_X_ls itself, or best found so far for current_X_ls)\n",
        "                # Then, we have found an improvement. We can break the BO loop and update X.\n",
        "                # For a true \"first improvement\" local search, we'd break here.\n",
        "                # For a \"best improvement\" within BO, we continue BO, then update LS.\n",
        "                # Let's implement \"first improvement found by BO for current X\"\n",
        "                # To make it more \"local searchy\", we compare to the objective of X itself\n",
        "                # or a very good objective found from X.\n",
        "                # If we are trying to improve upon current_X_ls (whose implicit U is \"all zeros\" for modifications)\n",
        "                # obj_of_current_X = evaluate_objective(np.zeros(T, dtype=int), current_X_ls, v_star, convolutions, d, w)\n",
        "                # We need a baseline. The baseline is the best objective found so far *for the current X_ls*.\n",
        "                # This is `best_f_this_bo_run` before this candidate was evaluated.\n",
        "                # So, if `next_f_bo` is better than the previous `best_f_this_bo_run`, it's an improvement.\n",
        "                # No, `best_f_this_bo_run` is the best *during this specific BO run*.\n",
        "                # We need to compare to `current_best_f_in_ls_iter` which should be the obj of `current_X_ls`\n",
        "                # if we haven't found a better U for it yet.\n",
        "                # Let's initialize `current_best_f_in_ls_iter` with the objective of `current_X_ls` (U=zeros)\n",
        "                if ls_iter == 0 and bo_iter == 0 and i ==0: # At very start, or start of new LS with new X\n",
        "                    obj_of_current_X_ls = evaluate_objective(np.zeros(T, dtype=int), current_X_ls, v_star, convolutions, d, w)\n",
        "                    current_best_f_in_ls_iter = obj_of_current_X_ls\n",
        "\n",
        "                if next_f_bo < current_best_f_in_ls_iter:\n",
        "                    print(f\"    BO found a U that improves upon current_X_ls (Obj: {next_f_bo:.4f} < {current_best_f_in_ls_iter:.4f}).\")\n",
        "                    current_best_f_in_ls_iter = next_f_bo # Update best for this LS iter\n",
        "                    # This U (next_U_bo) is the candidate for the next X\n",
        "                    # We can decide to break BO here and update X, or continue BO for best improvement.\n",
        "                    # For \"first improvement\" local search:\n",
        "                    # break # This break will exit the loop over next_U_candidates_np_bo\n",
        "                    pass # Let BO finish its batch to make a more informed choice for this LS step\n",
        "\n",
        "        # h. Augment Dataset for this BO run\n",
        "        evaluated_U_np_list_bo.extend(newly_evaluated_U_bo)\n",
        "        evaluated_f_vals_bo.extend(newly_evaluated_f_bo)\n",
        "        train_Y_list_bo.extend(newly_evaluated_neg_f_bo)\n",
        "\n",
        "        iter_time_bo = time.time() - start_time_bo\n",
        "        print(f\"  Best objective for current X in this BO run: {best_f_this_bo_run:.4f}\")\n",
        "        print(f\"  BO Iteration {bo_iter + 1} completed in {iter_time_bo:.2f} sec.\")\n",
        "\n",
        "        # if improvement_found_in_bo and \"first improvement\" was desired from the batch:\n",
        "        #    break # This break will exit the BO iterations loop\n",
        "\n",
        "    # --- End of BO Loop for current_X_ls ---\n",
        "    print(f\"\\nFinished BO for LS Iteration {ls_iter + 1}.\")\n",
        "    print(f\"Best U found by BO for current_X_ls: {best_U_this_bo_run}, Obj: {best_f_this_bo_run:.4f}\")\n",
        "\n",
        "    # Compare the best result from this BO run (best_f_this_bo_run for current_X_ls)\n",
        "    # with the overall best found so far in the local search (best_overall_f_ls)\n",
        "    # The \"move\" in local search happens if `best_f_this_bo_run` is better than\n",
        "    # the objective of `current_X_ls` (using U=zeros for modifications).\n",
        "    obj_of_current_X_ls_no_mod = evaluate_objective(np.zeros(T, dtype=int), current_X_ls, v_star, convolutions, d, w)\n",
        "\n",
        "    if best_f_this_bo_run < obj_of_current_X_ls_no_mod - CONVERGENCE_TOLERANCE:\n",
        "        print(f\"Improvement found! New best obj {best_f_this_bo_run:.4f} < obj of current X {obj_of_current_X_ls_no_mod:.4f}\")\n",
        "        Y_improved = current_X_ls + np.sum(v_star[best_U_this_bo_run == 1, :], axis=0)\n",
        "\n",
        "        # Check feasibility of the new Y before accepting the move\n",
        "        is_Y_feasible = np.all(Y_improved >= 0)\n",
        "        if not is_Y_feasible:\n",
        "            # Re-evaluate with penalty if the direct evaluation didn't catch it\n",
        "            # (depends on how evaluate_objective handles feasibility internally)\n",
        "            # This check is mostly for sanity, evaluate_objective should handle it.\n",
        "            obj_of_Y_improved_check = evaluate_objective(best_U_this_bo_run, current_X_ls, v_star, convolutions, d, w)\n",
        "            if obj_of_Y_improved_check >= LARGE_PENALTY:\n",
        "                 print(f\"  Proposed Y from U={best_U_this_bo_run} is INFEASIBLE. Obj={obj_of_Y_improved_check:.4f}. Rejecting move.\")\n",
        "                 no_improvement_streak += 1\n",
        "            else: # It was feasible or penalty was low enough\n",
        "                print(f\"  Moving to new state Y: {Y_improved}\")\n",
        "                current_X_ls = np.copy(Y_improved) # Update X for the next LS iteration\n",
        "                no_improvement_streak = 0\n",
        "        else:\n",
        "            print(f\"  Moving to new state Y: {Y_improved}\")\n",
        "            current_X_ls = np.copy(Y_improved) # Update X for the next LS iteration\n",
        "            no_improvement_streak = 0\n",
        "\n",
        "        # Update overall best if this new state (Y_improved) is the best ever\n",
        "        # The objective of Y_improved *as a new starting state X* is evaluate_objective(zeros, Y_improved, ...)\n",
        "        obj_of_new_X_state = evaluate_objective(np.zeros(T, dtype=int), current_X_ls, v_star, convolutions, d, w)\n",
        "        if obj_of_new_X_state < best_overall_f_ls:\n",
        "            best_overall_f_ls = obj_of_new_X_state\n",
        "            best_overall_U_ls = np.zeros(T, dtype=int) # U is relative to X, so for the new X, U is initially zeros\n",
        "            best_overall_Y_ls = np.copy(current_X_ls)\n",
        "            print(f\"    New OVERALL best state found. Obj of new X: {best_overall_f_ls:.4f}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"No improvement over current_X_ls (Obj {obj_of_current_X_ls_no_mod:.4f} vs BO best {best_f_this_bo_run:.4f}). Staying at current X.\")\n",
        "        no_improvement_streak += 1\n",
        "\n",
        "    if no_improvement_streak >= NO_IMPROVEMENT_STREAK_LIMIT:\n",
        "        print(f\"No improvement for {no_improvement_streak} local search iterations. Stopping.\")\n",
        "        break\n",
        "    if ls_iter > 0 and abs(best_f_this_bo_run - obj_of_current_X_ls_no_mod) < CONVERGENCE_TOLERANCE and best_f_this_bo_run < obj_of_current_X_ls_no_mod :\n",
        "        # This condition is a bit tricky. If BO found something slightly better but very close,\n",
        "        # and we made the move, the next obj_of_current_X_ls_no_mod will be that new value.\n",
        "        # True convergence is when BO cannot find a U that makes current_X_ls + U_vstar better than current_X_ls.\n",
        "        pass # Rely on no_improvement_streak for now.\n",
        "\n",
        "# --- End of Local Search Loop ---\n",
        "\n",
        "print(f\"\\n\\n{'='*10} LOCAL SEARCH FINISHED {'='*10}\")\n",
        "if best_overall_Y_ls is not None:\n",
        "    print(f\"Best Overall Objective (of the final X state): {best_overall_f_ls:.4f}\")\n",
        "    # print(f\"Best Overall U (relative to the X that produced it): {best_overall_U_ls}\") # This U is just zeros for the final X\n",
        "    print(f\"Best Overall Y (final state vector): {best_overall_Y_ls}\")\n",
        "\n",
        "    # Verification of the final best_overall_Y_ls\n",
        "    print(\"\\n--- Verification of Final Best Local Search State ---\")\n",
        "    is_feasible_final = np.all(best_overall_Y_ls >= 0)\n",
        "    recalculated_obj_final = LARGE_PENALTY\n",
        "    if is_feasible_final:\n",
        "        # Objective of this state is when U=zeros (no further modifications)\n",
        "        ewt_f, esp_f = calculate_objective_serv_time_lookup(best_overall_Y_ls, d, convolutions)\n",
        "        recalculated_obj_final = w * ewt_f + (1 - w) * esp_f\n",
        "    print(f\"Is the best Y_ls feasible? {is_feasible_final}\")\n",
        "    if is_feasible_final:\n",
        "        print(f\"Objective value of best Y_ls (recalculated): {recalculated_obj_final:.4f}\")\n",
        "        if not np.isclose(best_overall_f_ls, recalculated_obj_final):\n",
        "            print(f\"Warning: Stored best LS objective ({best_overall_f_ls:.4f}) \"\n",
        "                  f\"does not match recalculation ({recalculated_obj_final:.4f})!\")\n",
        "    elif best_overall_f_ls < LARGE_PENALTY:\n",
        "        print(f\"Warning: Best LS objective ({best_overall_f_ls:.4f}) is not penalty, but feasibility check failed for Y.\")\n",
        "    else:\n",
        "        print(\"Best LS solution found corresponds to an infeasible penalty value.\")\n",
        "\n",
        "else:\n",
        "    print(\"Local search did not find any valid state or no iterations were run meaningfully.\")\n",
        "end_time = time.time()\n",
        "print(f\"Total time taken for Local Search: {end_time - start_time:.2f} seconds\")"
      ],
      "id": "826049ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Experiment 2:\n"
      ],
      "id": "3f456268"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Local Search with Embedded BO (Stricter First Improvement) ---\n",
        "start_time = time.time()\n",
        "MAX_LOCAL_SEARCH_ITERATIONS = 20\n",
        "CONVERGENCE_TOLERANCE = 1e-8\n",
        "NO_IMPROVEMENT_STREAK_LIMIT = 5\n",
        "\n",
        "KAPPA = 2.576\n",
        "N_INITIAL_BO = 50\n",
        "N_ITERATIONS_BO = 20 # Max BO iterations *if no improvement is found earlier*\n",
        "BATCH_SIZE_q = 5     # Number of candidates BO proposes at once\n",
        "m = math.ceil(T/2) if T > 0 else 1\n",
        "NUM_CANDIDATES_Acqf = (T * 2 * 1024) if T > 0 else 100 # Ensure Acqf has candidates even if T=0 (though U will be empty)\n",
        "\n",
        "\n",
        "current_X_ls = np.copy(X)\n",
        "best_overall_f_ls = float('inf')\n",
        "best_overall_Y_ls = None\n",
        "no_improvement_streak = 0\n",
        "\n",
        "# Initial evaluation of the starting X state\n",
        "obj_of_initial_X = evaluate_objective(np.zeros(T, dtype=int) if T > 0 else np.array([]), current_X_ls, v_star, convolutions, d, w)\n",
        "best_overall_f_ls = obj_of_initial_X\n",
        "best_overall_Y_ls = np.copy(current_X_ls)\n",
        "print(f\"Starting Local Search. Initial X obj: {best_overall_f_ls:.4f}\")\n",
        "\n",
        "\n",
        "for ls_iter in range(MAX_LOCAL_SEARCH_ITERATIONS):\n",
        "    print(f\"\\n\\n{'='*10} LOCAL SEARCH ITERATION {ls_iter + 1}/{MAX_LOCAL_SEARCH_ITERATIONS} {'='*10}\")\n",
        "    print(f\"Current X for this LS iteration (Obj: {evaluate_objective(np.zeros(T, dtype=int) if T > 0 else np.array([]), current_X_ls, v_star, convolutions, d, w):.4f})\")\n",
        "\n",
        "    # The baseline to beat in this LS iteration is the objective of current_X_ls with no modifications\n",
        "    baseline_obj_for_ls_iter = evaluate_objective(np.zeros(T, dtype=int) if T > 0 else np.array([]), current_X_ls, v_star, convolutions, d, w)\n",
        "    print(f\"  Baseline objective to beat for current X: {baseline_obj_for_ls_iter:.4f}\")\n",
        "\n",
        "    bo_found_improving_U = False\n",
        "    best_U_from_bo_for_current_X = np.zeros(T, dtype=int) if T > 0 else np.array([])\n",
        "    best_f_from_bo_for_current_X = baseline_obj_for_ls_iter # Start with baseline\n",
        "\n",
        "    # --- BO Loop (inner loop) ---\n",
        "    evaluated_U_np_list_bo = []\n",
        "    evaluated_f_vals_bo = []\n",
        "    train_Y_list_bo = []\n",
        "\n",
        "    # 1. Initialization for BO\n",
        "    if T > 0: # Only generate U candidates if T > 0\n",
        "        bo_initial_points = [np.random.randint(0, 2, size=T) for _ in range(N_INITIAL_BO)]\n",
        "        bo_initial_points.insert(0, np.zeros(T, dtype=int)) # Add U=zeros as the first point\n",
        "        if not any(np.any(u) for u in bo_initial_points) and N_INITIAL_BO > 0 :\n",
        "            idx_to_set = np.random.randint(T)\n",
        "            bo_initial_points[0][idx_to_set] = 1\n",
        "    else: # T=0\n",
        "        bo_initial_points = [np.array([])] # U is empty if T=0\n",
        "\n",
        "    for U_init in bo_initial_points:\n",
        "        f_val = evaluate_objective(U_init, current_X_ls, v_star, convolutions, d, w)\n",
        "        neg_f_val = -f_val\n",
        "        evaluated_U_np_list_bo.append(U_init)\n",
        "        evaluated_f_vals_bo.append(f_val)\n",
        "        train_Y_list_bo.append(neg_f_val)\n",
        "        if f_val < best_f_from_bo_for_current_X:\n",
        "            best_f_from_bo_for_current_X = f_val\n",
        "            best_U_from_bo_for_current_X = U_init\n",
        "            if f_val < baseline_obj_for_ls_iter - CONVERGENCE_TOLERANCE:\n",
        "                print(f\"  BO Init found immediate improver U={U_init}, Obj={f_val:.4f} (beats {baseline_obj_for_ls_iter:.4f})\")\n",
        "                bo_found_improving_U = True\n",
        "                break # Exit BO initialization loop\n",
        "\n",
        "    if bo_found_improving_U: # If init found improvement, skip further BO iterations\n",
        "        pass # Proceed to LS decision block\n",
        "    else: # Continue with BO iterations if no immediate improvement from init\n",
        "        for bo_iter in range(N_ITERATIONS_BO):\n",
        "            print(f\"\\n--- BO Iteration {bo_iter + 1}/{N_ITERATIONS_BO} (LS Iter {ls_iter + 1}) ---\")\n",
        "            start_time_bo = time.time()\n",
        "\n",
        "            current_dictionary_A = generate_diverse_random_dictionary(T, m)\n",
        "\n",
        "            if not evaluated_U_np_list_bo: continue\n",
        "            evaluated_U_np_array_bo = np.array(evaluated_U_np_list_bo, dtype=object) # dtype=object for ragged arrays if T=0\n",
        "            # Ensure U vectors are 2D for embed_batch, even if T=0 (N,0)\n",
        "            if T == 0 and evaluated_U_np_array_bo.ndim == 1:\n",
        "                 evaluated_U_np_array_bo = evaluated_U_np_array_bo.reshape(-1,0)\n",
        "\n",
        "\n",
        "            embedded_train_X_bo = embed_batch(evaluated_U_np_array_bo, current_dictionary_A)\n",
        "\n",
        "            scaler_bo = MinMaxScaler()\n",
        "            if embedded_train_X_bo.shape[0] > 0:\n",
        "                embedded_train_X_scaled_bo = scaler_bo.fit_transform(embedded_train_X_bo)\n",
        "            else:\n",
        "                embedded_train_X_scaled_bo = embedded_train_X_bo\n",
        "\n",
        "            train_Y_for_fit_bo = np.array(train_Y_list_bo)\n",
        "\n",
        "            if embedded_train_X_scaled_bo.shape[0] > 0 and \\\n",
        "               train_Y_for_fit_bo.shape[0] == embedded_train_X_scaled_bo.shape[0]:\n",
        "                try:\n",
        "                    gp_model_bo = get_fitted_model(embedded_train_X_scaled_bo, train_Y_for_fit_bo, m)\n",
        "                except ValueError as e:\n",
        "                    print(f\"Warning: GP fitting error: {e}. Skipping BO iteration.\")\n",
        "                    continue\n",
        "            else:\n",
        "                print(\"Warning: Not enough/mismatched data for GP. Skipping BO iteration.\")\n",
        "                continue\n",
        "\n",
        "            next_U_candidates_np_bo = optimize_acqf_discrete_via_embedding(\n",
        "                gp_model=gp_model_bo, scaler=scaler_bo, dictionary_A=current_dictionary_A,\n",
        "                T=T, q=BATCH_SIZE_q, num_candidates=NUM_CANDIDATES_Acqf, kappa=KAPPA\n",
        "            )\n",
        "            if next_U_candidates_np_bo.shape[0] == 0:\n",
        "                print(\"Acquisition function yielded no new candidates. Continuing BO.\")\n",
        "                continue\n",
        "\n",
        "            newly_evaluated_U_bo = []\n",
        "            newly_evaluated_f_bo = []\n",
        "            newly_evaluated_neg_f_bo = []\n",
        "\n",
        "            for i_cand in range(next_U_candidates_np_bo.shape[0]):\n",
        "                next_U_bo = next_U_candidates_np_bo[i_cand, :]\n",
        "                already_evaluated_this_bo = any(np.array_equal(next_U_bo, u) for u in evaluated_U_np_list_bo)\n",
        "                if already_evaluated_this_bo: continue\n",
        "\n",
        "                next_f_bo = evaluate_objective(next_U_bo, current_X_ls, v_star, convolutions, d, w)\n",
        "                next_neg_f_bo = -next_f_bo\n",
        "                # temp_Y_bo = current_X_ls + (np.sum(v_star[next_U_bo == 1, :], axis=0) if T > 0 else 0)\n",
        "                print(f\"  BO Acqf Candidate {i_cand}: Obj = {next_f_bo:.4f}\")\n",
        "\n",
        "                newly_evaluated_U_bo.append(next_U_bo)\n",
        "                newly_evaluated_f_bo.append(next_f_bo)\n",
        "                newly_evaluated_neg_f_bo.append(next_neg_f_bo)\n",
        "\n",
        "                if next_f_bo < best_f_from_bo_for_current_X: # Update best found in this BO run\n",
        "                    best_f_from_bo_for_current_X = next_f_bo\n",
        "                    best_U_from_bo_for_current_X = next_U_bo\n",
        "\n",
        "                if next_f_bo < baseline_obj_for_ls_iter - CONVERGENCE_TOLERANCE:\n",
        "                    print(f\"    BO Acqf found immediate improver U={next_U_bo}, Obj={next_f_bo:.4f} (beats {baseline_obj_for_ls_iter:.4f})\")\n",
        "                    bo_found_improving_U = True\n",
        "                    # Update best_U_from_bo_for_current_X one last time with this specific improver\n",
        "                    best_U_from_bo_for_current_X = next_U_bo\n",
        "                    best_f_from_bo_for_current_X = next_f_bo\n",
        "                    break # Exit loop evaluating candidates from acquisition function\n",
        "\n",
        "            evaluated_U_np_list_bo.extend(newly_evaluated_U_bo)\n",
        "            evaluated_f_vals_bo.extend(newly_evaluated_f_bo)\n",
        "            train_Y_list_bo.extend(newly_evaluated_neg_f_bo)\n",
        "\n",
        "            iter_time_bo = time.time() - start_time_bo\n",
        "            print(f\"  Best obj for current X in this BO run so far: {best_f_from_bo_for_current_X:.4f}\")\n",
        "            print(f\"  BO Iteration {bo_iter + 1} completed in {iter_time_bo:.2f} sec.\")\n",
        "\n",
        "            if bo_found_improving_U:\n",
        "                break # Exit BO iterations loop as an improver was found\n",
        "\n",
        "    # --- End of BO Loop for current_X_ls ---\n",
        "    print(f\"\\nFinished BO for LS Iteration {ls_iter + 1}.\")\n",
        "\n",
        "    if bo_found_improving_U:\n",
        "        print(f\"BO found an improving U: {best_U_from_bo_for_current_X}, Obj: {best_f_from_bo_for_current_X:.4f}\")\n",
        "        Y_improved = current_X_ls + (np.sum(v_star[best_U_from_bo_for_current_X == 1, :], axis=0) if T > 0 else 0)\n",
        "\n",
        "        # Sanity check feasibility of Y_improved (evaluate_objective should handle penalties)\n",
        "        obj_of_Y_improved_check = evaluate_objective(np.zeros(T,dtype=int) if T > 0 else np.array([]), Y_improved, v_star, convolutions, d, w)\n",
        "\n",
        "        if obj_of_Y_improved_check >= LARGE_PENALTY and best_f_from_bo_for_current_X >= LARGE_PENALTY : # Check if the *state* Y_improved is penalized\n",
        "            print(f\"  Proposed Y from U={best_U_from_bo_for_current_X} results in a penalized state. Obj={obj_of_Y_improved_check:.4f}. Rejecting move.\")\n",
        "            no_improvement_streak += 1\n",
        "        else:\n",
        "            print(f\"  Improvement accepted. Moving to new state Y: (Obj of Y state: {obj_of_Y_improved_check:.4f})\")\n",
        "            current_X_ls = np.copy(Y_improved)\n",
        "            no_improvement_streak = 0\n",
        "\n",
        "            # Update overall best if this new state is the best ever\n",
        "            if obj_of_Y_improved_check < best_overall_f_ls:\n",
        "                best_overall_f_ls = obj_of_Y_improved_check\n",
        "                best_overall_Y_ls = np.copy(current_X_ls)\n",
        "                print(f\"    New OVERALL best state found. Obj of new X: {best_overall_f_ls:.4f}\")\n",
        "    else:\n",
        "        print(f\"BO did not find a U that improves upon current_X_ls (baseline: {baseline_obj_for_ls_iter:.4f}, BO best for X: {best_f_from_bo_for_current_X:.4f}).\")\n",
        "        no_improvement_streak += 1\n",
        "\n",
        "    if no_improvement_streak >= NO_IMPROVEMENT_STREAK_LIMIT:\n",
        "        print(f\"No improvement for {no_improvement_streak} local search iterations. Stopping.\")\n",
        "        break\n",
        "\n",
        "# --- End of Local Search Loop ---\n",
        "# (Results and Verification part remains the same as your previous version, using best_overall_f_ls and best_overall_Y_ls)\n",
        "print(f\"\\n\\n{'='*10} LOCAL SEARCH FINISHED {'='*10}\")\n",
        "if best_overall_Y_ls is not None:\n",
        "    print(f\"Best Overall Objective (of the final X state): {best_overall_f_ls:.4f}\")\n",
        "    print(f\"Best Overall Y (final state vector): {best_overall_Y_ls}\")\n",
        "\n",
        "    # Verification of the final best_overall_Y_ls\n",
        "    print(\"\\n--- Verification of Final Best Local Search State ---\")\n",
        "    is_feasible_final = np.all(best_overall_Y_ls >= 0) # Basic feasibility\n",
        "    recalculated_obj_final = LARGE_PENALTY\n",
        "    # The true objective of the state Y_ls is when U is all zeros for it\n",
        "    recalculated_obj_final = evaluate_objective(np.zeros(T,dtype=int) if T > 0 else np.array([]), best_overall_Y_ls, v_star, convolutions, d, w)\n",
        "\n",
        "\n",
        "    print(f\"Is the best Y_ls feasible (according to its obj value < LARGE_PENALTY)? {recalculated_obj_final < LARGE_PENALTY}\")\n",
        "    if recalculated_obj_final < LARGE_PENALTY:\n",
        "        print(f\"Objective value of best Y_ls (recalculated from state): {recalculated_obj_final:.4f}\")\n",
        "        if not np.isclose(best_overall_f_ls, recalculated_obj_final):\n",
        "            print(f\"Warning: Stored best LS objective ({best_overall_f_ls:.4f}) \"\n",
        "                  f\"does not match recalculation ({recalculated_obj_final:.4f})!\")\n",
        "    else:\n",
        "        print(f\"Best LS solution state has a penalty objective: {recalculated_obj_final:.4f}\")\n",
        "else:\n",
        "    print(\"Local search did not find any valid state or no iterations were run meaningfully.\")\n",
        "    \n",
        "end_time = time.time()\n",
        "print(f\"Total time taken for Local Search: {end_time - start_time:.2f} seconds\")"
      ],
      "id": "33326af0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6. Experiment 3:\n",
        "\n",
        "```{}\n",
        "```\n",
        "\n",
        "## Results\n",
        "\n",
        "### Experiment 1: \n",
        "\n",
        "### Experiment 2: \n",
        "\n",
        "### Summary of Best ObjectivesDiscussion\n",
        "\n",
        "1.  **Performance Comparison**:\n",
        "\n",
        "2.  **Hypothesis Evaluation**:\n",
        "\n",
        "    -   Hypothesis 1\n",
        "    -   Hypothesis 2\n",
        "\n",
        "3.  **Exploration vs. Exploitation**:\n",
        "\n",
        "4.  **Computational Effort**:\n",
        "\n",
        "5.  **Limitations and Future Work**:\n",
        "\n",
        "    -   The optimality guarantee mentioned by @kaandorp_optimal_2007 applies to their specific local search algorithm operating directly on the schedule space $\\mathcal{F}$, leveraging multimodularity. Our BO approach operates on the perturbation vector space $\\mathbf{U}$ via HED embeddings. While BO aims for global optimization, it doesn't inherit the same theoretical guarantee of finding the global optimum as the original local search, especially given the stochastic nature of GP modeling and acquisition function optimization.\n",
        "    -   The performance is likely sensitive to BO hyperparameters (dictionary size $m$, $\\kappa$ values, number of candidates for acquisition optimization).\n",
        "    -   Further investigation into different dictionary construction methods (e.g., binary wavelets as mentioned in Deshwal et al., 2023) or adaptive $\\kappa$ schedules could be beneficial.\n",
        "\n",
        "## Timeline\n",
        "\n",
        "-   **Experiment Setup and Code Implementation**: 09-05-2025\n",
        "-   **Results Analysis and Report Compilation**:\n",
        "\n",
        "## References\n",
        "\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "c12ec3a3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}