@article{bailey1952study,
  title={A study of queues and appointment systems in hospital out-patient departments, with special reference to waiting-times},
  author={Bailey, Norman TJ},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={14},
  number={2},
  pages={185--199},
  year={1952},
  publisher={Oxford University Press}
}

@article{kaandorp_optimal_2007,
	title = {Optimal outpatient appointment scheduling},
	volume = {10},
	issn = {1386-9620, 1572-9389},
	url = {https://link.springer.com/10.1007/s10729-007-9015-x},
	doi = {10.1007/s10729-007-9015-x},
	abstract = {In this paper optimal outpatient appointment scheduling is studied. A local search procedure is derived that converges to the optimal schedule with a weighted average of expected waiting times of patients, idle time of the doctor and tardiness (lateness) as objective. No-shows are allowed to happen. For certain combinations of parameters the well-known Bailey-Welch rule is found to be the optimal appointment schedule.},
	language = {en},
	number = {3},
	urldate = {2022-08-27},
	journal = {Health Care Management Science},
	author = {Kaandorp, Guido C. and Koole, Ger},
	month = sep,
	year = {2007},
	pages = {217--229},
	file = {Kaandorp and Koole - 2007 - Optimal outpatient appointment scheduling.pdf:/Users/witoldtenhove/Zotero/storage/K9VMKYDQ/Kaandorp and Koole - 2007 - Optimal outpatient appointment scheduling.pdf:application/pdf},
}

@article{koeleman_optimal_2012,
	title = {Optimal outpatient appointment scheduling with emergency arrivals and general service times},
	volume = {2},
	issn = {1948-8300, 1948-8319},
	url = {http://www.tandfonline.com/doi/abs/10.1080/19488300.2012.665154},
	doi = {10.1080/19488300.2012.665154},
	abstract = {In this paper we study the problem of deciding at what times to schedule patients when there are emergency arrivals following a non-stationary Poisson process. The service times can be any given distribution. The objective function consists of a weighted sum of the waiting times, idle time and tardiness. We prove that this objective function is multimodular, and then use a local search algorithm which in that case is guaranteed to ﬁnd the optimal solution. Numerical examples show that this method gives considerable improvements over the standard even-spaced schedule, and that the schedules for di↵erent service time distributions can look quite di↵erent.},
	language = {en},
	number = {1},
	urldate = {2022-08-27},
	journal = {IIE Transactions on Healthcare Systems Engineering},
	author = {Koeleman, Paulien M. and Koole, Ger M.},
	month = jan,
	year = {2012},
	pages = {14--30},
	file = {Koeleman and Koole - 2012 - Optimal outpatient appointment scheduling with eme.pdf:/Users/witoldtenhove/Zotero/storage/K9ZVMBTX/Koeleman and Koole - 2012 - Optimal outpatient appointment scheduling with eme.pdf:application/pdf},
}


@inproceedings{deshwal_bayesian_2023,
	title = {Bayesian {Optimization} over {High}-{Dimensional} {Combinatorial} {Spaces} via {Dictionary}-based {Embeddings}},
	url = {https://proceedings.mlr.press/v206/deshwal23a.html},
	abstract = {We consider the problem of optimizing expensive black-box functions over high-dimensional combinatorial spaces which arises in many science, engineering, and ML applications. We use Bayesian Optimization (BO) and propose a novel surrogate modeling approach for efficiently handling a large number of binary and categorical parameters. The key idea is to select a number of discrete structures from the input space (the dictionary) and use them to define an ordinal embedding for high-dimensional combinatorial structures. This allows us to use existing Gaussian process models for continuous spaces. We develop a principled approach based on binary wavelets to construct dictionaries for binary spaces, and propose a randomized construction method that generalizes to categorical spaces. We provide theoretical justification to support the effectiveness of the dictionary-based embeddings. Our experiments on diverse real-world benchmarks demonstrate the effectiveness of our proposed surrogate modeling approach over state-of-the-art BO methods.},
	language = {en},
	urldate = {2025-05-05},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Deshwal, Aryan and Ament, Sebastian and Balandat, Maximilian and Bakshy, Eytan and Doppa, Janardhan Rao and Eriksson, David},
	month = apr,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {7021--7039},
	file = {Full Text PDF:/Users/witoldtenhove/Zotero/storage/TL7FQNJU/Deshwal et al. - 2023 - Bayesian Optimization over High-Dimensional Combinatorial Spaces via Dictionary-based Embeddings.pdf:application/pdf},
}

@inproceedings{swersky_amortized_2020,
	title = {Amortized {Bayesian} {Optimization} over {Discrete} {Spaces}},
	url = {https://proceedings.mlr.press/v124/swersky20a.html},
	abstract = {Bayesian optimization is a principled approach for globally optimizing expensive, black-box functions by using a surrogate model of the objective. However, each step of Bayesian optimization involves solving an inner optimization problem, in which we maximize an acquisition function  derived from the surrogate model to decide where to query next. This inner problem can be challenging to solve, particularly in discrete spaces, such as protein sequences or molecular graphs, where gradient-based optimization cannot be used. Our key insight is that we can train a generative model to generate candidates that maximize the acquisition function. This is faster than standard model-free local search methods, since we can amortize the cost of learning the model across multiple rounds of Bayesian optimization. We therefore call this Amortized Bayesian Optimization. On several challenging discrete design problems, we show this method generally outperforms other methods at optimizing the inner acquisition function, resulting in more efficient optimization of the outer black-box objective.},
	language = {en},
	urldate = {2025-04-29},
	booktitle = {Proceedings of the 36th {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	publisher = {PMLR},
	author = {Swersky, Kevin and Rubanova, Yulia and Dohan, David and Murphy, Kevin},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {769--778},
	file = {Full Text PDF:/Users/witoldtenhove/Zotero/storage/TL8NDMUC/Swersky et al. - 2020 - Amortized Bayesian Optimization over Discrete Spaces.pdf:application/pdf},
}

@misc{aglietti_funbo_2024,
	title = {{FunBO}: {Discovering} {Acquisition} {Functions} for {Bayesian} {Optimization} with {FunSearch}},
	shorttitle = {{FunBO}},
	url = {http://arxiv.org/abs/2406.04824},
	doi = {10.48550/arXiv.2406.04824},
	abstract = {The sample efficiency of Bayesian optimization algorithms depends on carefully crafted acquisition functions (AFs) guiding the sequential collection of function evaluations. The best-performing AF can vary significantly across optimization problems, often requiring ad-hoc and problem-specific choices. This work tackles the challenge of designing novel AFs that perform well across a variety of experimental settings. Based on FunSearch, a recent work using Large Language Models (LLMs) for discovery in mathematical sciences, we propose FunBO, an LLM-based method that can be used to learn new AFs written in computer code by leveraging access to a limited number of evaluations for a set of objective functions. We provide the analytic expression of all discovered AFs and evaluate them on various global optimization benchmarks and hyperparameter optimization tasks. We show how FunBO identifies AFs that generalize well in and out of the training distribution of functions, thus outperforming established general-purpose AFs and achieving competitive performance against AFs that are customized to specific function types and are learned via transfer-learning algorithms.},
	urldate = {2025-05-19},
	publisher = {arXiv},
	author = {Aglietti, Virginia and Ktena, Ira and Schrouff, Jessica and Sgouritsa, Eleni and Ruiz, Francisco J. R. and Malek, Alan and Bellot, Alexis and Chiappa, Silvia},
	month = jul,
	year = {2024},
	note = {arXiv:2406.04824 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Full Text PDF:/Users/witoldtenhove/Zotero/storage/6ZG7Y3TZ/Aglietti et al. - 2024 - FunBO Discovering Acquisition Functions for Bayesian Optimization with FunSearch.pdf:application/pdf;Snapshot:/Users/witoldtenhove/Zotero/storage/KYH6SZHJ/2406.html:text/html},
}

@misc{zhang_pabbo_2025,
	title = {{PABBO}: {Preferential} {Amortized} {Black}-{Box} {Optimization}},
	shorttitle = {{PABBO}},
	url = {http://arxiv.org/abs/2503.00924},
	doi = {10.48550/arXiv.2503.00924},
	abstract = {Preferential Bayesian Optimization (PBO) is a sample-efficient method to learn latent user utilities from preferential feedback over a pair of designs. It relies on a statistical surrogate model for the latent function, usually a Gaussian process, and an acquisition strategy to select the next candidate pair to get user feedback on. Due to the non-conjugacy of the associated likelihood, every PBO step requires a significant amount of computations with various approximate inference techniques. This computational overhead is incompatible with the way humans interact with computers, hindering the use of PBO in real-world cases. Building on the recent advances of amortized BO, we propose to circumvent this issue by fully amortizing PBO, meta-learning both the surrogate and the acquisition function. Our method comprises a novel transformer neural process architecture, trained using reinforcement learning and tailored auxiliary losses. On a benchmark composed of synthetic and real-world datasets, our method is several orders of magnitude faster than the usual Gaussian process-based strategies and often outperforms them in accuracy.},
	urldate = {2025-05-19},
	publisher = {arXiv},
	author = {Zhang, Xinyu and Huang, Daolang and Kaski, Samuel and Martinelli, Julien},
	month = mar,
	year = {2025},
	note = {arXiv:2503.00924 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 25 pages, 17 figures. Accepted at the Thirteenth International Conference on Learning Representations (ICLR 2025)},
	file = {Full Text PDF:/Users/witoldtenhove/Zotero/storage/P39T585N/Zhang et al. - 2025 - PABBO Preferential Amortized Black-Box Optimization.pdf:application/pdf;Snapshot:/Users/witoldtenhove/Zotero/storage/8VT95ALF/2503.html:text/html},
}

@misc{gonzalez_preferential_2017,
	title = {Preferential {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1704.03651},
	doi = {10.48550/arXiv.1704.03651},
	abstract = {Bayesian optimization (BO) has emerged during the last few years as an effective approach to optimizing black-box functions where direct queries of the objective are expensive. In this paper we consider the case where direct access to the function is not possible, but information about user preferences is. Such scenarios arise in problems where human preferences are modeled, such as A/B tests or recommender systems. We present a new framework for this scenario that we call Preferential Bayesian Optimization (PBO) which allows us to find the optimum of a latent function that can only be queried through pairwise comparisons, the so-called duels. PBO extends the applicability of standard BO ideas and generalizes previous discrete dueling approaches by modeling the probability of the winner of each duel by means of a Gaussian process model with a Bernoulli likelihood. The latent preference function is used to define a family of acquisition functions that extend usual policies used in BO. We illustrate the benefits of PBO in a variety of experiments, showing that PBO needs drastically fewer comparisons for finding the optimum. According to our experiments, the way of modeling correlations in PBO is key in obtaining this advantage.},
	urldate = {2025-05-19},
	publisher = {arXiv},
	author = {Gonzalez, Javier and Dai, Zhenwen and Damianou, Andreas and Lawrence, Neil D.},
	month = apr,
	year = {2017},
	note = {arXiv:1704.03651 [stat]},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: 10 pages, 6 figures},
	file = {Full Text PDF:/Users/witoldtenhove/Zotero/storage/5BMG4BHA/Gonzalez et al. - 2017 - Preferential Bayesian Optimization.pdf:application/pdf;Snapshot:/Users/witoldtenhove/Zotero/storage/ILAN2WEK/1704.html:text/html},
}

@inproceedings{balandat2020botorch,
  title={{BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization}},
  author={Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
  booktitle = {Advances in Neural Information Processing Systems 33},
  year={2020},
  url = {http://arxiv.org/abs/1910.06403}
}


@article{altman_multimodularity_2000,
	title = {Multimodularity, {Convexity}, and {Optimization} {Properties}},
	volume = {25},
	issn = {0364-765X},
	url = {https://www.jstor.org/stable/3690584},
	abstract = {In this paper we investigate the properties of multimodular functions. In doing so we give elementary proofs for properties already established by Hajek and we generalize some of his results. In particular, we extend the relation between convexity and multimodularity to some convex subsets of {\textless}tex-math{\textgreater}\$\{{\textbackslash}Bbb Z\}{\textasciicircum}\{m\}\${\textless}/tex-math{\textgreater}. We also obtain general optimization results for average costs related to a sequence of multimodular functions rather than to a single function. Under this general context, we show that the expected average cost problem is optimized by using regular sequences. We finally illustrate the usefulness of this theory in admission control into a D/D/1 queue with fixed batch arrivals, with no state information. We show that the regular policy minimizes the average queue length for the case of an infinite queue, but not for the case of a finite queue. When further adding a constraint on the losses, it is shown that a regular policy is also optimal for the finite queue case.},
	number = {2},
	urldate = {2023-11-29},
	journal = {Mathematics of Operations Research},
	author = {Altman, Eitan and Gaujal, Bruno and Hordijk, Arie},
	year = {2000},
	note = {Publisher: INFORMS},
	pages = {324--347},
	file = {JSTOR Full Text PDF:/Users/witoldtenhove/Zotero/storage/5S8XZKWQ/Altman et al. - 2000 - Multimodularity, Convexity, and Optimization Prope.pdf:application/pdf},
}


