[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Appointment Scheduling Experiments",
    "section": "",
    "text": "To Do",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#to-do",
    "href": "index.html#to-do",
    "title": "Appointment Scheduling Experiments",
    "section": "",
    "text": "Add a brief introduction to the project\nEvaluator Functions\n\nDocument and test the evaluator functions\nRun experiments using the evaluator functions\n\nSearcher Functions\n\nDocument and test the search functions\nRun experiments using the search functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "function-testing.html",
    "href": "function-testing.html",
    "title": "2  Evaluator functions testing",
    "section": "",
    "text": "2.1 Objective\nIn this experiment we will test whether the functions for calculating the objective values work properly and efficiently.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#background",
    "href": "function-testing.html#background",
    "title": "2  Evaluator functions testing",
    "section": "2.2 Background",
    "text": "2.2 Background\nFor developing new methods for optimizing appointment schedules it is necessary that the function for calculating objective values works properly. It is also important that the function is efficient, as it will be used in optimization algorithms that will be run many times.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#hypothesis",
    "href": "function-testing.html#hypothesis",
    "title": "2  Evaluator functions testing",
    "section": "2.3 Hypothesis",
    "text": "2.3 Hypothesis\nThe functions for calculating that have been developed are working fast and generate correct results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#methodology",
    "href": "function-testing.html#methodology",
    "title": "2  Evaluator functions testing",
    "section": "2.4 Methodology",
    "text": "2.4 Methodology\n\n2.4.1 Tools and Materials\nFor testing the correct working of the functions used to calculate objective values we will compare the exact calculation to results from Monte Carlo (MC) simulations. The MC simulations allow modeling the system and replicating closely the actual process of patients arriving and being served. The exact calculation is based on the convolution of the service time distribution and the number of patients arriving at each time slot.\n\n\n2.4.2 Experimental Design\nWe will define some typical instances of schedules and calculate the objective values for them both using the exact method as well as through MC simulations. We will then compare the results.\n\n\n2.4.3 Variables\n\nIndependent Variables:\n\nDifferent instances of appointment schedules.\n\nDependent Variables:\n\nObjective value results from exact calculations and simulations.\nSpeed indicators\n\n\n\n\n2.4.4 Setup\nWe have defined the following test cases:\n\nimport numpy as np\nimport pandas as pd\nimport time\nimport plotly.graph_objects as go\nfrom functions import service_time_with_no_shows, compute_convolutions, compute_convolutions_fft, calculate_objective_serv_time_lookup\n\n# Parameters\nd = 5\nq = 0.1\n    \n# Create service time distribution\nservice_time = np.zeros(11)\nservice_time[3] = 0.2\nservice_time[5] = 0.3\nservice_time[8] = 0.5\n\naverage_service_time = np.dot(range(len(service_time)), service_time)\nprint(f\"Average service time: {average_service_time}\")\n    \n# Different schedule patterns with the same total number of patients (except for test schedule)\nschedules = [\n    (\"Calibration\", [2, 0, 0, 0, 0, 1]),\n    (\"Uniform\", [2, 2, 2, 2]),\n    (\"Decreasing\", [5, 2, 1, 0]),\n    (\"Increasing\", [0, 1, 2, 5]),\n    (\"Front-heavy\", [4, 4, 0, 0]),\n    (\"Back-heavy\", [0, 0, 4, 4]),\n    (\"Alternating\", [4, 0, 4, 0]),\n    (\"Bailey-rule\", [2, 1, 1, 1, 1, 1, 1])  # Schedule 2 initially, then 1 each slot\n]\n\n# Set number of simulations for Monte Carlo simulation\nnr_simulations = 1000\n\n# Create dictionary for storing results\nresults_dict = {'schedule_name': [], 'average_waiting_time': [], 'average_overtime': [], 'expected_waiting_time': [], 'expected_overtime': [], 'average_computation_time': []}\nresults_dict['schedule_name'] = [s[0] for s in schedules]\n\nAverage service time: 6.1\n\n\nThe “Calibration” test set is used to calibrate the simulation results with the exact results. For this schedule, the exact results can be easily calculated by hand. The average service time after correcting for no-shows for one patient is 5.49 (see below) and only the second patient in this example will experience waiting times. So the average expected waiting time is 5.49 / 3 = 1.83.\nThe expected overtime is is the expected spillover time caused by the last patient in the schedule. As there are no patients before the last patient in the last interval, the spillover time distribution is simply distribution of the event that the (adjusted) service time will exceed the interval time. For the case that overtime is 3 (8 - 5), the probability is 0.45 and for other values it is 0.55. So the expected overtime is 0.45 * 3 + 0.55 * 0 = 1.35.\nThe other test sets are used to examine the performance of the functions for different schedule patterns.\n\n\n2.4.5 Sample Size and Selection\nSample Size: - For each schedule instance we will run 1000 simulations.\nSample Selection: - During each simulation for each patient a random service time will be sampled from the distribution (adjusted for no-shows).\n\n\n2.4.6 Experimental Procedure\n\n2.4.6.1 Step 1: Adjust the service time distribution for no-shows.\n\n# Adjust service time distribution for no-shows and compare to original\nservice_time_no_shows = service_time_with_no_shows(service_time, q)\nprint(f\"Service time distribution with no-shows: {service_time_no_shows}\")\n\naverage_service_time_no_shows = np.dot(range(len(service_time_no_shows)), service_time_no_shows)\nprint(f\"Average service time with no-shows: {average_service_time_no_shows}\")\n\nService time distribution with no-shows: [0.1, 0.0, 0.0, 0.18000000000000002, 0.0, 0.27, 0.0, 0.0, 0.45, 0.0, 0.0]\nAverage service time with no-shows: 5.49\n\n\n\n\n2.4.6.2 Step 2: Monte carlo simulation\nFor each schedule instance:\n\nCalculate \\(N\\) and \\(T\\), the total number of patients and the total time of the schedule.\nFor each simulation:\n\nSample random service times for each of the \\(N\\) patient from service times distribution with no-shows.\nCalculate the the average waiting time and the overtime for the schedule using a Lindley recursion, starting at \\(t = 0\\) and ending at \\(t = T - 1\\).\n\n\n\nimport numpy as np\nfrom typing import List, Tuple, Union\n\ndef simulate_schedule(\n    schedule: List[int],\n    service_time_no_shows: Union[List[float], np.ndarray],\n    d: int,\n    nr_simulations: int\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Runs a Monte Carlo simulation for a single schedule.\n\n    This function simulates patient scheduling over multiple time slots and computes the average waiting \n    time per patient and the average overtime across all simulation iterations. Each time slot has a duration \n    threshold `d`. Service times for patients are sampled based on the provided probability mass function.\n\n    Parameters:\n        schedule (List[int]): A list where each element represents the number of patients scheduled in each time slot.\n        service_time_no_shows (Union[List[float], np.ndarray]): A probability mass function (PMF) for service times.\n        d (int): The duration threshold for a time slot.\n        nr_simulations (int): The number of simulation iterations to run.\n\n    Returns:\n        Tuple[float, float]: A tuple containing:\n            - The average waiting time per patient across simulations.\n            - The average overtime across simulations.\n    \"\"\"\n    N: int = sum(schedule)  # Total number of patients\n    T: int = len(schedule)  # Total number of time slots\n\n    total_waiting_time: float = 0.0\n    total_overtime: float = 0.0\n\n    for _ in range(nr_simulations):\n        cum_waiting_time: float = 0.0\n\n        # --- Process the first time slot ---\n        num_patients: int = schedule[0]\n        # Generate random service times for the first slot\n        sampled: np.ndarray = np.random.choice(\n            range(len(service_time_no_shows)),\n            size=num_patients,\n            p=service_time_no_shows\n        )\n\n        if num_patients == 0:\n            waiting_time: float = 0.0\n            spillover_time: float = 0.0\n        elif num_patients == 1:\n            waiting_time = 0.0\n            spillover_time = max(0, sampled[0])\n        else:\n            # For more than one patient, the waiting time is the cumulative sum\n            # of the service times for all but the last patient.\n            waiting_time = float(sum(np.cumsum(sampled[:-1])))\n            spillover_time = max(0, sum(sampled) - d)\n        cum_waiting_time += waiting_time\n\n        # --- Process the remaining time slots ---\n        for t in range(1, T):\n            num_patients = schedule[t]\n            # Generate random service times for time slot t\n            sampled = np.random.choice(\n                range(len(service_time_no_shows)),\n                size=num_patients,\n                p=service_time_no_shows\n            )\n            if num_patients == 0:\n                waiting_time = 0.0\n                spillover_time = max(0, spillover_time - d)\n            elif num_patients == 1:\n                waiting_time = spillover_time\n                spillover_time = max(0, spillover_time + sampled[0] - d)\n            else:\n                # Each patient waits the current spillover,\n                # plus additional waiting due to the service times of those ahead.\n                waiting_time = spillover_time * num_patients + sum(np.cumsum(sampled[:-1]))\n                spillover_time = max(0, spillover_time + sum(sampled) - d)\n            cum_waiting_time += waiting_time\n\n        # Accumulate normalized waiting time (per patient) and overtime\n        total_waiting_time += cum_waiting_time / N\n        total_overtime += spillover_time\n\n    avg_waiting_time: float = total_waiting_time / nr_simulations\n    avg_overtime: float = total_overtime / nr_simulations\n\n    return avg_waiting_time, avg_overtime\n\n\n# Loop through the schedules\nfor schedule_name, schedule in schedules:\n    N = sum(schedule)\n    T = len(schedule)\n    print(f\"Schedule: {schedule_name} {schedule}, N: {N}, T: {T}\")\n    \n    avg_waiting_time, avg_overtime = simulate_schedule(schedule, service_time_no_shows, d, nr_simulations)\n    \n    print(f\"Average waiting time: {avg_waiting_time}, average overtime: {avg_overtime}\")\n    results_dict['average_waiting_time'].append(avg_waiting_time)\n    results_dict['average_overtime'].append(avg_overtime)\n\nSchedule: Calibration [2, 0, 0, 0, 0, 1], N: 3, T: 6\nAverage waiting time: 1.8486666666666853, average overtime: 1.392\nSchedule: Uniform [2, 2, 2, 2], N: 8, T: 4\nAverage waiting time: 12.0175, average overtime: 24.247\nSchedule: Decreasing [5, 2, 1, 0], N: 8, T: 4\nAverage waiting time: 16.872375, average overtime: 24.117\nSchedule: Increasing [0, 1, 2, 5], N: 8, T: 4\nAverage waiting time: 12.272375, average overtime: 29.515\nSchedule: Front-heavy [4, 4, 0, 0], N: 8, T: 4\nAverage waiting time: 16.85025, average overtime: 24.151\nSchedule: Back-heavy [0, 0, 4, 4], N: 8, T: 4\nAverage waiting time: 16.67225, average overtime: 33.73\nSchedule: Alternating [4, 0, 4, 0], N: 8, T: 4\nAverage waiting time: 14.208375, average overtime: 24.128\nSchedule: Bailey-rule [2, 1, 1, 1, 1, 1, 1], N: 8, T: 7\nAverage waiting time: 6.508875, average overtime: 9.837\n\n\n\n\n2.4.6.3 Step 3: Exact calculation\nFor each schedule instance run 10 evaluations of the objective value using the exact method and calculate the average waiting time and overtime.\n\n# Loop through the schedules, run 10 evaluations, calculate average waiting time and overtime for each schedule, calculate average computation times and store the results in the results dictionary\n\nfor schedule_name, schedule in schedules:\n    N = sum(schedule)\n    T = len(schedule)\n    print(f\"Schedule: {schedule_name} {schedule}, N: {N}, T: {T}\")\n    convolutions = compute_convolutions(service_time, N, q)\n    \n    total_time = 0\n    # Exact calculation over 10 evaluations\n    for i in range(10):\n        start_time = time.time()\n        # Calculate the objective value using the exact method\n        waiting_time, overtime = calculate_objective_serv_time_lookup(schedule, d, convolutions)\n        elapsed_time = time.time() - start_time\n        total_time += elapsed_time\n        \n    avg_time = total_time / 10\n    print(f\"Expected waiting time: {waiting_time / N}, Expected overtime: {overtime}\")\n    results_dict['expected_waiting_time'].append(waiting_time / N)\n    results_dict['expected_overtime'].append(overtime)\n    results_dict['average_computation_time'].append(avg_time)\n\nSchedule: Calibration [2, 0, 0, 0, 0, 1], N: 3, T: 6\nExpected waiting time: 1.83, Expected overtime: 1.35\nSchedule: Uniform [2, 2, 2, 2], N: 8, T: 4\nExpected waiting time: 11.816608810500004, Expected overtime: 24.064827562971374\nSchedule: Decreasing [5, 2, 1, 0], N: 8, T: 4\nExpected waiting time: 16.715096633250006, Expected overtime: 23.92331748953545\nSchedule: Increasing [0, 1, 2, 5], N: 8, T: 4\nExpected waiting time: 12.5150625, Expected overtime: 29.856120798600017\nSchedule: Front-heavy [4, 4, 0, 0], N: 8, T: 4\nExpected waiting time: 16.715970000000002, Expected overtime: 23.92482686022145\nSchedule: Back-heavy [0, 0, 4, 4], N: 8, T: 4\nExpected waiting time: 16.715970000000002, Expected overtime: 33.92194762296002\nSchedule: Alternating [4, 0, 4, 0], N: 8, T: 4\nExpected waiting time: 14.233406400000002, Expected overtime: 23.95841024194273\nSchedule: Bailey-rule [2, 1, 1, 1, 1, 1, 1], N: 8, T: 7\nExpected waiting time: 6.439653759761102, Expected overtime: 9.820826086143853",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#results",
    "href": "function-testing.html#results",
    "title": "2  Evaluator functions testing",
    "section": "2.5 Results",
    "text": "2.5 Results\nComparison of the results of the exact calculations with the results of the Monte Carlo simulations.\n\ndf_results = pd.DataFrame.from_dict(results_dict)\ndf_results\n\n\n\n\n\n\n\n\n\nschedule_name\naverage_waiting_time\naverage_overtime\nexpected_waiting_time\nexpected_overtime\naverage_computation_time\n\n\n\n\n0\nCalibration\n1.848667\n1.392\n1.830000\n1.350000\n0.000098\n\n\n1\nUniform\n12.017500\n24.247\n11.816609\n24.064828\n0.000081\n\n\n2\nDecreasing\n16.872375\n24.117\n16.715097\n23.923317\n0.000069\n\n\n3\nIncreasing\n12.272375\n29.515\n12.515063\n29.856121\n0.000062\n\n\n4\nFront-heavy\n16.850250\n24.151\n16.715970\n23.924827\n0.000054\n\n\n5\nBack-heavy\n16.672250\n33.730\n16.715970\n33.921948\n0.000056\n\n\n6\nAlternating\n14.208375\n24.128\n14.233406\n23.958410\n0.000053\n\n\n7\nBailey-rule\n6.508875\n9.837\n6.439654\n9.820826\n0.000138\n\n\n\n\n\n\n\n\n\n# Extract schedule names from the dataframe\nschedule_names = df_results['schedule_name'].tolist()\n\n# Create new x-values for simulation and exact results\nx_sim = [f\"{s}&lt;br&gt;Simulation\" for s in schedule_names]\nx_exact = [f\"{s}&lt;br&gt;Exact\" for s in schedule_names]\n\n# Extract values from the dataframe\nsim_wait = df_results['average_waiting_time'].tolist()\nsim_over = df_results['average_overtime'].tolist()\nexact_wait = df_results['expected_waiting_time'].tolist()\nexact_over = df_results['expected_overtime'].tolist()\n\n# Create a combined category list with an empty category between the two groups\ncategories = x_sim + [\"\"] + x_exact\n\n# Create the figure\nfig = go.Figure()\n\n# Simulation bar traces (stacked)\nfig.add_trace(go.Bar(\n    x=x_sim,\n    y=sim_wait,\n    name='Waiting Time',\n    marker_color='blue'\n))\nfig.add_trace(go.Bar(\n    x=x_sim,\n    y=sim_over,\n    name='Overtime',\n    marker_color='red'\n))\n\n# Exact bar traces (stacked)\nfig.add_trace(go.Bar(\n    x=x_exact,\n    y=exact_wait,\n    name='Waiting Time',\n    marker_color='blue',\n    showlegend=False  # legend already shown for waiting time above\n))\nfig.add_trace(go.Bar(\n    x=x_exact,\n    y=exact_over,\n    name='Overtime',\n    marker_color='red',\n    showlegend=False  # legend already shown for overtime above\n))\n\n# Update x-axis to use the full category array (which includes the gap)\nfig.update_xaxes(\n    tickangle=45,\n    categoryorder='array',\n    categoryarray=categories\n)\n\n# Optionally, adjust the vertical dotted line.\n# For example, you can remove it if the gap is sufficient or reposition it.\nfig.update_layout(\n    title=\"Comparison of Simulation vs. Exact Results\",\n    xaxis_title=\"Schedule Type\",\n    yaxis_title=\"Time\",\n    barmode='stack',\n    shapes=[\n        dict(\n            type=\"line\",\n            xref=\"paper\", x0=0.5, x1=0.5,\n            yref=\"paper\", y0=0, y1=1,\n            line=dict(\n                color=\"black\",\n                width=2,\n                dash=\"dot\"\n            )\n        )\n    ]\n)\n\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#discussion",
    "href": "function-testing.html#discussion",
    "title": "2  Evaluator functions testing",
    "section": "2.6 Discussion",
    "text": "2.6 Discussion\nThe results show that the exact calculations and the Monte Carlo simulations are in good agreement. The average waiting times and overtimes are very close for both methods. The computation times for the exact calculations are also reasonable, indicating that the functions are efficient - at least for these limited instances.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#timeline",
    "href": "function-testing.html#timeline",
    "title": "2  Evaluator functions testing",
    "section": "2.7 Timeline",
    "text": "2.7 Timeline\nThis experiment has been started on 07-03-2025 and is expected to be finished on 14-03-2025.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html",
    "href": "xgboost-pairwise-ranking-large.html",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "",
    "text": "3.1 Objective\nObjective: Testing the performance of an XGBoost model trained for ranking pairwise schedules.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#background",
    "href": "xgboost-pairwise-ranking-large.html#background",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.2 Background",
    "text": "3.2 Background\nIn this experiment we develop a Machine Learning model using XGBoost that can evaluate two neighboring schedules and rank them according to preference. This ranking model can be applied to quickly guide the search process towards a ‘good enough’ solution.\nThe choice of using an ordinal model instead of a cardinal model is based on the consideration that it is significantly easier to determine whether alternative A is superior to B than to quantify the exact difference between A and B. This makes intuitive sense when considering the scenario of holding two identical-looking packages and deciding which one is heavier, as opposed to estimating the precise weight difference between them. (ho_ordinal_2000?).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#hypothesis",
    "href": "xgboost-pairwise-ranking-large.html#hypothesis",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.3 Hypothesis",
    "text": "3.3 Hypothesis\nAn XGBoost ranking model achieves superior computational efficiency compared to evaluating each element of a pair individually, leading to faster overall performance in ranking tasks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#methodology",
    "href": "xgboost-pairwise-ranking-large.html#methodology",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.4 Methodology",
    "text": "3.4 Methodology\n\n3.4.1 Tools and Materials\nWe use packages from Scikit-learn to prepare training data and evaluate the model and the XGBClassifier interface from the XGBoost library.\n\nimport time\nimport math\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.base import clone\nimport xgboost as xgb\nfrom xgboost.callback import TrainingCallback\nimport plotly.graph_objects as go\nimport pickle\nimport random\nfrom scipy.optimize import minimize\nfrom itertools import combinations\n\n\n\n3.4.2 Experimental Design\nTo compare an XGBoost Machine Learning model with a simple evaluation of each individual element of the pair, we will use a pairwise ranking approach. The objective is to rank two neighboring schedules according to preference.\n\nfrom functions import compute_convolutions\n\nN = 22 # Number of patients\nT = 20 # Number of intervals\nd = 5 # Length of each interval\nmax_s = 20 # Maximum service time\nq = 0.20 # Probability of a scheduled patient not showing up\nw = 0.1 # Weight for the waiting time in objective function\nl = 10\nnum_schedules = 100000 # Number of schedules to sample\n\n# Create service time distribution\ndef generate_weighted_list(max_s, l, i):\n    # Initialize an array of T+1 values, starting with zero\n    values = np.zeros(T + 1)\n    \n    # Objective function: Sum of squared differences between current weighted average and the desired l\n    def objective(x):\n        weighted_avg = np.dot(np.arange(1, T + 1), x) / np.sum(x)\n        return (weighted_avg - l) ** 2\n\n    # Constraint: The sum of the values from index 1 to T must be 1\n    constraints = ({\n        'type': 'eq',\n        'fun': lambda x: np.sum(x) - 1\n    })\n    \n    # Bounds: Each value should be between 0 and 1\n    bounds = [(0, 1)] * T\n\n    # Initial guess: Random distribution that sums to 1\n    initial_guess = np.random.dirichlet(np.ones(T))\n\n    # Optimization: Minimize the objective function subject to the sum and bounds constraints\n    result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n\n    # Set the values in the array (index 0 remains 0)\n    values[1:] = result.x\n\n    # Now we need to reorder the values as per the new requirement\n    first_part = np.sort(values[1:i+1])  # Sort the first 'i' values in ascending order\n    second_part = np.sort(values[i+1:])[::-1]  # Sort the remaining 'T-i' values in descending order\n    \n    # Combine the sorted parts back together\n    values[1:i+1] = first_part\n    values[i+1:] = second_part\n    \n    return values\n\ni = 5  # First 5 highest values in ascending order, rest in descending order\ns = generate_weighted_list(max_s, l, i)\nprint(s)\nprint(\"Sum:\", np.sum(s[1:]))  # This should be 1\nprint(\"Weighted service time:\", np.dot(np.arange(1, T + 1), s[1:]))  # This should be close to l\n\nconvolutions = compute_convolutions(s, N, q)\nfile_path_parameters = f\"datasets/parameters_{N}_{T}_{l}.pkl\"\nwith open(file_path_parameters, 'wb') as f:\n    pickle.dump({\n      'N': N,\n      'T': T,\n      'd': d,\n      'max_s': max_s,\n      'q': q,\n      'w': w,\n      'l': l,\n      'num_schedules': num_schedules,\n      'convolutions': convolutions\n      }, f)\n    print(f\"Data saved successfully to '{file_path_parameters}'\")\n\n[0.         0.00543629 0.03030549 0.04163197 0.11719454 0.11859203\n 0.11731255 0.11310677 0.10304861 0.05397991 0.05117193 0.03973443\n 0.03821059 0.03796436 0.02880373 0.02842576 0.02524925 0.01903744\n 0.01432856 0.01035653 0.00610927]\nSum: 1.0000000000096703\nWeighted service time: 8.093510567287902\nData saved successfully to 'datasets/parameters_22_20_10.pkl'\n\n\nWe will create a random set of pairs of neighboring schedules with \\(N = 22\\) patients and \\(T = 20\\) intervals of length \\(d = 5\\).\nA neighbor of a schedule x is considered a schedule x’ where single patients have been shifted one interval to the left. Eg: ([2,1,1,2], [1,2,0,3]) are neighbors and ([2,1,1,2], [2,1,3,0]) are not, because [1,2,0,3] - [2,1,1,2] = [-1, 1, -1, 1] and [2,1,3,0] - [2,1,1,2] = [0, 0, 2, -2].\nService times will have a discrete distribution. The probability of a scheduled patient not showing up will be \\(q = 0.2\\).\nThe objective function will be the weighted average of the total waiting time of all patients and overtime. The model will be trained to predict which of the two neighboring schedules has the lowest objective value. The prediction time will be recorded. Then the same schedules will be evaluated by computing the objective value and then ranked.\n\n\n3.4.3 Variables\n\nIndependent Variables: A list of tuples with pairs of neighboring schedules.\nDependent Variables: A list with rankings for each tuple of pairwise schedules. Eg: If the rank for ([2,1,1], [1,1,2]) equals 0 this means that the schedule with index 0 ([2,1,1]) has the lowest objective value.\n\n\n\n3.4.4 Data Collection\nThe data set will be generated using simulation in which random samples will be drawn from the population of all possible schedules. For each sample a random neighboring schedule will be created.\n\n\n3.4.5 Sample Size and Selection\nSample Size: The total population size equals \\({{N + T -1}\\choose{N}} \\approx\\) 244663.0 mln. For this experiment we will be using a relatively small sample of 100000 pairs of schedules.\nSample Selection: The samples will be drawn from a lexicographic order of possible schedules in order to accurately reflect the combinatorial nature of the problem and to ensure unbiased sampling from the entire combinatorial space.\n\n\n3.4.6 Experimental Procedure\nThe experiment involves multiple steps, beginning with data preparation and concluding with model evaluation.The diagram below illustrates the sequence of steps.\n\n\n\n\n\ngraph TD\n    A[\"From population\"] --&gt;|\"Sample\"| B[\"Random subset\"]\n    B --&gt; |Create neighbors| C[\"Features: Schedule pairs\"]\n    C --&gt; |Calculate objectives| D[\"Objective values\"]\n    D --&gt; |Rank objectives| E[\"Labels: Rankings\"]\n    E --&gt; |\"Split dataset\"| F[\"Training set\"]\n    E --&gt; |\"Split dataset\"| G[\"Test set\"]\n    F --&gt; |\"Train\"| H[\"Model\"]\n    H[\"Model\"] --&gt; |\"Apply\"| G[\"Test set\"]\n    G[\"Test set\"] --&gt; |\"Evaluate\"| I[\"Performance\"]\n\n\n\n\n\n\nStep 1: Randomly select a subset of schedules.\n\nfrom functions import create_random_schedules\n\nstart = time.time()\n# schedules = random_combination_with_replacement(T, N, num_schedules)\nschedules = create_random_schedules(T, N, num_schedules)\nprint(f\"Sampled: {len(schedules):,} schedules\\n\")\nh = random.choices(range(len(schedules)), k=7)\nprint(f\"Sampled schedules: {h}\")\nfor i in h:\n    print(f\"Schedule: {schedules[i]}\")\nend = time.time()\ndata_prep_time = end - start\n\nprint(f\"\\nProcessing time: {data_prep_time} seconds\\n\")\n\nSampled: 100,000 schedules\n\nSampled schedules: [85534, 49568, 65853, 46331, 77732, 38614, 7289]\nSchedule: [0, 1, 2, 2, 1, 0, 4, 3, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 0]\nSchedule: [0, 2, 0, 3, 0, 1, 2, 1, 2, 1, 1, 0, 1, 2, 0, 1, 0, 2, 1, 2]\nSchedule: [1, 1, 0, 1, 1, 2, 1, 2, 1, 0, 0, 0, 2, 2, 0, 2, 1, 0, 1, 4]\nSchedule: [1, 2, 0, 4, 2, 1, 0, 0, 2, 2, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1]\nSchedule: [0, 3, 2, 2, 1, 2, 1, 1, 1, 0, 0, 1, 2, 0, 1, 1, 0, 2, 2, 0]\nSchedule: [2, 1, 1, 0, 0, 5, 2, 0, 1, 3, 0, 1, 3, 1, 0, 0, 0, 0, 1, 1]\nSchedule: [1, 1, 1, 2, 4, 0, 0, 0, 1, 0, 1, 0, 2, 0, 1, 2, 1, 1, 3, 1]\n\nProcessing time: 0.6191208362579346 seconds\n\n\n\nStep 2: Create pairs of neighboring schedules.\n\nfrom functions import get_v_star\n\ndef create_neighbors_list(s: list[int], v_star: np.ndarray) -&gt; (list[int], list[int]):\n    \"\"\"\n    Create a set of pairs of schedules that are from the same neighborhood.\n    \n    Parameters:\n      s (list[int]): A list of integers with |s| = T and sum N.\n      v_star (np.ndarray): Precomputed vectors V* of length T.\n      \n    Returns:\n      tuple(list[int], list[int]): A pair of schedules.\n    \"\"\"\n    T = len(s)\n\n    # Precompute binomial coefficients (weights for random.choices)\n    binom_coeff = [math.comb(T, i) for i in range(1, T)]\n\n    # Choose a random value of i with the corresponding probability\n    i = random.choices(range(1, T), weights=binom_coeff)[0]\n\n    # Instead of generating the full list of combinations, sample one directly\n    j = random.sample(range(T), i)\n    \n    s_p = s.copy()\n    for k in j:\n        s_temp = np.array(s_p) + v_star[k]\n        s_temp = s_temp.astype(int)\n        if np.all(s_temp &gt;= 0):\n            s_p = s_temp.astype(int).tolist()\n        \n    return s, s_p\n\nstart = time.time()\nv_star = get_v_star(T)\nneighbors_list = [create_neighbors_list(schedule, v_star) for schedule in schedules] # This can be done in parellel to improve speed\nend = time.time()\nfor i in h:\n    original_schedule = neighbors_list[i][0]\n    neighbor_schedule = neighbors_list[i][1]\n    difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]\n    print(f\"Neighbors\\n{original_schedule}\\n{neighbor_schedule}\\n{difference}\")\ntraining_set_feat_time = end - start\nprint(f\"\\nProcessing time: {training_set_feat_time} seconds\\n\")\n\nNeighbors\n[0, 1, 2, 2, 1, 0, 4, 3, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 0]\n[0, 1, 2, 3, 0, 1, 4, 2, 1, 1, 2, 0, 0, 1, 0, 1, 1, 1, 1, 0]\n[0, 0, 0, -1, 1, -1, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0]\nNeighbors\n[0, 2, 0, 3, 0, 1, 2, 1, 2, 1, 1, 0, 1, 2, 0, 1, 0, 2, 1, 2]\n[1, 1, 1, 2, 0, 1, 3, 1, 2, 1, 0, 0, 2, 1, 1, 0, 1, 2, 0, 2]\n[-1, 1, -1, 1, 0, 0, -1, 0, 0, 0, 1, 0, -1, 1, -1, 1, -1, 0, 1, 0]\nNeighbors\n[1, 1, 0, 1, 1, 2, 1, 2, 1, 0, 0, 0, 2, 2, 0, 2, 1, 0, 1, 4]\n[1, 0, 0, 2, 1, 1, 2, 2, 0, 0, 0, 0, 2, 3, 0, 2, 0, 0, 1, 5]\n[0, 1, 0, -1, 0, 1, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, -1]\nNeighbors\n[1, 2, 0, 4, 2, 1, 0, 0, 2, 2, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1]\n[1, 2, 0, 5, 1, 1, 1, 0, 1, 3, 0, 1, 0, 0, 1, 3, 1, 0, 1, 0]\n[0, 0, 0, -1, 1, 0, -1, 0, 1, -1, 1, 0, 0, 0, 0, -1, 0, 1, -1, 1]\nNeighbors\n[0, 3, 2, 2, 1, 2, 1, 1, 1, 0, 0, 1, 2, 0, 1, 1, 0, 2, 2, 0]\n[1, 2, 2, 2, 2, 2, 1, 1, 0, 0, 1, 0, 2, 0, 2, 0, 1, 1, 2, 0]\n[-1, 1, 0, 0, -1, 0, 0, 0, 1, 0, -1, 1, 0, 0, -1, 1, -1, 1, 0, 0]\nNeighbors\n[2, 1, 1, 0, 0, 5, 2, 0, 1, 3, 0, 1, 3, 1, 0, 0, 0, 0, 1, 1]\n[3, 1, 0, 0, 1, 4, 2, 0, 1, 3, 0, 2, 2, 1, 0, 0, 0, 1, 1, 0]\n[-1, 0, 1, 0, -1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, -1, 0, 1]\nNeighbors\n[1, 1, 1, 2, 4, 0, 0, 0, 1, 0, 1, 0, 2, 0, 1, 2, 1, 1, 3, 1]\n[0, 1, 2, 2, 3, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 3, 0, 1, 3, 2]\n[1, 0, -1, 0, 1, 0, 0, -1, 1, -1, 1, -1, 1, 0, 0, -1, 1, 0, 0, -1]\n\nProcessing time: 9.164121150970459 seconds\n\n\n\nStep 3: For each schedule in each pair calculate the objective. For each pair save the index of the schedule that has the lowest objective value.\n\nfrom functions import calculate_objective_serv_time_lookup\n\nobjectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[0], d, convolutions)]\n]\nstart = time.time()\nobjectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[1], d, convolutions)]\n]\nend = time.time()\ntraining_set_lab_time = end - start\nobjectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]\nrankings = np.argmin(objectives, axis=1).tolist()\nfor i in range(5):\n    print(f\"Objectives: {objectives[i]}, Ranking: {rankings[i]}\")\n\nprint(f\"\\nProcessing time: {training_set_lab_time} seconds\\n\")\n\n# Saving neighbors_list and objectives to a pickle file\n\nfile_path_neighbors = f\"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl\"\nwith open(file_path_neighbors, 'wb') as f:\n    pickle.dump({'neighbors_list': neighbors_list, 'objectives': objectives, 'rankings': rankings}, f)\n    print(f\"Data saved successfully to '{file_path_neighbors}'\")\n\nObjectives: [88.17925506905414, 97.97325997172206], Ranking: 0\nObjectives: [97.91182775072866, 97.8591016726144], Ranking: 1\nObjectives: [126.1522161894033, 127.1647578693569], Ranking: 0\nObjectives: [101.79321072681091, 98.59424287048375], Ranking: 1\nObjectives: [121.71994028846842, 123.49965359375256], Ranking: 0\n\nProcessing time: 48.1225049495697 seconds\n\nData saved successfully to 'datasets/neighbors_and_objectives_22_20_10.pkl'\n\n\nStep 4: Create training and test sets.\n\n# Prepare the dataset\nX = []\nfor neighbors in neighbors_list:\n    X.append(neighbors[0] + neighbors[1])\n\nX = np.array(X)\ny = np.array(rankings)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 5: Train the XGBoost model.\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B[Initialize StratifiedKFold]\n    B --&gt; C[Initialize XGBClassifier]\n    C --&gt; D[Set results as empty list]\n    D --&gt; E[Loop through each split of cv split]\n    E --&gt; F[Get train and test indices]\n    F --&gt; G[Split X and y into X_train, X_test, y_train, y_test]\n    G --&gt; H[Clone the classifier]\n    H --&gt; I[Call fit_and_score function]\n    I --&gt; J[Fit the estimator]\n    J --&gt; K[Score on training set]\n    J --&gt; L[Score on test set]\n    K --&gt; M[Return estimator, train_score, test_score]\n    L --&gt; M\n    M --&gt; N[Append the results]\n    N --&gt; E\n    E --&gt; O[Loop ends]\n    O --&gt; P[Print results]\n    P --&gt; Q[End]\n\n\n\n\n\n\n\nclass CustomCallback(TrainingCallback):\n    def __init__(self, period=10):\n        self.period = period\n\n    def after_iteration(self, model, epoch, evals_log):\n        if (epoch + 1) % self.period == 0:\n            print(f\"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}\")\n        return False\n    \ndef fit_and_score(estimator, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0\n    )\n\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return estimator, train_score, test_score\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n\n# Initialize the XGBClassifier without early stopping here\n# Load the best trial parameters from a JSON file.\nwith open(\"model_params.json\", \"r\") as f:\n    model_params = json.load(f)\n    \n# Initialize the EarlyStopping callback with validation dataset\nearly_stop = xgb.callback.EarlyStopping(\n    rounds=10, metric_name='logloss', data_name='validation_0', save_best=True\n)\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n    early_stopping_rounds=9,\n    #callbacks=[CustomCallback(period=50), early_stop],\n    callbacks=[CustomCallback(period=50)],\n)\nprint(\"Params: \")\nfor key, value in model_params.items():\n    print(f\" {key}: {value}\")\n\nstart = time.time()\nresults = []\n\nfor train_idx, test_idx in cv.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    est, train_score, test_score = fit_and_score(\n        clone(clf), X_train, X_test, y_train, y_test\n    )\n    results.append((est, train_score, test_score))\nend = time.time()\ntraining_time = end - start\nprint(f\"\\nTraining time: {training_time} seconds\\n\")\n\nParams: \n max_depth: 6\n min_child_weight: 1\n gamma: 0.1\n subsample: 0.8\n colsample_bytree: 0.8\n learning_rate: 0.1\n n_estimators: 100\nEpoch 49, Evaluation log: 0.37537274533435705\nEpoch 99, Evaluation log: 0.3202946408016607\nEpoch 49, Evaluation log: 0.36902658584490416\nEpoch 99, Evaluation log: 0.31779550507897514\nEpoch 49, Evaluation log: 0.37383715238538573\nEpoch 99, Evaluation log: 0.32173826755817864\nEpoch 49, Evaluation log: 0.37095330382874236\nEpoch 99, Evaluation log: 0.32059324672384537\nEpoch 49, Evaluation log: 0.3741410843353253\nEpoch 99, Evaluation log: 0.3237181565931067\n\nTraining time: 2.9833967685699463 seconds\n\n\n\nStep 6: To evaluate the performance of the XGBoost ranking model, we will use Stratified K-Fold Cross-Validation with 5 splits, ensuring each fold maintains the same class distribution as the original dataset. Using StratifiedKFold(n_splits=5, shuffle=True, random_state=94), the dataset will be divided into five folds. In each iteration, the model will be trained on four folds and evaluated on the remaining fold. A custom callback, CustomCallback(period=10), will print the evaluation log every 10 epochs.\nThe fit_and_score function will fit the model and score it on both the training and test sets, storing the results for each fold. This provides insight into the model’s performance across different subsets of the data, helps in understanding how well the model generalizes to unseen data and identifies potential overfitting or underfitting issues. The overall processing time for the cross-validation will also be recorded.\n\n# Print results\nfor i, (est, train_score, test_score) in enumerate(results):\n    print(f\"Fold {i+1} - Train Score (Accuracy): {train_score:.4f}, Test Score (Accuracy): {test_score:.4f}\")\n\nFold 1 - Train Score (Accuracy): 0.8820, Test Score (Accuracy): 0.8743\nFold 2 - Train Score (Accuracy): 0.8824, Test Score (Accuracy): 0.8751\nFold 3 - Train Score (Accuracy): 0.8842, Test Score (Accuracy): 0.8726\nFold 4 - Train Score (Accuracy): 0.8849, Test Score (Accuracy): 0.8709\nFold 5 - Train Score (Accuracy): 0.8820, Test Score (Accuracy): 0.8713\n\n\nTraining the model on the entire dataset provides a final model that has learned from all available data. Recording the training time helps in understanding the computational efficiency and scalability of the model with the given hyperparameters.\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nclf.save_model('models/classifier_large_instance.json')\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\\n\")\n\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\nTraining accuracy: 88.11%\n\n\nTraining time: 0.46138501167297363 seconds\n\n\n\n\n\n3.4.7 Validation\nGenerating test schedules and calculating their objectives and rankings allows us to create a new dataset for evaluating the model’s performance on unseen data.\n\nnum_test_schedules = 1000\n\n#test_schedules = random_combination_with_replacement(T, N, num_test_schedules)\ntest_schedules = create_random_schedules(T, N, num_test_schedules)\n\ntest_neighbors = [create_neighbors_list(test_schedule, v_star) for test_schedule in test_schedules] # This can be done in parellel to improve speed\n\nprint(f\"Sampled: {len(test_schedules)} schedules\\n\")\n\ntest_objectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[0], d, convolutions)]\n]\n# Start time measurement for the evaluation\nstart = time.time()\ntest_objectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[1], d, convolutions)]\n]\ntest_rankings = [0 if test_obj &lt; test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]\nend = time.time()\nevaluation_time = end - start\n\n# Combine the objectives for each pair for later processing\ntest_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]\n\nprint(f\"\\nEvaluation time: {evaluation_time} seconds\\n\")\n\nfor i in range(6):\n    print(f\"Neighbors: {test_neighbors[i]},\\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\\n\")\n\nSampled: 1000 schedules\n\n\nEvaluation time: 0.5211591720581055 seconds\n\nNeighbors: ([0, 2, 2, 0, 0, 1, 3, 3, 2, 2, 0, 0, 1, 0, 1, 1, 1, 0, 1, 2], [1, 2, 1, 0, 0, 2, 3, 3, 2, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 2]),\nObjectives: [110.75226450345767, 105.5293674787008], Ranking: 1\n\nNeighbors: ([0, 2, 1, 1, 3, 2, 0, 0, 1, 1, 1, 0, 1, 1, 2, 2, 0, 0, 0, 4], [1, 1, 1, 2, 3, 1, 0, 0, 2, 1, 0, 1, 1, 0, 2, 2, 0, 1, 0, 3]),\nObjectives: [99.9391699307969, 94.16440194650053], Ranking: 1\n\nNeighbors: ([1, 0, 3, 0, 0, 2, 2, 0, 5, 4, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0], [1, 0, 3, 0, 0, 3, 1, 0, 5, 5, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0]),\nObjectives: [122.34851422377199, 123.69654835822257], Ranking: 0\n\nNeighbors: ([2, 1, 3, 2, 1, 0, 2, 0, 3, 0, 0, 2, 0, 0, 2, 0, 2, 1, 1, 0], [2, 2, 2, 3, 1, 0, 1, 0, 3, 0, 1, 1, 0, 0, 2, 1, 2, 1, 0, 0]),\nObjectives: [105.10528129650763, 108.43029352470786], Ranking: 0\n\nNeighbors: ([2, 0, 2, 1, 0, 2, 1, 0, 1, 3, 3, 0, 0, 1, 0, 0, 1, 2, 1, 2], [1, 1, 2, 0, 0, 2, 2, 0, 0, 4, 2, 0, 1, 0, 0, 0, 2, 1, 1, 3]),\nObjectives: [93.32367885774698, 93.1968800740998], Ranking: 1\n\nNeighbors: ([0, 1, 3, 0, 0, 0, 2, 1, 3, 1, 1, 0, 2, 1, 0, 1, 1, 1, 3, 1], [1, 0, 3, 0, 0, 1, 1, 2, 2, 2, 0, 0, 2, 1, 0, 2, 1, 0, 3, 1]),\nObjectives: [102.08587233729006, 97.05508233059925], Ranking: 1\n\n\n\nMaking predictions on new data and comparing them to the actual rankings provides an evaluation of the model’s performance in practical applications. Recording the prediction time helps in understanding the model’s efficiency during inference.\n\ninput_X = test_neighbors\nX_new = []\nfor test_neighbor in input_X:\n    X_new.append(test_neighbor[0] + test_neighbor[1])\n    \n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.00497889518737793 seconds\n\ntest_rankings = [1 1 0 0 1 1], \ny_pred = [1 1 0 0 0 1], \ny_pred_proba = \n[[0.03129494 0.96870506]\n [0.01167864 0.98832136]\n [0.84496987 0.15503016]\n [0.8883002  0.11169982]\n [0.50500226 0.49499774]\n [0.03031099 0.969689  ]]\n\n\nCalculating the ambiguousness of the predicted probabilities helps in understanding the model’s confidence in its predictions. High ambiguousness indicates uncertain predictions, while low ambiguousness indicates confident predictions.\nAmbiguousness is calculated using the formula for entropy:\n\\[\nH(X) = - \\sum_{i} p(x_i) \\log_b p(x_i)\n\\]\nWhere in our case:\n\n\\(H(X)\\) is the ambiguousness of the random variable \\(X\\) - the set of probability scores for the predicted rankings,\n\\(p(x_i)\\) is probability score \\(x_i\\),\n\\(\\log_b\\) is the logarithm with base \\(b\\) (here \\(\\log_2\\) as we have two predicted values),\nThe sum is taken over all possible outcomes of \\(X\\).\n\nCalculating cumulative error rate and cumulative accuracy helps in understanding how the model’s performance evolves over the dataset.\nVisualizing the relationship between ambiguousness and error provides insights into how uncertainty in the model’s predictions correlates with its accuracy. This can help in identifying patterns and understanding the conditions under which the model performs well or poorly.\n\nfrom functions import calculate_ambiguousness\n\nerrors = np.abs(y_pred - np.array(test_rankings))\n\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9)))\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()\n\n                                                \n\n\n\n\n3.4.8 Hyperparameter Optimization\nIn the initial model the choice of hyperparameters was based on default values, examples from demo’s or trial and error. To improve the model’s performance, we applied a hyperparameter optimization technique to find the best set of hyperparameters. We used a grid search with cross-validation to find the optimal hyperparameters for the XGBoost model. The grid search was performed over a predefined set of hyperparameters, and the best hyperparameters were selected based on the model’s performance on the validation set. The best hyperparameters were then used to train the final model.\n\nfrom functions import compare_json\n\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n    \ndifferences = compare_json(model_params, best_trial_params)\n\nparams_tbl = pd.DataFrame(differences)\nparams_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)\nprint(params_tbl)\n\n                      max_depth     gamma  subsample  colsample_bytree  \\\nbase parameters               6  0.100000   0.800000          0.800000   \noptimized parameters          5  0.304548   0.781029          0.922528   \n\n                      learning_rate  n_estimators  \nbase parameters            0.100000           100  \noptimized parameters       0.239488           490  \n\n\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\n# Load the best trial parameters from a JSON file.\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\")\n\n\nTraining time: 1.9750583171844482 seconds\n\nTraining accuracy: 95.18%\n\n\n\n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.00556492805480957 seconds\n\ntest_rankings = [1 1 0 0 1 1], \ny_pred = [1 1 0 0 0 1], \ny_pred_proba = \n[[5.82575798e-04 9.99417424e-01]\n [1.33752823e-04 9.99866247e-01]\n [9.72199440e-01 2.78005321e-02]\n [9.87804174e-01 1.21957995e-02]\n [5.05212545e-01 4.94787425e-01]\n [1.01699233e-02 9.89830077e-01]]\n\n\n\nerrors = np.abs(y_pred - np.array(test_rankings))\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors, \"Schedules\": test_neighbors, \"Objectives\": test_objectives}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n\n\n\n\n\n\n\nAmbiguousness\nError\nSchedules\nObjectives\nCumulative error rate\nCumulative accuracy\n\n\n\n\n460\n0.000075\n0\n([1, 2, 4, 1, 1, 0, 1, 1, 3, 1, 1, 0, 1, 1, 2,...\n[111.9192594976547, 117.92971932670405]\n0.0\n1.0\n\n\n977\n0.000193\n0\n([1, 3, 1, 1, 1, 0, 0, 2, 2, 1, 3, 0, 2, 2, 0,...\n[106.06261375989104, 113.30079370453848]\n0.0\n1.0\n\n\n140\n0.000241\n0\n([1, 5, 0, 2, 3, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1,...\n[108.78749046739821, 114.52227389419927]\n0.0\n1.0\n\n\n379\n0.000384\n0\n([1, 0, 0, 1, 1, 1, 1, 5, 0, 1, 0, 2, 1, 1, 2,...\n[110.47000733469568, 123.31454961809814]\n0.0\n1.0\n\n\n337\n0.000418\n0\n([1, 1, 1, 0, 2, 1, 2, 1, 1, 0, 2, 0, 1, 1, 0,...\n[90.22676574302173, 100.06884212878104]\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9),\n                    customdata=df[[\"Schedules\", \"Objectives\"]],\n                    hovertemplate=\n                        \"Ambiguousness: %{x} &lt;br&gt;\" +\n                        \"Error: %{y} &lt;br&gt;\" +\n                        \"Schedules: %{customdata[0][0]} / %{customdata[0][1]} &lt;br&gt;\" +\n                        \"Objectives: %{customdata[1]} &lt;br&gt;\"\n                    ))\n                  \nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#results",
    "href": "xgboost-pairwise-ranking-large.html#results",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.5 Results",
    "text": "3.5 Results\nWe wanted to test whether an XGBoost classification model could be used to assess and rank the quality of pairs of schedules. For performance benchmarking we use the conventional calculation method utilizing Lindley recursions.\nWe trained the XGBoost ranking model with a limited set of features (schedules) and labels (objectives). The total number of possible schedules is approximately 244663.0 million. For training and evaluation, we sampled 200000 schedules and corresponding neighbors. Generating the feature and label set took a total of 57.9057 seconds, with the calculation of objective values accounting for 48.1225 seconds.\nThe model demonstrates strong and consistent performance with high accuracies both for training, testing and validation (92.38%) with good generalization and stability. Total training time for the final model was 1.9751 seconds. The evaluation of 1000 test schedules took 0.0056 seconds for the the XGBoost model and 0.5212 for the conventional method, which is an improvement of 93X.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#discussion",
    "href": "xgboost-pairwise-ranking-large.html#discussion",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.6 Discussion",
    "text": "3.6 Discussion\n\ntraining_time = round(modeling_time, 4)\nconventional_time = round(evaluation_time, 4)\nxgboost_time = round(prediction_time, 4)\n\n# Define time values for plotting\ntime_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds\n\n# Calculate evaluations for method 1\nmethod1_evaluations = np.where(time_values &gt;= training_time, (time_values - training_time) / xgboost_time * 1000, 0)\n\n# Calculate evaluations for method 2\nmethod2_evaluations = time_values / conventional_time * 1000\n\n# Create line chart\nfig = go.Figure()\n\n# Add method 1 trace\nfig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))\n\n# Add method 2 trace\nfig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))\n\n# Update layout\nfig.update_layout(\n    title=\"Speed comparison between XGBoost ranking model and conventional method\",\n    xaxis_title=\"Time (seconds)\",\n    yaxis_title=\"Number of Evaluations\",\n    legend_title=\"Methods\",\n    template=\"plotly_white\"\n)\n\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#timeline",
    "href": "xgboost-pairwise-ranking-large.html#timeline",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.7 Timeline",
    "text": "3.7 Timeline\nThis experiment was started on 26-04-2025. The expected completion date is 26-04-2025.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#references",
    "href": "xgboost-pairwise-ranking-large.html#references",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.8 References",
    "text": "3.8 References",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "",
    "text": "4.1 Objective\nObjective: Testing the performance of an XGBoost model trained for ranking pairwise schedules taken from a neighborhood around quasi optimal initial schedule (Bailey-Welch).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#background",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#background",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.2 Background",
    "text": "4.2 Background\nIn a previous experiment we developed a Machine Learning model using XGBoost that can evaluate two neighboring schedules and rank them according to preference. For evaluation random schedules were sampled from the full solution set.\nThe full solution set however contains many schedules that are obviously not optimal. Adding them to the training set would provide the model with rather useless knowledge. Therefore in this experiment we only sample pairs of schedules taken from within the vicinity of a ‘good’ starting point.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#hypothesis",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#hypothesis",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.3 Hypothesis",
    "text": "4.3 Hypothesis\nAn XGBoost ranking model achieves superior computational efficiency compared to evaluating each element of a pair individually, leading to faster overall performance in ranking tasks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#methodology",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#methodology",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.4 Methodology",
    "text": "4.4 Methodology\n\n4.4.1 Tools and Materials\nWe use packages from Scikit-learn to prepare training data and evaluate the model and the XGBClassifier interface from the XGBoost library.\n\nimport time\nimport math\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.base import clone\nimport xgboost as xgb\nfrom xgboost.callback import TrainingCallback\nimport plotly.graph_objects as go\nimport pickle\nimport random\nfrom scipy.optimize import minimize\nfrom itertools import combinations\n\n\n\n4.4.2 Experimental Design\nTo compare an XGBoost Machine Learning model with a simple evaluation of each individual element of the pair, we will use a pairwise ranking approach. The objective is to rank two neighboring schedules according to preference.\n\nfrom functions import compute_convolutions, bailey_welch_schedule\n\nN = 22 # Number of patients\nT = 20 # Number of intervals\nd = 5 # Length of each interval\nmax_s = 20 # Maximum service time\nq = 0.20 # Probability of a scheduled patient not showing up\nw = 0.1 # Weight for the waiting time in objective function\nl = 10\nnum_schedules = 300000 # Number of schedules to sample\n\n# Create service time distribution\ndef generate_weighted_list(max_s, l, i):\n    \"\"\"\n    Generates a service time probability distribution using optimization.\n\n    This function creates a discrete probability distribution over T possible\n    service times (from 1 to T). It uses optimization (SLSQP) to find a\n    distribution whose weighted average service time is as close as possible\n    to a target value 'l', subject to the constraint that the probabilities\n    sum to 1 and each probability is between 0 and 1.\n\n    After finding the distribution, it sorts the probabilities: the first 'i'\n    probabilities (corresponding to service times 1 to i) are sorted in\n    ascending order, and the remaining probabilities (service times i+1 to T)\n    are sorted in descending order.\n\n    Note:\n        - This function relies on a globally defined integer 'T', representing\n          the maximum service time considered (or number of probability bins).\n        - The parameter 'max_s' is accepted but not used directly within this\n          function's optimization or sorting logic as shown. It might be\n          related to how 'T' is determined externally.\n        - Requires NumPy and SciPy libraries (specifically scipy.optimize.minimize).\n\n    Args:\n        max_s (any): Maximum service time parameter (currently unused in the\n                     provided function body's core logic).\n        l (float): The target weighted average service time for the distribution.\n        i (int): The index determining the sorting split point. Probabilities\n                 for service times 1 to 'i' are sorted ascendingly, and\n                 probabilities for service times 'i+1' to 'T' are sorted\n                 descendingly. Must be between 1 and T-1 for meaningful sorting.\n\n    Returns:\n        numpy.ndarray: An array of size T+1. The first element (index 0) is 0.\n                       Elements from index 1 to T represent the calculated\n                       and sorted probability distribution, summing to 1.\n                       Returns None if optimization fails.\n    \"\"\"\n    # Initialize an array of T+1 values, starting with zero\n    # Index 0 is unused for probability, indices 1 to T hold the distribution\n    values = np.zeros(l + 1)\n\n    # --- Inner helper function for optimization ---\n    def objective(x):\n        \"\"\"Objective function: Squared difference between weighted average and target l.\"\"\"\n        # Calculate weighted average: sum(index * probability) / sum(probability)\n        # Since sum(probability) is constrained to 1, it simplifies.\n        weighted_avg = np.dot(np.arange(1, l + 1), x) # Corresponds to sum(k * P(ServiceTime=k))\n        return (weighted_avg - l) ** 2\n\n    # --- Constraints for optimization ---\n    # Constraint 1: The sum of the probabilities (x[0] to x[T-1]) must be 1\n    constraints = ({\n        'type': 'eq',\n        'fun': lambda x: np.sum(x) - 1\n    })\n\n    # Bounds: Each probability value x[k] must be between 0 and 1\n    # Creates a list of T tuples, e.g., [(0, 1), (0, 1), ..., (0, 1)]\n    bounds = [(0, 1)] * l\n\n    # Initial guess: Use Dirichlet distribution to get a random distribution that sums to 1\n    # Provides a starting point for the optimizer. np.ones(T) gives equal weights initially.\n    initial_guess = np.random.dirichlet(np.ones(l))\n\n    # --- Perform Optimization ---\n    # Minimize the objective function subject to the sum and bounds constraints\n    # using the Sequential Least Squares Programming (SLSQP) method.\n    result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n\n    # Check if optimization was successful\n    if not result.success:\n        print(f\"Warning: Optimization failed! Message: {result.message}\")\n        # Handle failure case, e.g., return None or raise an error\n        return None # Or potentially return a default distribution\n\n    # Assign the optimized probabilities (result.x) to the correct slice of the values array\n    # result.x contains the T probabilities for service times 1 to T.\n    values[1:] = result.x\n\n    # --- Reorder the values based on the index 'i' ---\n    # Ensure 'i' is within a valid range for slicing and sorting\n    if not (0 &lt; i &lt; l):\n       print(f\"Warning: Index 'i' ({i}) is outside the valid range (1 to {T-1}). Sorting might be trivial.\")\n       # Adjust i or handle as an error depending on requirements\n       i = max(1, min(i, l - 1)) # Clamp i to a safe range for demonstration\n\n    # Sort the first 'i' probabilities (indices 1 to i) in ascending order\n    first_part = np.sort(values[1:i+1])\n    # Sort the remaining 'T-i' probabilities (indices i+1 to T) in descending order\n    second_part = np.sort(values[i+1:])[::-1] # [::-1] reverses the sorted array\n\n    # Combine the sorted parts back into the 'values' array\n    values[1:i+1] = first_part\n    values[i+1:] = second_part\n\n    # Return the final array with the sorted probability distribution\n    return values\n\ni = 5  # First 5 highest values in ascending order, rest in descending order\ns = generate_weighted_list(max_s, l, i)\nprint(s)\nprint(\"Sum:\", np.sum(s[1:]))  # This should be 1\nprint(\"Weighted service time:\", np.dot(np.arange(len(s)), s))  # This should be close to l\ninitial_x = bailey_welch_schedule(T, d, N, s)\nprint(f\"Initial schedule: {initial_x}\")\nconvolutions = compute_convolutions(s, N, q)\nfile_path_parameters = f\"datasets/parameters_{N}_{T}_{l}.pkl\"\nwith open(file_path_parameters, 'wb') as f:\n    pickle.dump({\n      'N': N,\n      'T': T,\n      'd': d,\n      'max_s': max_s,\n      'q': q,\n      'w': w,\n      'l': l,\n      'num_schedules': num_schedules,\n      'convolutions': convolutions\n      }, f)\n    print(f\"Data saved successfully to '{file_path_parameters}'\")\n\n[0.00000000e+00 8.55350513e-12 2.62106933e-11 4.38869080e-11\n 6.27233022e-11 7.84102228e-11 1.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00]\nSum: 1.0000000001420395\nWeighted service time: 6.000000000369108\nInitial schedule: [2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 5]\nData saved successfully to 'datasets/parameters_22_20_10.pkl'\n\n\nWe will create a random set of pairs of neighboring schedules from within the neighborhood around schedule [2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 5].\nA neighbor of a schedule x is considered a schedule x’ where single patients have been shifted one interval to the left. Eg: ([2,1,1,2], [1,2,0,3]) are neighbors and ([2,1,1,2], [2,1,3,0]) are not, because [1,2,0,3] - [2,1,1,2] = [-1, 1, -1, 1] and [2,1,3,0] - [2,1,1,2] = [0, 0, 2, -2].\nService times will have a discrete distribution. The probability of a scheduled patient not showing up will be \\(q = 0.2\\).\nThe objective function will be the weighted average of the total waiting time of all patients and overtime. The model will be trained to predict which of the two neighboring schedules has the lowest objective value. The prediction time will be recorded. Then the same schedules will be evaluated by computing the objective value and then ranked.\n\n\n4.4.3 Variables\n\nIndependent Variables: A list of tuples with pairs of neighboring schedules.\nDependent Variables: A list with rankings for each tuple of pairwise schedules. Eg: If the rank for ([2,1,1], [1,1,2]) equals 0 this means that the schedule with index 0 ([2,1,1]) has the lowest objective value.\n\n\n\n4.4.4 Data Collection\nThe data set will be generated using simulation in which random samples will be drawn from the population of all possible schedules. For each sample a random neighboring schedule will be created.\n\n\n4.4.5 Sample Size and Selection\nSample Size: The total population size equals \\({{N + T -1}\\choose{N}} \\approx\\) 244663.0 mln. For this experiment we will be using a relatively small sample of 300000 pairs of schedules.\nSample Selection: The samples will be drawn from a lexicographic order of possible schedules in order to accurately reflect the combinatorial nature of the problem and to ensure unbiased sampling from the entire combinatorial space.\n\n\n4.4.6 Experimental Procedure\nThe experiment involves multiple steps, beginning with data preparation and concluding with model evaluation.The diagram below illustrates the sequence of steps.\n\n\n\n\n\ngraph TD\n    A[\"From population\"] --&gt;|\"Sample\"| B[\"Random subset\"]\n    B --&gt; |Create neighbors| C[\"Features: Schedule pairs\"]\n    C --&gt; |Calculate objectives| D[\"Objective values\"]\n    D --&gt; |Rank objectives| E[\"Labels: Rankings\"]\n    E --&gt; |\"Split dataset\"| F[\"Training set\"]\n    E --&gt; |\"Split dataset\"| G[\"Test set\"]\n    F --&gt; |\"Train\"| H[\"Model\"]\n    H[\"Model\"] --&gt; |\"Apply\"| G[\"Test set\"]\n    G[\"Test set\"] --&gt; |\"Evaluate\"| I[\"Performance\"]\n\n\n\n\n\n\nStep 1: Create pairs of neighboring schedules. A set of 300000 schedules will be sampled from the neighborhood of the initial schedule. For each schedule a pair of neighbors will be created. The order of the neighbors will be randomly switched to create a more diverse training set. The time taken to sample the schedules and create the neighbors will be recorded.\n\nfrom functions import get_v_star, get_neighborhood\n\ndef sample_neighbors_list(x: list[int], v_star: np.ndarray, all = True) -&gt; (list[int], list[int]):\n    \"\"\"\n    Create a set of pairs of schedules that are from the same neighborhood.\n    \n    Parameters:\n      x (list[int]): A list of integers with |s| = T and sum N.\n      v_star (np.ndarray): Precomputed vectors V* of length T.\n      \n    Returns:\n      tuple(list[int], list[int]): A pair of schedules.\n    \"\"\"\n    T = len(x)\n\n    # Precompute binomial coefficients (weights for random.choices)\n    binom_coeff = [math.comb(T, i) for i in range(1, T)]\n\n    # Choose a random value of i with the corresponding probability\n    i = random.choices(range(1, T), weights=binom_coeff)[0]\n\n    # Instead of generating the full list of combinations, sample one directly\n    j = random.sample(range(T), i)\n    \n    x_p = x.copy()\n    for k in j:\n        x_temp = np.array(x_p) + v_star[k]\n        x_temp = x_temp.astype(int)\n        if np.all(x_temp &gt;= 0):\n            x_p = x_temp.astype(int).tolist()\n    if all:\n        return x, x_p\n    else:    \n        return x_p\n\nstart = time.time()\nv_star = get_v_star(T)\n# Sample a set of schedules from the neighborhood of the initial schedule\nneighbors_selection = [sample_neighbors_list(initial_x, v_star, all = False) for i in range(num_schedules)] # This can be done in parallel to improve speed\nprint(len(neighbors_selection))\nend = time.time()\n# For the sampled schedules, create the neighbors\nneighbors_list = [sample_neighbors_list(schedule, v_star) for schedule in neighbors_selection]\n# Randomly switch the order of the neighbors\nneighbors_list = [neighbor if random.random() &lt; 0.5 else neighbor[::-1] for neighbor in neighbors_list]\nend = time.time()\nh = random.choices(range(num_schedules), k=7)\nprint(f\"Sampled schedules: {h}\")\nfor i in h:\n    original_schedule = neighbors_list[i][0]\n    neighbor_schedule = neighbors_list[i][1]\n    difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]\n    print(f\"Neighbors\\n{original_schedule}\\n{neighbor_schedule}\\n{difference}\")\ntraining_set_feat_time = end - start\nprint(f\"\\nProcessing time: {training_set_feat_time} seconds\\n\")\n\n300000\nSampled schedules: [297709, 111924, 88811, 264090, 4945, 156347, 211174]\nNeighbors\n[2, 0, 2, 1, 0, 1, 1, 1, 1, 0, 2, 0, 1, 2, 0, 1, 0, 0, 3, 4]\n[2, 0, 2, 0, 1, 1, 0, 1, 1, 1, 2, 0, 0, 2, 0, 1, 1, 0, 2, 5]\n[0, 0, 0, 1, -1, 0, 1, 0, 0, -1, 0, 0, 1, 0, 0, 0, -1, 0, 1, -1]\nNeighbors\n[1, 2, 0, 1, 0, 0, 2, 1, 0, 2, 1, 0, 1, 0, 2, 0, 1, 1, 0, 7]\n[2, 1, 0, 2, 0, 0, 2, 1, 0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 0, 6]\n[-1, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 1]\nNeighbors\n[4, 1, 0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 0, 2, 2, 0, 0, 1, 1, 4]\n[3, 1, 1, 0, 1, 0, 1, 1, 2, 0, 1, 1, 0, 2, 1, 1, 0, 0, 2, 4]\n[1, 0, -1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 1, -1, 0]\nNeighbors\n[1, 2, 1, 0, 1, 0, 2, 0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0, 1, 6]\n[1, 1, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 2, 0, 0, 7]\n[0, 1, -1, 0, 0, 0, 1, -1, 1, -1, 1, 0, 0, 0, 0, 0, -1, 0, 1, -1]\nNeighbors\n[2, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 2, 0, 1, 2, 4]\n[3, 0, 1, 2, 0, 0, 1, 2, 1, 0, 1, 1, 1, 0, 1, 2, 0, 0, 2, 4]\n[-1, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, 0]\nNeighbors\n[2, 1, 0, 2, 0, 1, 0, 2, 1, 1, 0, 0, 1, 2, 2, 0, 1, 0, 0, 6]\n[2, 1, 0, 2, 0, 1, 0, 1, 2, 0, 1, 0, 1, 1, 2, 1, 0, 1, 0, 6]\n[0, 0, 0, 0, 0, 0, 0, 1, -1, 1, -1, 0, 0, 1, 0, -1, 1, -1, 0, 0]\nNeighbors\n[2, 2, 0, 2, 0, 0, 2, 1, 0, 1, 2, 0, 0, 1, 1, 2, 0, 0, 1, 5]\n[3, 1, 0, 2, 0, 0, 2, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 5]\n[-1, 1, 0, 0, 0, 0, 0, -1, 0, 0, 1, -1, 0, 0, 0, 1, -1, 0, 1, 0]\n\nProcessing time: 56.18493580818176 seconds\n\n\n\nStep 2: For each schedule in each pair calculate the objective. For each pair save the index of the schedule that has the lowest objective value.\n\nfrom functions import calculate_objective_serv_time_lookup\n\nobjectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[0], d, convolutions)]\n]\nstart = time.time()\nobjectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[1], d, convolutions)]\n]\nend = time.time()\ntraining_set_lab_time = end - start\nobjectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]\nrankings = np.argmin(objectives, axis=1).tolist()\nfor i in range(5):\n    print(f\"Objectives: {objectives[i]}, Ranking: {rankings[i]}\")\n\nprint(f\"\\nProcessing time: {training_set_lab_time} seconds\\n\")\n\n# Step 1: Flatten the objectives into a 1D array\nflattened_data = [value for sublist in objectives for value in sublist]\n\n# Step 2: Find the index of the minimum value\nmin_index = np.argmin(flattened_data)\n\n# Step 3: Convert that index back to the original 2D structure\nrow_index = min_index // 2  # Assuming each inner list has 2 values\ncol_index = min_index % 2\n\nprint(f\"The minimum objective value is at index [{row_index}][{col_index}].\\nThis is schedule: {neighbors_list[row_index][col_index]} with objective value {objectives[row_index][col_index]}.\")\n\nfile_path_best_schedule = f\"datasets/best_schedule_{N}_{T}_{l}.pkl\"\nwith open(file_path_best_schedule, 'wb') as f:\n    pickle.dump({'best_schedule':neighbors_list[row_index][col_index], 'objective': objectives[row_index][col_index]}, f)\n    print(f\"Data saved successfully to '{file_path_best_schedule}'\")\n\nprint(f\"\\nAverage ranking: {np.mean(rankings)}\\n\")\n\n# Saving neighbors_list and objectives to a pickle file\nfile_path_neighbors = f\"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl\"\nwith open(file_path_neighbors, 'wb') as f:\n    pickle.dump({'neighbors_list': neighbors_list, 'objectives': objectives, 'rankings': rankings}, f)\n    print(f\"Data saved successfully to '{file_path_neighbors}'\")\n\nObjectives: [41.192163397073834, 33.91792953004357], Ranking: 1\nObjectives: [30.015257826244635, 34.82531832638152], Ranking: 0\nObjectives: [28.970935329939742, 31.69520476993224], Ranking: 0\nObjectives: [29.423810859939547, 32.22552602133364], Ranking: 0\nObjectives: [28.886313566007626, 31.062427422746687], Ranking: 0\n\nProcessing time: 94.84818005561829 seconds\n\nThe minimum objective value is at index [234129][1].\nThis is schedule: [2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3] with objective value 23.723087354309918.\nData saved successfully to 'datasets/best_schedule_22_20_10.pkl'\n\nAverage ranking: 0.49960666666666664\n\nData saved successfully to 'datasets/neighbors_and_objectives_22_20_10.pkl'\n\n\nStep 3: Create training and test sets.\n\n# Prepare the dataset\nX = []\nfor neighbors in neighbors_list:\n    X.append(neighbors[0] + neighbors[1])\n\nX = np.array(X)\ny = np.array(rankings)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 4: Train the XGBoost model.\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B[Initialize StratifiedKFold]\n    B --&gt; C[Initialize XGBClassifier]\n    C --&gt; D[Set results as empty list]\n    D --&gt; E[Loop through each split of cv split]\n    E --&gt; F[Get train and test indices]\n    F --&gt; G[Split X and y into X_train, X_test, y_train, y_test]\n    G --&gt; H[Clone the classifier]\n    H --&gt; I[Call fit_and_score function]\n    I --&gt; J[Fit the estimator]\n    J --&gt; K[Score on training set]\n    J --&gt; L[Score on test set]\n    K --&gt; M[Return estimator, train_score, test_score]\n    L --&gt; M\n    M --&gt; N[Append the results]\n    N --&gt; E\n    E --&gt; O[Loop ends]\n    O --&gt; P[Print results]\n    P --&gt; Q[End]\n\n\n\n\n\n\n\nclass CustomCallback(TrainingCallback):\n    def __init__(self, period=10):\n        self.period = period\n\n    def after_iteration(self, model, epoch, evals_log):\n        if (epoch + 1) % self.period == 0:\n            print(f\"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}\")\n        return False\n    \ndef fit_and_score(estimator, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0\n    )\n\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return estimator, train_score, test_score\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n\n# Initialize the XGBClassifier without early stopping here\n# Load the best trial parameters from a JSON file.\nwith open(\"model_params.json\", \"r\") as f:\n    model_params = json.load(f)\n    \n# Initialize the EarlyStopping callback with validation dataset\nearly_stop = xgb.callback.EarlyStopping(\n    rounds=10, metric_name='logloss', data_name='validation_0', save_best=True\n)\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n    early_stopping_rounds=9,\n    #callbacks=[CustomCallback(period=50), early_stop],\n    callbacks=[CustomCallback(period=50)],\n)\nprint(\"Params: \")\nfor key, value in model_params.items():\n    print(f\" {key}: {value}\")\n\nstart = time.time()\nresults = []\n\nfor train_idx, test_idx in cv.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    est, train_score, test_score = fit_and_score(\n        clone(clf), X_train, X_test, y_train, y_test\n    )\n    results.append((est, train_score, test_score))\nend = time.time()\ntraining_time = end - start\nprint(f\"\\nTraining time: {training_time} seconds\\n\")\n\nParams: \n max_depth: 6\n min_child_weight: 1\n gamma: 0.1\n subsample: 0.8\n colsample_bytree: 0.8\n learning_rate: 0.1\n n_estimators: 100\nEpoch 49, Evaluation log: 0.330827276971781\nEpoch 99, Evaluation log: 0.241263629116538\nEpoch 49, Evaluation log: 0.3329712547330962\nEpoch 99, Evaluation log: 0.24284057029957865\nEpoch 49, Evaluation log: 0.3354860251456111\nEpoch 99, Evaluation log: 0.24594283219885352\nEpoch 49, Evaluation log: 0.33087436630042893\nEpoch 99, Evaluation log: 0.24281658210926543\nEpoch 49, Evaluation log: 0.3315848726167266\nEpoch 99, Evaluation log: 0.23840238815734482\n\nTraining time: 8.271502256393433 seconds\n\n\n\nStep 5: To evaluate the performance of the XGBoost ranking model, we will use Stratified K-Fold Cross-Validation with 5 splits, ensuring each fold maintains the same class distribution as the original dataset. Using StratifiedKFold(n_splits=5, shuffle=True, random_state=94), the dataset will be divided into five folds. In each iteration, the model will be trained on four folds and evaluated on the remaining fold. A custom callback, CustomCallback(period=10), will print the evaluation log every 10 epochs.\nThe fit_and_score function will fit the model and score it on both the training and test sets, storing the results for each fold. This provides insight into the model’s performance across different subsets of the data, helps in understanding how well the model generalizes to unseen data and identifies potential overfitting or underfitting issues. The overall processing time for the cross-validation will also be recorded.\n\n# Print results\nfor i, (est, train_score, test_score) in enumerate(results):\n    print(f\"Fold {i+1} - Train Score (Accuracy): {train_score:.4f}, Test Score (Accuracy): {test_score:.4f}\")\n\nFold 1 - Train Score (Accuracy): 0.9298, Test Score (Accuracy): 0.9292\nFold 2 - Train Score (Accuracy): 0.9287, Test Score (Accuracy): 0.9283\nFold 3 - Train Score (Accuracy): 0.9278, Test Score (Accuracy): 0.9253\nFold 4 - Train Score (Accuracy): 0.9296, Test Score (Accuracy): 0.9288\nFold 5 - Train Score (Accuracy): 0.9316, Test Score (Accuracy): 0.9308\n\n\nTraining the model on the entire dataset provides a final model that has learned from all available data. Recording the training time helps in understanding the computational efficiency and scalability of the model with the given hyperparameters.\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nclf.save_model('models/classifier_large_instance.json')\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\\n\")\n\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\nTraining accuracy: 93.04%\n\n\nTraining time: 1.2039070129394531 seconds\n\n\n\n\n\n4.4.7 Validation\nGenerating test schedules and calculating their objectives and rankings allows us to create a new dataset for evaluating the model’s performance on unseen data.\n\nnum_test_schedules = 1000\n\n#test_schedules = random_combination_with_replacement(T, N, num_test_schedules)\ntest_schedules = [sample_neighbors_list(initial_x, v_star, all = False) for i in range(num_test_schedules)]\n\ntest_neighbors = [sample_neighbors_list(test_schedule, v_star) for test_schedule in test_schedules] # This can be done in parellel to improve speed\n\nprint(f\"Sampled: {len(test_schedules)} schedules\\n\")\n\ntest_objectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[0], d, convolutions)]\n]\n# Start time measurement for the evaluation\nstart = time.time()\ntest_objectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[1], d, convolutions)]\n]\ntest_rankings = [0 if test_obj &lt; test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]\nend = time.time()\nevaluation_time = end - start\n\n# Combine the objectives for each pair for later processing\ntest_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]\n\nprint(f\"\\nEvaluation time: {evaluation_time} seconds\\n\")\n\nfor i in range(6):\n    print(f\"Neighbors: {test_neighbors[i]},\\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\\n\")\n\nSampled: 1000 schedules\n\n\nEvaluation time: 0.3122389316558838 seconds\n\nNeighbors: ([2, 2, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 2, 0, 1, 0, 2, 4], [1, 2, 2, 0, 0, 1, 0, 2, 0, 2, 1, 1, 0, 0, 2, 1, 0, 1, 2, 4]),\nObjectives: [29.701457292464866, 31.715569768348942], Ranking: 0\n\nNeighbors: ([1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 2, 0, 2, 0, 1, 0, 6], [1, 0, 2, 1, 0, 1, 2, 1, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0, 0, 7]),\nObjectives: [32.852065799807626, 41.51282904718643], Ranking: 0\n\nNeighbors: ([2, 2, 1, 0, 2, 0, 1, 1, 0, 2, 1, 0, 1, 0, 1, 1, 1, 0, 1, 5], [3, 1, 1, 1, 1, 0, 1, 1, 0, 3, 0, 0, 1, 0, 1, 2, 0, 0, 2, 4]),\nObjectives: [30.925256669858793, 31.973426712512044], Ranking: 0\n\nNeighbors: ([2, 1, 1, 2, 1, 0, 0, 2, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 0, 5], [1, 2, 1, 2, 0, 0, 0, 2, 0, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 5]),\nObjectives: [29.217772508491286, 31.270721985609374], Ranking: 0\n\nNeighbors: ([2, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 2, 1, 1, 0, 1, 1, 0, 5], [1, 2, 2, 0, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 2, 0, 0, 6]),\nObjectives: [30.389467121853016, 34.18981552935611], Ranking: 0\n\nNeighbors: ([2, 2, 0, 2, 0, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 5], [2, 2, 0, 2, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1, 1, 2, 0, 0, 0, 5]),\nObjectives: [30.236820391496245, 31.378353603885415], Ranking: 0\n\n\n\nMaking predictions on new data and comparing them to the actual rankings provides an evaluation of the model’s performance in practical applications. Recording the prediction time helps in understanding the model’s efficiency during inference.\n\ninput_X = test_neighbors\nX_new = []\nfor test_neighbor in input_X:\n    X_new.append(test_neighbor[0] + test_neighbor[1])\n    \n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.004350185394287109 seconds\n\ntest_rankings = [0 0 0 0 0 0], \ny_pred = [0 0 0 0 0 0], \ny_pred_proba = \n[[0.8908624  0.10913759]\n [0.987966   0.012034  ]\n [0.7802712  0.2197288 ]\n [0.63890153 0.36109847]\n [0.9876534  0.0123466 ]\n [0.5742072  0.42579284]]\n\n\nCalculating the ambiguousness of the predicted probabilities helps in understanding the model’s confidence in its predictions. High ambiguousness indicates uncertain predictions, while low ambiguousness indicates confident predictions.\nAmbiguousness is calculated using the formula for entropy:\n\\[\nH(X) = - \\sum_{i} p(x_i) \\log_b p(x_i)\n\\]\nWhere in our case:\n\n\\(H(X)\\) is the ambiguousness of the random variable \\(X\\) - the set of probability scores for the predicted rankings,\n\\(p(x_i)\\) is probability score \\(x_i\\),\n\\(\\log_b\\) is the logarithm with base \\(b\\) (here \\(\\log_2\\) as we have two predicted values),\nThe sum is taken over all possible outcomes of \\(X\\).\n\nCalculating cumulative error rate and cumulative accuracy helps in understanding how the model’s performance evolves over the dataset.\nVisualizing the relationship between ambiguousness and error provides insights into how uncertainty in the model’s predictions correlates with its accuracy. This can help in identifying patterns and understanding the conditions under which the model performs well or poorly.\n\nfrom functions import calculate_ambiguousness\n\nerrors = np.abs(y_pred - np.array(test_rankings))\n\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9)))\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()\n\n                                                \n\n\n\n\n4.4.8 Hyperparameter Optimization\nIn the initial model the choice of hyperparameters was based on default values, examples from demo’s or trial and error. To improve the model’s performance, we applied a hyperparameter optimization technique to find the best set of hyperparameters. We used a grid search with cross-validation to find the optimal hyperparameters for the XGBoost model. The grid search was performed over a predefined set of hyperparameters, and the best hyperparameters were selected based on the model’s performance on the validation set. The best hyperparameters were then used to train the final model.\n\nfrom functions import compare_json\n\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n    \ndifferences = compare_json(model_params, best_trial_params)\n\nparams_tbl = pd.DataFrame(differences)\nparams_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)\nprint(params_tbl)\n\n                      max_depth     gamma  subsample  colsample_bytree  \\\nbase parameters               6  0.100000   0.800000          0.800000   \noptimized parameters          5  0.304548   0.781029          0.922528   \n\n                      learning_rate  n_estimators  \nbase parameters            0.100000           100  \noptimized parameters       0.239488           490  \n\n\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\n# Load the best trial parameters from a JSON file.\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\")\n\n\nTraining time: 4.910453796386719 seconds\n\nTraining accuracy: 97.38%\n\n\n\n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.0065288543701171875 seconds\n\ntest_rankings = [0 0 0 0 0 0], \ny_pred = [0 0 0 0 0 0], \ny_pred_proba = \n[[9.9449468e-01 5.5053122e-03]\n [9.9999994e-01 5.4009387e-08]\n [9.4748026e-01 5.2519754e-02]\n [9.7302395e-01 2.6976075e-02]\n [9.9998993e-01 1.0066873e-05]\n [8.9615613e-01 1.0384388e-01]]\n\n\n\nerrors = np.abs(y_pred - np.array(test_rankings))\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors, \"Schedules\": test_neighbors, \"Objectives\": test_objectives}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n\n\n\n\n\n\n\nAmbiguousness\nError\nSchedules\nObjectives\nCumulative error rate\nCumulative accuracy\n\n\n\n\n296\n3.640491e-08\n0\n([1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 0, 1, 0, 2, 1,...\n[30.661641910651696, 39.30636469198758]\n0.0\n1.0\n\n\n885\n5.872603e-08\n0\n([1, 1, 1, 1, 1, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1,...\n[30.088477020892046, 38.69555704455301]\n0.0\n1.0\n\n\n127\n8.667784e-08\n0\n([3, 1, 1, 0, 1, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1,...\n[29.628436760540822, 35.263428221789745]\n0.0\n1.0\n\n\n612\n1.884952e-07\n0\n([1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 0, 2, 0, 2,...\n[33.32991990572627, 42.335310966341225]\n0.0\n1.0\n\n\n430\n2.506813e-07\n0\n([1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1,...\n[29.760357965079237, 37.65524385585658]\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9),\n                    customdata=df[[\"Schedules\", \"Objectives\"]],\n                    hovertemplate=\n                        \"Ambiguousness: %{x} &lt;br&gt;\" +\n                        \"Error: %{y} &lt;br&gt;\" +\n                        \"Schedules: %{customdata[0][0]} / %{customdata[0][1]} &lt;br&gt;\" +\n                        \"Objectives: %{customdata[1]} &lt;br&gt;\"\n                    ))\n                  \nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#results",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#results",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.5 Results",
    "text": "4.5 Results\nWe wanted to test whether an XGBoost classification model could be used to assess and rank the quality of pairs of schedules. For performance benchmarking we use the conventional calculation method utilizing Lindley recursions.\nWe trained the XGBoost ranking model with a limited set of features (schedules) and labels (objectives). The total number of possible schedules is approximately 244663.0 million. For training and evaluation, we sampled 600000 schedules and corresponding neighbors. Generating the feature and label set took a total of 151.0331 seconds, with the calculation of objective values accounting for 94.8482 seconds.\nThe model demonstrates strong and consistent performance with high accuracies both for training, testing and validation (96.1%) with good generalization and stability. Total training time for the final model was 4.9105 seconds. The evaluation of 1000 test schedules took 0.0065 seconds for the the XGBoost model and 0.3122 for the conventional method, which is an improvement of 47X.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#discussion",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#discussion",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.6 Discussion",
    "text": "4.6 Discussion\n\ntraining_time = round(modeling_time, 4)\nconventional_time = round(evaluation_time, 4)\nxgboost_time = round(prediction_time, 4)\n\n# Define time values for plotting\ntime_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds\n\n# Calculate evaluations for method 1\nmethod1_evaluations = np.where(time_values &gt;= training_time, (time_values - training_time) / xgboost_time * 1000, 0)\n\n# Calculate evaluations for method 2\nmethod2_evaluations = time_values / conventional_time * 1000\n\n# Create line chart\nfig = go.Figure()\n\n# Add method 1 trace\nfig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))\n\n# Add method 2 trace\nfig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))\n\n# Update layout\nfig.update_layout(\n    title=\"Speed comparison between XGBoost ranking model and conventional method\",\n    xaxis_title=\"Time (seconds)\",\n    yaxis_title=\"Number of Evaluations\",\n    legend_title=\"Methods\",\n    template=\"plotly_white\"\n)\n\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#timeline",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#timeline",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.7 Timeline",
    "text": "4.7 Timeline\nThis experiment was started on 26-04-2025. The expected completion date is 26-04-2025.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#references",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#references",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.8 References",
    "text": "4.8 References",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html",
    "href": "local-search-ranking-large.html",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "",
    "text": "5.1 Objective\nTest the working and performance of a previously trained XGBoost Ranking model in a local search application.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#background",
    "href": "local-search-ranking-large.html#background",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.2 Background",
    "text": "5.2 Background\nIn previous experiments, we trained an XGBoost Classifier model to predict the objective values of neighboring schedules. In this experiment, we will use the trained models to perform a local search to find the best schedule.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#hypothesis",
    "href": "local-search-ranking-large.html#hypothesis",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.3 Hypothesis",
    "text": "5.3 Hypothesis\nThe XGBoost Classifier model will be able to efficiently guide the local search algorithm to find a schedule with a lower objective value than the initial schedule.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#methodology",
    "href": "local-search-ranking-large.html#methodology",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.4 Methodology",
    "text": "5.4 Methodology\n\n5.4.1 Tools and Materials\n\nimport numpy as np\nimport json\nimport time\nfrom itertools import chain, combinations\nimport sys\nfrom math import comb  # Python 3.8 and later\nimport xgboost as xgb\nimport pickle\nfrom typing import List, Tuple, Dict, Iterable, TypeVar, Union, Any, Optional, Literal\n\nimport logging\nimport sys # Needed for StreamHandler in order to enable explicit console output\n\n# Logging configuration\nlog_level = logging.DEBUG # DEBUG or INFO\nlog_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n\n# Log to a file instead of to the console:\nlogging.basicConfig(level=log_level, format=log_format, filename='search.log', filemode='w')\n\n# Get a logger instance\nlogger = logging.getLogger(__name__)\n\n\n\n5.4.2 Load Parameters\n\nN = 22 # Number of patients\nT = 20 # Number of intervals\nl = 10 # Target service time length\n\nfile_path_parameters = f\"datasets/parameters_{N}_{T}_{l}.pkl\" # For retrieving saved scheduling parameters\n# Load the data from the pickle file\nwith open(file_path_parameters, 'rb') as f:\n    data_params = pickle.load(f)\n\nN = data_params['N'] # Number of patients\nT = data_params['T'] # Number of intervals\nd = data_params['d'] # Length of each interval\nmax_s = data_params['max_s'] # Maximum service time\nq = data_params['q'] # Probability of a scheduled patient not showing up\nw = data_params['w'] # Weight for the waiting time in objective function\nl = data_params['l']\n  \nnum_schedules = data_params['num_schedules'] # Size of training set\nconvolutions = data_params['convolutions'] # Service time distributions used in training phase adjusted for no-shows\nprint(f\"Parameters loaded: N={N}, T={T}, l={l}, d={d}, max_s={max_s}, q={q}, w={w}, num_schedules={num_schedules}\")\n\nParameters loaded: N=22, T=20, l=10, d=5, max_s=20, q=0.2, w=0.1, num_schedules=300000\n\n\n\n\n5.4.3 Experimental Design\nWe will use the trained XGBoost Classifier model to guide a local search algorithm to find the best schedule. The local search algorithm will start with an initial schedule and iteratively explore the neighborhood of the current schedule to find a better one. As an initial schedule, we will use the schedule with the lowest objective value from the training dataset that was used to train the XGBoost Classifier model.\n\n\n5.4.4 Variables\n\nIndependent Variables:\n\nInitial schedule, trained XGBoost Classifier\n\nDependent Variables:\n\nSpeed, accuracy, and convergence of the local search algorithm.\n\n\n\n\n5.4.5 Data Collection\nWe will use the training dataset to initialize the local search algorithm.\n\n\n5.4.6 Sample Size and Selection\n\n\n5.4.7 Experimental Procedure\n\n\n\n\n\ngraph TD\n                A[Start] --&gt; B(\"Initialize schedule x\");\n                B --&gt; C{\"Iterate through all subsets U of V*\"};\n                C -- \"For each U\" --&gt; D{\"Compute y = x + sum(v in U)\"};\n                D -- \"Check y &gt;= 0\" --&gt; E{\"Compute cost C(y)\"};\n                E --&gt; F{\"Is C(y) &lt; C(x)?\"};\n                F -- \"Yes\" --&gt; G[\"Update x := y\"];\n                G --&gt; C;\n                F -- \"No\" --&gt; H{\"Finished iterating all U?\"};\n                H -- \"Yes\" --&gt; I[\"End: x is optimal schedule\"];\n                H -- \"No\" --&gt; C;\n                D -- \"If y &lt; 0\" --&gt; C;",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#results",
    "href": "local-search-ranking-large.html#results",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.5 Results",
    "text": "5.5 Results\n\n5.5.1 Load the initial best schedule.\nStart with the best solution found so far \\(\\{x^*, C(x^*)\\}\\) from the training set.\n\n# Load the best solution from the training dataset\nfile_path_schedules = f\"datasets/best_schedule_{N}_{T}_{l}.pkl\"\n# Load the data from the pickle file\nwith open(file_path_schedules, 'rb') as f:\n    best_schedule_data = pickle.load(f)\n    \nprint(f\"The data has following keys: {[key for key in best_schedule_data.keys()]}\")\n\nprint(f\"The current best schedule is: {best_schedule_data['best_schedule']} with objective value {best_schedule_data['objective']}.\")\n\n# Set the initial schedule to the best solution from the training dataset\ninitial_schedule = best_schedule_data['best_schedule']\n\nThe data has following keys: ['best_schedule', 'objective']\nThe current best schedule is: [2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3] with objective value 23.723087354309918.\n\n\n\n\n5.5.2 Generate the neighborhood of \\(x^*\\).\n\n5.5.2.1 Define \\(V^*\\) and \\(U_t\\).\nDefine the vectors \\(V^*\\) as follows:\n\\[\n\\left\\{\n\\begin{array}{c}\n\\vec{v_1}, \\\\\n\\vec{v_2}, \\\\\n\\vec{v_3}, \\\\\n\\vdots \\\\\n\\vec{v_{T-1}}, \\\\\n\\vec{v_T} \\\\\n\\end{array}\n\\right\\} =\n\\left\\{\n\\begin{array}{c}\n(-1, 0,...., 0, 1), \\\\\n(1, -1, 0,...., 0), \\\\\n(0, 1, -1,...., 0), \\\\\n\\vdots \\\\\n(0,...., 1, -1, 0), \\\\\n(0,...., 0, 1, -1) \\\\\n\\end{array}\n\\right\\}\n\\]\nDefine \\(U_t\\) as the set of all possible subsets of \\(V^*\\) such that each subset contains exactly \\(t\\) elements, i.e.,\n\\[\nU_t = \\{ S \\subsetneq V^* \\mid |S| = t \\}, \\quad t \\in \\{1, 2, \\dots, T\\}.\n\\]\n\nfrom functions import get_v_star\n\ndef powerset(iterable, size=1):\n    \"powerset([1,2,3], 2) --&gt; (1,2) (1,3) (2,3)\"\n    return [[i for i in item] for item in combinations(iterable, size)]\n  \nx = initial_schedule\n\n# Generate a matrix 'v_star' using the 'get_v_star' function\nv_star = get_v_star(T)\n\n# Generate all possible non-empty subsets (powerset) of the set {0, 1, 2, ..., t-1}\n# 'ids' will be a list of tuples, where each tuple is a subset of indices\nsize = 2\nids = powerset(range(T), size)\nlen(ids)\nids[:T]\n\n[[0, 1],\n [0, 2],\n [0, 3],\n [0, 4],\n [0, 5],\n [0, 6],\n [0, 7],\n [0, 8],\n [0, 9],\n [0, 10],\n [0, 11],\n [0, 12],\n [0, 13],\n [0, 14],\n [0, 15],\n [0, 16],\n [0, 17],\n [0, 18],\n [0, 19],\n [1, 2]]\n\n\n\n\n5.5.2.2 Define the neighborhood of \\(x\\)\nDefine the neighborhood of \\(x\\) as all vectors of the form \\(x + u_{tk}, \\forall \\, u_{tk} \\in U_t\\).\n\nfrom functions import get_neighborhood\ntest_nh = get_neighborhood(x, v_star, ids)\nprint(f\"All neighborhoods with {size} patients switched:\\n x = {np.array(x)}: \\n {test_nh}\")\n\nAll neighborhoods with 2 patients switched:\n x = [2 1 1 2 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 3]: \n [[2 0 1 ... 1 1 4]\n [1 2 0 ... 1 1 4]\n [1 1 2 ... 1 1 4]\n ...\n [2 1 1 ... 1 0 3]\n [2 1 1 ... 0 2 2]\n [2 1 1 ... 2 1 2]]\n\n\n\n\n\n5.5.3 Local search algorithm with prediction\nLoad the pre-trained model and use it for evaluating schedules within a local search algorithm. The search algorithm checks for false positives (prediction improvement = “True”, actual is improvement = “False”) and false negatives (prediction improvement = “False”, actual is improvement = “True”). In both cases the model is updated using the schedules and associated objective values (rankings).\n\n\n\n\n\ngraph TD\n    %% --- Part 1: Initialization & Outer Loop ---\n\n    A[Start: local_search_predict_update] --&gt; B{Inputs: x, w, v_star, clf, params, size, restarts, threshold};\n    B --&gt; C{\"Validate Inputs (clf, x length)\"};\n    C -- Valid --&gt; D[\"Initialize: x_star, T, restart_count=0, t=1\"];\n    C -- Invalid --&gt; Z_Err1[\"Raise ValueError\"];\n    D --&gt; E{\"Calculate Initial cost_star\"};\n    E -- Success --&gt; F{\"Outer Loop: t &lt;= size AND restart_count &lt; restarts?\"};\n    E -- Error --&gt; Z_Err2[\"Return x_star, clf\"];\n\n    %% Connections FROM other parts back to the Outer Loop check (F)\n    Connector_O([From Part 2: Break Inner Loop]) --&gt; F;\n    Connector_CC_Yes([From Part 3: Found Better at Level t]) --&gt; F;\n    Connector_DD([From Part 3: Incremented t]) --&gt; F;\n\n    %% Connections TO other parts\n    F -- No --&gt; Y[\"End: Return x_star, clf\"];\n    F -- Yes --&gt; G[\"Generate Neighborhood (level t)\"];\n    G --&gt; Connector_H([To Part 2: Start Inner Loop]);\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n    %% --- Part 2: Inner Loop - Neighbor Evaluation ---\n\n    Connector_G([From Part 1: Generate Neighborhood]) --&gt; H{\"Inner Loop: For each neighbor\"};\n\n    H -- Next Neighbor --&gt; I{\"Predict with clf: prediction, P(0)\"};\n    I -- Error Predicting --&gt; I_Err[\"Log Error, Assume P=0\"];\n    I_Err --&gt; J;\n    I -- Success --&gt; J{\"Perform Expensive Check? (Pred=1 OR P(0) &lt; threshold)\"};\n    J -- No --&gt; H_Next[Next Neighbor]; %% Skip expensive check\n    J -- Yes --&gt; K{\"Calculate True Cost (Expensive Objective Func)\"};\n    K -- Error --&gt; K_Err[\"Log Error\"];\n    K_Err --&gt; H_Next;\n    K -- Success --&gt; L{\"Is neighbor truly better? (cost_neighbor &lt; cost_star)\"};\n\n    %% Path 1: Improvement Found\n    L -- Yes --&gt; M[\"Update x_star, cost_star, T\"];\n    M --&gt; N[\"Set found_better=True, t=1, restart_count++\"];\n    N --&gt; O[\"Break Inner Loop\"];\n    O --&gt; Connector_F1([To Part 1: Outer Loop Check]); %% Connects back to F\n\n    %% Path 2: No Improvement\n    L -- No --&gt; P{\"Misprediction? (Pred=1 AND Actual=0)\"};\n    P -- No --&gt; Q[\"Log Borderline/Correct Pred=0\"];\n    Q --&gt; H_Next;\n    P -- Yes --&gt; R[\"Log Misprediction\"];\n    R --&gt; Connector_S([To Part 3: Start Retraining]); %% Trigger Retraining\n\n    %% Loop Control\n    H_Next --&gt; H; %% Process next neighbor\n    H -- End of Neighbors --&gt; BB{\"End Inner Loop\"};\n    BB --&gt; Connector_BB([To Part 3: Check Level Result]);\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n    %% --- Part 3: Retraining Sub-routine & Loop Control ---\n\n    %% Retraining Sub-routine Start\n    Connector_R([From Part 2: Misprediction Detected]) --&gt; S[\"Start Retraining Sub-routine\"];\n    subgraph Retraining Sub-routine\n        direction TB\n        S --&gt; T{\"Calculate True Costs for ALL neighbors at level t\"};\n        T --&gt; U{\"Opportunistic Better Found during Cost Calc?\"};\n        U -- Yes --&gt; V[\"Update x_star, cost_star, T\"];\n        V --&gt; W[\"Set found_better_retrain=True\"];\n        W --&gt; X[\"Collect Data: Append features/labels for update\"];\n        U -- No --&gt; X;\n        X --&gt; X_Loop{\"More neighbors to process for retraining?\"};\n        X_Loop -- Yes --&gt; T;\n        X_Loop -- No --&gt; Y_Fit{\"Fit clf incrementally\"};\n        Y_Fit -- Error --&gt; Y_FitErr[\"Log Fit Error\"];\n        Y_FitErr --&gt; Z_CheckOpp{\"Check if found_better_retrain?\"};\n        Y_Fit -- Success --&gt; Z_CheckOpp;\n    end\n\n    %% Retraining Outcome\n    Z_CheckOpp -- Yes --&gt; AA[\"Set found_better=True, t=1, restart_count++\"];\n    AA --&gt; Connector_O([To Part 1: Outer Loop Check via Break]); %% Connects back to F via O\n    Z_CheckOpp -- No --&gt; Connector_H_Next([To Part 2: Next Neighbor]); %% Retraining finished, continue inner loop\n\n    %% Inner Loop Finished - Level Control Logic\n    Connector_BB([From Part 2: End Inner Loop]) --&gt; CC{\"Found better solution at level t?\"};\n    CC -- Yes --&gt; Connector_F2([To Part 1: Outer Loop Check]); %% Restart checks from t=1\n    CC -- No --&gt; DD[\"Increment t\"];\n    DD --&gt; Connector_F3([To Part 1: Outer Loop Check]); %% Continue outer loop with next t\n\n\n\n\n\n\n\ndef local_search_predict(\n    x: List[int],\n    w: float,\n    v_star: np.ndarray,\n    clf: xgb.XGBClassifier,\n    obj_func_params: Dict[str, Any],\n    size: int = 2,\n    restarts: int = 3,\n    check_proba_threshold: float = 0.7,\n    retrain_on: Literal['both', 'fp', 'fn', 'none'] = 'fp'\n) -&gt; Tuple[List[int], xgb.XGBClassifier]:\n    \"\"\"\n    Performs local search guided by an XGBClassifier, minimizing expensive\n    objective calls. Verifies prediction=0 if P(class=0) is below threshold.\n    Updates the classifier incrementally when specified mispredictions occur.\n    Uses logging instead of print statements. T is inferred from len(x).\n\n    Args:\n        x (List[int]): Starting point.\n        w (float): Weight for combining objectives.\n        v_star (np.ndarray): Current best overall solution (used for guidance).\n        clf (xgb.XGBClassifier): Pre-trained XGBoost Classifier.\n        obj_func_params (Dict[str, Any]): Parameters for objective function.\n        size (int, optional): Max neighborhood size. Defaults to 2.\n        restarts (int, optional): Max restarts. Defaults to 3.\n        check_proba_threshold (float, optional): Threshold for P(class=0) verification. Defaults to 0.7. # Corrected default in comment\n        retrain_on (Literal['both', 'fp', 'fn', 'none'], optional):\n            Specifies when to trigger retraining based on misprediction type:\n            - 'both': Retrain on False Positives (P=1, A=0) and False Negatives (P=0, A=1).\n            - 'fp': Retrain only on False Positives. (Default) # Corrected default in comment\n            - 'fn': Retrain only on False Negatives.\n            - 'none': Never retrain based on mispredictions.\n            Defaults to 'fp'.\n\n    Returns:\n        Tuple[List[int], xgb.XGBClassifier]: Best solution found and potentially updated classifier.\n    \"\"\"\n    # --- Input Validation ---\n    # Check if clf appears fitted (basic check)\n    if not hasattr(clf, 'classes_') or not hasattr(clf, 'n_features_in_'):\n         logger.warning(\"Classifier 'clf' may not be fitted. Proceeding with caution.\")\n         # Depending on strictness, you might raise an error here instead.\n         # raise ValueError(\"Classifier 'clf' must be fitted before use.\")\n\n    if not x:\n        logger.error(\"Input schedule x cannot be empty (length must be positive).\")\n        raise ValueError(\"Input schedule x cannot be empty (length must be positive).\")\n\n    allowed_retrain_values = {'both', 'fp', 'fn', 'none'}\n    if retrain_on not in allowed_retrain_values:\n        logger.error(\"Invalid value for 'retrain_on': %s. Must be one of %s\", retrain_on, allowed_retrain_values)\n        raise ValueError(f\"Invalid value for 'retrain_on'. Must be one of {allowed_retrain_values}\")\n\n    # --- Initialization ---\n    x_star = list(x) # Work with a copy\n    T = len(x_star) # Infer T from the length - calculated once initially\n    restart_count = 0\n    t = 1 # Start with neighborhood size 1\n\n    # Calculate initial cost\n    try:\n        logger.info(\"Calculating initial cost...\")\n        objectives_star = calculate_objective_serv_time_lookup(x_star, **obj_func_params)\n        cost_star = w * objectives_star[0] + (1 - w) * objectives_star[1]\n        logger.info(\"Initial solution cost: %.4f\", cost_star)\n    except Exception as e:\n        logger.exception(\"Error calculating initial cost: %s\", e)\n        return x_star, clf # Return current best and original classifier on error\n\n    # --- Main Search Loop ---\n    while t &lt;= size and restart_count &lt; restarts:\n        logger.info(\"--- Running local search level t=%d (Restart %d/%d) ---\", t, restart_count + 1, restarts)\n\n        ids_gen_iterable = powerset(range(T), t) # Use current T\n        # Pass x_star (current best) to neighborhood generation\n        neighborhood_iter = get_neighborhood(x_star, v_star, ids_gen_iterable)\n\n        found_better_solution_at_level_t = False\n        neighbors_data_at_level_t: List[Dict[str, Any]] = [] # Store data for potential retraining\n        neighbors_processed_count = 0\n\n        for neighbor_np in neighborhood_iter:\n            neighbors_processed_count += 1\n            neighbor = neighbor_np.tolist() # Convert numpy array to list\n            neighbor_info = {\"schedule\": neighbor, \"cost\": None, \"true_label\": None, \"prediction\": None}\n            neighbors_data_at_level_t.append(neighbor_info) # Add neighbor info early\n\n            # --- Feature Creation ---\n            # Feature is concatenation - ensure this matches how clf was trained\n            feature_pair = x_star + neighbor\n\n            # --- 1. Predict using the CHEAP classifier ---\n            prediction = 0 # Default prediction\n            proba_class_0 = 1.0 # Default probability\n            try:\n                # Reshape feature_pair for XGBoost if needed (expects 2D array)\n                feature_pair_np = np.array(feature_pair).reshape(1, -1)\n                prediction = clf.predict(feature_pair_np)[0]\n                proba = clf.predict_proba(feature_pair_np)[0]\n                # Ensure proba has expected structure (e.g., 2 elements for binary class)\n                if len(proba) &gt; 0:\n                   proba_class_0 = proba[0] # Probability of class 0\n                else:\n                   logger.warning(\"Predict_proba returned unexpected structure: %s. Using default P(0)=1.0\", proba)\n            except Exception as e:\n                logger.warning(\"Error predicting for neighbor %d: %s. Assuming prediction=0.\", neighbors_processed_count, e)\n                # Keep default prediction=0, proba_class_0=1.0\n\n            neighbor_info[\"prediction\"] = prediction # Store prediction\n            logger.debug(\"  Neighbor %d: Predicted=%d (P(0)=%.3f)\", neighbors_processed_count, prediction, proba_class_0)\n\n            # --- 2. Decide whether to perform expensive check ---\n            perform_expensive_check = False\n            check_reason = \"\"\n\n            if prediction == 1:\n                perform_expensive_check = True\n                check_reason = \"Predicted 1\"\n            elif proba_class_0 &lt; check_proba_threshold:\n                perform_expensive_check = True\n                check_reason = f\"Borderline P(0) &lt; {check_proba_threshold:.3f}\"\n            else: # prediction == 0 and proba_class_0 &gt;= threshold\n                logger.debug(\"  -&gt; Skipping objective function call (Confident P=0).\")\n\n            # --- 3. Perform EXPENSIVE check if needed ---\n            if perform_expensive_check:\n                logger.debug(\"  -&gt; Verifying (%s)...\", check_reason)\n                try:\n                    objectives_neighbor = calculate_objective_serv_time_lookup(neighbor, **obj_func_params)\n                    cost_neighbor = w * objectives_neighbor[0] + (1 - w) * objectives_neighbor[1]\n                    is_truly_better = cost_neighbor &lt; cost_star\n                    true_label = 1 if is_truly_better else 0\n\n                    # Store results in neighbor_info\n                    neighbor_info[\"cost\"] = cost_neighbor\n                    neighbor_info[\"true_label\"] = true_label\n\n                    logger.debug(\"      True Cost=%.4f (Current Best=%.4f) -&gt; Actual Better=%s\",\n                                 cost_neighbor, cost_star, is_truly_better)\n\n                    # --- 4. Check for Misprediction and Trigger Retraining (Conditional) ---\n                    misprediction = (prediction != true_label)\n                    trigger_retraining = False\n                    opportunistic_update_occurred = False # Reset for this neighbor check\n\n                    if misprediction and retrain_on != 'none':\n                        misprediction_type = \"\"\n                        should_retrain_this_type = False\n\n                        if prediction == 1 and not is_truly_better: # False Positive (P=1, A=0)\n                            misprediction_type = \"False Positive (P=1, A=0)\"\n                            should_retrain_this_type = retrain_on in ['both', 'fp']\n                        elif prediction == 0 and is_truly_better: # False Negative (P=0, A=1)\n                            misprediction_type = \"False Negative (P=0, A=1)\"\n                            should_retrain_this_type = retrain_on in ['both', 'fn']\n\n                        if should_retrain_this_type:\n                            logger.warning(\"      Misprediction! (%s). Triggering retraining process based on 'retrain_on=%s'.\",\n                                           misprediction_type, retrain_on)\n                            trigger_retraining = True\n                        elif misprediction_type: # Misprediction occurred but not the type we retrain on\n                             logger.info(\"      Misprediction occurred (%s), but retraining is disabled for this type ('retrain_on=%s').\",\n                                         misprediction_type, retrain_on)\n\n                    # --- Retraining Sub-routine (if triggered) ---\n                    if trigger_retraining:\n                        features_for_update: List[List[int]] = []\n                        labels_for_update: List[int] = []\n                        best_opportunistic_neighbor = None\n                        best_opportunistic_cost = cost_star # Initialize with current best cost\n\n                        logger.info(\"      Calculating true costs for %d neighbors at level %d for retraining...\",\n                                    len(neighbors_data_at_level_t), t)\n\n                        for n_idx, n_info in enumerate(neighbors_data_at_level_t):\n                            n_schedule = n_info[\"schedule\"]\n                            n_cost = n_info[\"cost\"]\n                            n_true_label = n_info[\"true_label\"]\n\n                            # Calculate cost if not already done (e.g., for neighbors skipped earlier)\n                            if n_cost is None or n_true_label is None:\n                                try:\n                                    logger.debug(\"          Calculating missing cost for neighbor %d...\", n_idx+1)\n                                    n_objectives = calculate_objective_serv_time_lookup(n_schedule, **obj_func_params)\n                                    n_cost = w * n_objectives[0] + (1 - w) * n_objectives[1]\n                                    n_is_better = n_cost &lt; cost_star\n                                    n_true_label = 1 if n_is_better else 0\n                                    n_info[\"cost\"] = n_cost # Update info cache\n                                    n_info[\"true_label\"] = n_true_label\n                                except Exception as e:\n                                    logger.warning(\"          Error calculating cost for neighbor %d (%s) during retraining: %s. Skipping.\",\n                                                   n_idx+1, n_schedule, e)\n                                    continue # Skip this neighbor for training data\n\n                            # Prepare data for fitting\n                            n_feature_pair = x_star + n_schedule # Create feature pair for this neighbor\n                            features_for_update.append(n_feature_pair)\n                            labels_for_update.append(n_true_label)\n                            logger.debug(\"          Neighbor %d: Cost=%.4f, True Label=%d (Used for training)\",\n                                         n_idx+1, n_cost, n_true_label)\n\n                            # Check for opportunistic update (find the best neighbor *among those evaluated*)\n                            if n_true_label == 1 and n_cost &lt; best_opportunistic_cost:\n                                logger.info(\"          Opportunistic Update Candidate! Found/Confirmed better neighbor (%d) during cost calculation.\", n_idx+1)\n                                best_opportunistic_neighbor = list(n_schedule) # Store a copy of the schedule\n                                best_opportunistic_cost = n_cost # Update best cost found *during retraining*\n                                opportunistic_update_occurred = True\n\n\n                        # Perform incremental fit if data was gathered\n                        if features_for_update:\n                            logger.info(\"      Fitting model incrementally with %d data points...\", len(labels_for_update))\n                            try:\n                                X_update = np.array(features_for_update) # Convert list of lists to 2D numpy array\n                                y_update = np.array(labels_for_update)\n\n                                # Ensure clf is fitted before incremental update if it's the first time\n                                # XGBoost's fit with xgb_model handles this correctly.\n                                clf.fit(X_update, y_update, xgb_model=clf.get_booster()) # Pass the existing booster\n                                logger.info(\"      Model update complete.\")\n\n                            except Exception as e:\n                                logger.exception(\"      Error during incremental model update: %s\", e)\n                        else:\n                            logger.warning(\"      No valid data gathered for retraining.\")\n\n                        # If an opportunistic update was found, apply it now\n                        if opportunistic_update_occurred:\n                             logger.info(f\"      Applying opportunistic update. New best: {best_opportunistic_neighbor} with cost = {best_opportunistic_cost:.4f}.\")\n                             x_star = best_opportunistic_neighbor # Use the best one found (already a list)\n                             cost_star = best_opportunistic_cost\n                             T = len(x_star) # Update T as length might have changed\n                        # --- End of Retraining Sub-routine ---\n\n                    # --- 5. Handle Updates & Loop Control ---\n                    # Check if we should update x_star and restart the search level\n                    if opportunistic_update_occurred:\n                        found_better_solution_at_level_t = True # Mark improvement found\n                        t = 1 # Reset level\n                        restart_count += 1\n                        logger.info(\"      Restarting search from t=1 due to opportunistic update during retraining. Restart count: %d\", restart_count)\n                        break # Exit inner loop (for neighbor_np in neighborhood_iter)\n\n                    elif is_truly_better: # True Positive or handled False Negative (update to the current neighbor)\n                        logger.info(f\"      Confirmed better solution (or handled FN). Updating x_star to {neighbor} with cost = {cost_neighbor:.4f}.\")\n                        # CORRECTED: Assign neighbor directly as it's already a list\n                        x_star = neighbor\n                        cost_star = cost_neighbor\n                        T = len(x_star) # Update T as length might have changed\n                        found_better_solution_at_level_t = True\n                        t = 1 # Reset level\n                        restart_count += 1\n                        logger.info(\"      Restarting search from t=1. Restart count: %d\", restart_count)\n                        break # Exit inner loop (for neighbor_np in neighborhood_iter)\n\n                    # else: (Not truly better and no opportunistic update) -&gt; continue to next neighbor implicitly\n\n                except Exception as e:\n                    logger.warning(\"  Error calculating objective or handling result for neighbor %d (%s): %s.\",\n                                   neighbors_processed_count, neighbor, e)\n            # --- End of 'if perform_expensive_check:' ---\n\n        # --- End of neighbor loop (for neighbor_np in neighborhood_iter) ---\n\n        # If we finished the loop for level t without finding a better solution (or breaking early)\n        if not found_better_solution_at_level_t:\n            if neighbors_processed_count &gt; 0:\n                logger.info(\"No improving solution found or confirmed at level t=%d.\", t)\n            else:\n                logger.info(\"No neighbors generated or processed at level t=%d.\", t)\n            t += 1 # Move to the next neighborhood size level\n\n    # --- End of outer while loop ---\n    logger.info(\"Local search finished after %d restarts or reaching max size %d.\", restart_count, size)\n    logger.info(\"Final solution: %s\", x_star)\n    logger.info(\"Final cost: %.4f\", cost_star)\n\n    return x_star, clf\n\n\nfrom functions import calculate_objective_serv_time_lookup\nstart = time.time()\n# Define the path to the saved model\nmodel_path = \"models/classifier_large_instance.json\" # Make sure this path is correct\n\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n)\n\n# Load the model directly from the file path\nclf.load_model(model_path)\n\nintial_objectives = calculate_objective_serv_time_lookup(x, d, convolutions)\ninitial_c_star = w * intial_objectives[0] + (1 - w) * intial_objectives[1]\nx_star = local_search_predict(x, w, v_star, clf, {'d': d, 'convolutions': convolutions}, size=T, restarts=T)[0]\nfinal_objectives = calculate_objective_serv_time_lookup(x_star, d, convolutions)\nfinal_c_star = w * final_objectives[0] + (1 - w) * final_objectives[1]\nend = time.time()\nprint(f\"\\nInitial schedule: {x}, with objective value: {initial_c_star}.\\nFinal schedule: {x_star}, with objective value: {final_c_star}. Search time {end - start:.2f} seconds.\")\n\n\nInitial schedule: [2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3], with objective value: 23.723087354309918.\nFinal schedule: [2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 3], with objective value: 23.700684253723423. Search time 939.88 seconds.\n\n\n\n\n5.5.4 Run the conventional local search algorithm for validation\nWe will run a conventional local search algorithm to evaluate the new method, assessing both the quality of the results and its computational efficiency.\n\nfrom functions import local_search\n# Computing optimal solution with real cost\nprint(f\"Initial schedule: {x}\")\nstart = time.time()\ntest_x = local_search(x, d, convolutions, w, v_star, T, echo=True)\nend = time.time()\n\nInitial schedule: [2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3]\nInitial solution: [2 1 1 2 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 3], cost: 23.723087354309918\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 18\nFound better solution: [2 1 1 2 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 3], cost: 23.700818524462598\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 18\nFound better solution: [2 1 1 2 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 3], cost: 23.700684253723423\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 18\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 155\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 850\nRunning local search with switching 4 patient(s)\nSize of neighborhood: 3333\nRunning local search with switching 5 patient(s)\nSize of neighborhood: 9944\nRunning local search with switching 6 patient(s)\nSize of neighborhood: 23444\nRunning local search with switching 7 patient(s)\nSize of neighborhood: 44760\nRunning local search with switching 8 patient(s)\nSize of neighborhood: 70330\nFound better solution: [2 1 1 2 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2], cost: 23.020346596110915\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 19\nFound better solution: [1 1 1 2 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3], cost: 23.01025194826672\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 19\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 172\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 987\nRunning local search with switching 4 patient(s)\nSize of neighborhood: 4029\nFound better solution: [2 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 3], cost: 22.966437347631064\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 19\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 172\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 987\nRunning local search with switching 4 patient(s)\nSize of neighborhood: 4029\nRunning local search with switching 5 patient(s)\nSize of neighborhood: 12444\nRunning local search with switching 6 patient(s)\nSize of neighborhood: 30192\nRunning local search with switching 7 patient(s)\nSize of neighborhood: 58956\nRunning local search with switching 8 patient(s)\nSize of neighborhood: 94146\nRunning local search with switching 9 patient(s)\nSize of neighborhood: 124202\nRunning local search with switching 10 patient(s)\nSize of neighborhood: 136136\nRunning local search with switching 11 patient(s)\nSize of neighborhood: 124202\nRunning local search with switching 12 patient(s)\nSize of neighborhood: 94146\nRunning local search with switching 13 patient(s)\nSize of neighborhood: 58956\nRunning local search with switching 14 patient(s)\nSize of neighborhood: 30192\nFound better solution: [2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2], cost: 22.62783494379382\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 20\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 190\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 1140\nRunning local search with switching 4 patient(s)\nSize of neighborhood: 4845\nRunning local search with switching 5 patient(s)\nSize of neighborhood: 15504\nRunning local search with switching 6 patient(s)\nSize of neighborhood: 38760\nRunning local search with switching 7 patient(s)\nSize of neighborhood: 77520\nRunning local search with switching 8 patient(s)\nSize of neighborhood: 125970\nRunning local search with switching 9 patient(s)\nSize of neighborhood: 167960\nRunning local search with switching 10 patient(s)\nSize of neighborhood: 184756\nRunning local search with switching 11 patient(s)\nSize of neighborhood: 167960\nRunning local search with switching 12 patient(s)\nSize of neighborhood: 125970\nRunning local search with switching 13 patient(s)\nSize of neighborhood: 77520\nRunning local search with switching 14 patient(s)\nSize of neighborhood: 38760\nRunning local search with switching 15 patient(s)\nSize of neighborhood: 15504\nRunning local search with switching 16 patient(s)\nSize of neighborhood: 4845\nRunning local search with switching 17 patient(s)\nSize of neighborhood: 1140\nRunning local search with switching 18 patient(s)\nSize of neighborhood: 190\nRunning local search with switching 19 patient(s)\nSize of neighborhood: 20\n\n\n\nprint(f\"Initial schedule: {x}\\nFinal schedule: {test_x[0]}\\nDifference: {test_x[0] - x}\\nObjective value: {test_x[1]}. Search time: {end - start:.2f} seconds.\")\ntest_res = calculate_objective_serv_time_lookup(test_x[0], d, convolutions)\n\nInitial schedule: [2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3]\nFinal schedule: [2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2]\nDifference: [ 0  0  0 -1  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0 -1]\nObjective value: 22.62783494379382. Search time: 844.30 seconds.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#discussion",
    "href": "local-search-ranking-large.html#discussion",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.6 Discussion",
    "text": "5.6 Discussion",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#timeline",
    "href": "local-search-ranking-large.html#timeline",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.7 Timeline",
    "text": "5.7 Timeline\nThis experiment was started on 01-04-2025 and concluded on 17-04-2025",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#references",
    "href": "local-search-ranking-large.html#references",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.8 References",
    "text": "5.8 References",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html",
    "href": "combinatorial-bayes-optimization.html",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "",
    "text": "6.1 Objective\nThe objective of this experiment is to evaluate and compare the performance of two distinct Combinatorial Bayesian Optimization (CBO) strategies for an outpatient appointment scheduling problem. We investigate:\nWe aim to determine which strategy is most effective in identifying an optimal or near-optimal schedule, as measured by the objective function value, leveraging dictionary-based embeddings for the high-dimensional combinatorial space (Deshwal et al. 2023).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#objective",
    "href": "combinatorial-bayes-optimization.html#objective",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "",
    "text": "CBO utilizing Expected Improvement (EI) as the acquisition function.\nCBO utilizing Lower Confidence Bound (LCB) as the acquisition function with a fixed kappa (\\(\\kappa\\)) value.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#background",
    "href": "combinatorial-bayes-optimization.html#background",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.2 Background",
    "text": "6.2 Background\nWe consider an outpatient appointment scheduling problem as described by Kaandorp and Koole (2007) where the schedule is represented by a vector \\(\\mathbf{x} = (x_0, x_1, \\ldots, x_{T-1})^T\\). This vector comprises \\(T\\) components, where \\(x_j\\) denotes the non-negative allocation (number of patients) to time slot \\(j\\), for \\(j = 0, \\ldots, T-1\\). A fundamental constraint is that the total allocation across all time slots must equal a fixed constant \\(N\\): \\[\\sum_{j=0}^{T-1} x_j = N\\] We require \\(x_j \\ge 0\\) for all \\(j = 0, \\ldots, T-1\\). Consequently, a valid schedule \\(\\mathbf{x}\\) belongs to the feasible set \\(\\mathcal{F} = \\{ \\mathbf{z} \\in \\mathbb{D}^{T} \\mid \\sum_{j=0}^{T-1} z_j = N, z_j \\ge 0 \\text{ for all } j\\}\\), where \\(\\mathbb{D}\\) is the set of non-negative integers (\\(\\mathbb{Z}_{\\ge 0}\\)).\nKaandorp and Koole (2007) define a neighborhood structure for local search based on perturbation vectors derived from a set of \\(T\\) basis change vectors, \\(v_i \\in \\mathbb{D}^{T}\\), for \\(i = 0, \\ldots, T-1\\). These basis vectors represent elementary shifts of allocation between time slots:\n\n\\(v_0 = (-1, 0, \\ldots, 0, 1)\\) (Shift unit from slot 0 to slot \\(T-1\\))\n\\(v_1 = (1, -1, 0, \\ldots, 0)\\) (Shift unit from slot 1 to slot 0)\n\\(v_i = (0, \\ldots, 0, \\underbrace{1}_{\\text{pos } i-1}, \\underbrace{-1}_{\\text{pos } i}, 0, \\ldots, 0)\\) for \\(i = 2, \\ldots, T-1\\) (Shift unit from slot \\(i\\) to slot \\(i-1\\))\n\nA key property of these basis vectors is that the sum of components for each vector is zero: \\(\\sum_{j=0}^{T-1} v_{ij} = 0\\) for all \\(i=0, \\ldots, T-1\\).\nPerturbations are constructed using a binary selection vector \\(\\mathbf{U} = (u_0, u_1, \\ldots, u_{T-1})\\), where \\(u_i \\in \\{0, 1\\}\\). Each \\(u_i\\) indicates whether the basis change \\(v_i\\) is included in the perturbation. The resulting perturbation vector \\(\\mathbf{r}(\\mathbf{U}) \\in \\mathbb{D}^{T}\\) is the linear combination: \\[\\mathbf{r}(\\mathbf{U}) := \\sum_{i=0}^{T-1} u_i v_i\\]\nSince each \\(v_i\\) sums to zero, any perturbation \\(\\mathbf{r}(\\mathbf{U})\\) also sums to zero: \\(\\sum_{j=0}^{T-1} r_j(\\mathbf{U}) = 0\\). This ensures that applying such a perturbation to a valid schedule \\(\\mathbf{x}\\) preserves the total allocation \\(N\\).\nThe neighborhood of a schedule \\(\\mathbf{x} \\in \\mathcal{F}\\), denoted by \\(\\mathcal{N}(\\mathbf{x})\\), comprises all distinct, feasible schedules \\(\\mathbf{x}'\\) reachable by applying a non-zero perturbation \\(\\mathbf{r}(\\mathbf{U})\\) (Kaandorp and Koole (2007), use a slightly different but related neighborhood definition based on combinations of these basis vectors).\nThe objective function to be minimized is a weighted sum of Expected Waiting Time (EWT) and Expected Staff Penalty (ESP), as defined by Kaandorp and Koole (2007): \\[C(\\mathbf{x}) = w \\cdot EWT(\\mathbf{x}) + (1-w) \\cdot ESP(\\mathbf{x})\\] Kaandorp and Koole (2007) prove that this objective function is multimodular, which guarantees that a local search algorithm using their defined neighborhood converges to the global optimum.\nHowever, evaluating \\(C(\\mathbf{x})\\) can be computationally expensive, especially for large \\(N\\) and \\(T\\). Furthermore, the search space defined by the binary vectors \\(\\mathbf{U}\\) is high-dimensional (\\(2^T - 2\\) possibilities, excluding \\(\\mathbf{0}\\) and \\(\\mathbf{1}\\)). Bayesian Optimization (BO) is a suitable framework for optimizing such expensive black-box functions. Standard BO methods often struggle with high-dimensional combinatorial spaces. Deshwal et al. (2023) propose a method using dictionary-based embeddings (Hamming Embedding via Dictionaries - HED) to map the high-dimensional binary space of \\(\\mathbf{U}\\) vectors into a lower-dimensional continuous space, where standard Gaussian Process (GP) models can be effectively applied. This experiment applies the HED approach within a BO framework to solve the scheduling problem formulated by Kaandorp and Koole (2007).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#hypothesis",
    "href": "combinatorial-bayes-optimization.html#hypothesis",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.3 Hypothesis",
    "text": "6.3 Hypothesis\nWe hypothesize that:\n\nBoth CBO strategies, leveraging the HED embedding (Deshwal et al. 2023), will be capable of finding schedules superior to the initial schedule derived from the Bailey-Welch method (@).\nCBO strategies employing Lower Confidence Bound (LCB) may exhibit superior performance or faster convergence compared to Expected Improvement (EI), due to the explicit exploration-exploitation trade-off inherent in LCB.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#methodology",
    "href": "combinatorial-bayes-optimization.html#methodology",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.4 Methodology",
    "text": "6.4 Methodology\n\n6.4.1 Tools and Materials\n\nProgramming Language: Python 3\nCore Libraries: NumPy, SciPy\nMachine Learning: Scikit-learn (for GaussianProcessRegressor, MinMaxScaler)\nData Structures: Standard Python lists and dictionaries, NumPy arrays.\nImported functions: bailey_welch_schedule, get_v_star, compute_convolutions, calculate_objective_serv_time_lookup (implementing the logic from Bailey (1952), assumed to be in an external functions.py file).\n\n\n\n6.4.2 Experimental Design\nThree distinct Bayesian optimization experiments are conducted, applying the HED embedding approach (Deshwal et al. 2023) to the scheduling problem:\n\nExperiment 1: Expected Improvement (EI)\n\nAcquisition Function: Expected Improvement.\nObjective: Minimize \\(C(\\mathbf{x})\\) by iteratively selecting candidate vectors \\(\\mathbf{U}\\) (via their embeddings) that maximize the EI.\n\nExperiment 2: Lower Confidence Bound (LCB) - Fixed Kappa\n\nAcquisition Function: Lower Confidence Bound.\nObjective: Minimize \\(C(\\mathbf{x})\\) using a fixed kappa (\\(\\kappa\\)) value in the LCB acquisition function applied to the GP model over the embedded space.\n\n\nFor all experiments, Hamming Distance Embedding (HED) with a “diverse random” dictionary construction strategy (Deshwal et al. 2023) is employed to map the binary perturbation vectors \\(\\mathbf{U}\\) to a continuous embedding space. A Gaussian Process (GP) model with Automatic Relevance Determination (ARD) kernels models the (negative) objective function in this embedded space.\n\n\n6.4.3 Variables\n\nIndependent Variables:\n\nType of acquisition function (EI, LCB).\nThe specific binary perturbation vector \\(\\mathbf{U}\\) selected in each iteration (chosen via optimizing the acquisition function over the embedded space).\n\nDependent Variables:\n\nThe objective function value \\(C(\\mathbf{x}')\\) for the resulting schedule \\(\\mathbf{x}' = \\mathbf{x} + \\mathbf{r}(\\mathbf{U})\\) (calculated using the method from Kaandorp and Koole (2007)).\nThe best objective function value found throughout the optimization process.\n\n\n\n\n6.4.4 Data Collection\nData, comprising evaluated pairs \\((\\mathbf{U}, C(\\mathbf{x}'))\\), is collected iteratively:\n\nAn initial set of N_INITIAL randomly generated \\(\\mathbf{U}\\) vectors is evaluated.\nIn each of the subsequent N_ITERATIONS, BATCH_SIZE_q new \\(\\mathbf{U}\\) vectors are selected by optimizing the respective acquisition function over NUM_CANDIDATES_Acqf randomly generated candidate vectors in the original binary space (evaluated via their embeddings). These newly selected vectors are then evaluated, and the results are added to the dataset.\n\n\n\n6.4.5 Sample Size and Selection\n\nN_INITIAL: 20 (number of initial random evaluations)\nN_ITERATIONS: 20 (number of Bayesian optimization iterations)\nBATCH_SIZE_q: 5 (number of candidates selected and evaluated per iteration)\nNUM_CANDIDATES_Acqf: \\(T \\times 1024 = 20 \\times 1024 = 20480\\) (number of random candidates generated for optimizing the acquisition function in each iteration)\nm: 128 (dimensionality of the HED embedding space, following Deshwal et al. (2023))\n\nThe selection of new points for evaluation is guided by the respective acquisition function (EI or LCB) optimized over the embedded space representation of candidate \\(\\mathbf{U}\\) vectors.\n\n\n6.4.6 Experimental Procedure\n\n6.4.6.1 1. Setup\nImport necessary libraries and configure warning filters.\n\n# Core Libraries\nimport numpy as np\nimport time\nimport warnings\nfrom scipy.optimize import minimize\nfrom typing import List, Dict, Tuple, Callable, Optional, Union, Any, Iterable\n\n# Scikit-learn for GP, Scaling, and potentially acquisition functions\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, ConstantKernel, WhiteKernel\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.exceptions import ConvergenceWarning\n\n# SciPy for statistics (needed for Expected Improvement calculation)\nfrom scipy.stats import norm\n\nfrom functions import bailey_welch_schedule, get_v_star, compute_convolutions, calculate_objective_serv_time_lookup\n\n# Filter warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning) # GP fitting might not always converge perfectly\n\n\n\n6.4.6.2 2. Constants\nDefinition of problem parameters and initial configuration.\n\n# --- Problem Definition ---\n\n# Fixed Data (Use your actual data)\nN = 21 # Total number of patients\nT = 20 # Dimension of the binary vector U\nd = 5 # Length of each interval\nmax_s = 20 # Maximum service time\nq = 0.20 # Probability of a scheduled patient not showing up\nw = 0.1 # Weight for the waiting time in objective function\nl = 10\nv_star = get_v_star(T) # Get the V* matrix(T x T)\n# Create service time distribution\ndef generate_weighted_list(max_s: int, l: float, i: int) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Generates a service time probability distribution using optimization.\n\n    This function creates a discrete probability distribution over max_s possible\n    service times (from 1 to max_s). It uses optimization (SLSQP) to find a\n    distribution whose weighted average service time is as close as possible\n    to a target value 'l', subject to the constraint that the probabilities\n    sum to 1 and each probability is between 0 and 1.\n\n    After finding the distribution, it sorts the probabilities: the first 'i'\n    probabilities (corresponding to service times 1 to i) are sorted in\n    ascending order, and the remaining probabilities (service times i+1 to max_s)\n    are sorted in descending order.\n\n    Note:\n        - Requires NumPy and SciPy libraries (specifically scipy.optimize.minimize).\n\n    Args:\n        max_s (int): Maximum service time parameter (number of probability bins).\n                     Must be a positive integer.\n        l (float): The target weighted average service time for the distribution.\n                   Must be between 1 and max_s, inclusive.\n        i (int): The index determining the sorting split point. Probabilities\n                 for service times 1 to 'i' are sorted ascendingly, and\n                 probabilities for service times 'i+1' to 'max_s' are sorted\n                 descendingly. Must be between 1 and max_s-1 for meaningful sorting.\n\n    Returns:\n        numpy.ndarray: An array of size max_s+1. The first element (index 0) is 0.\n                       Elements from index 1 to max_s represent the calculated\n                       and sorted probability distribution, summing to 1.\n                       Returns None if optimization fails or inputs are invalid.\n    \"\"\"\n\n    # --- Input Validation ---\n    if not isinstance(max_s, int) or max_s &lt;= 0:\n        print(f\"Error: max_s must be a positive integer, but got {max_s}\")\n        return None\n    if not isinstance(l, (int, float)) or not (1 &lt;= l &lt;= max_s):\n        print(f\"Error: Target average 'l' ({l}) must be between 1 and max_s ({max_s}).\")\n        return None\n    if not isinstance(i, int) or not (0 &lt; i &lt; max_s):\n        print(f\"Error: Sorting index 'i' ({i}) must be between 1 and max_s-1 ({max_s-1}).\")\n        # If clamping is desired instead of error:\n        # print(f\"Warning: Index 'i' ({i}) is outside the valid range (1 to {max_s-1}). Clamping i.\")\n        # i = max(1, min(i, max_s - 1))\n        return None # Strict check based on docstring requirement\n\n    # --- Inner helper function for optimization ---\n    def objective(x: np.ndarray) -&gt; float:\n        \"\"\"Objective function: Squared difference between weighted average and target l.\"\"\"\n        # x represents probabilities P(1) to P(max_s)\n        service_times = np.arange(1, max_s + 1)\n        weighted_avg = np.dot(service_times, x) # Equivalent to sum(k * P(k) for k=1 to max_s)\n        return (weighted_avg - l) ** 2\n\n    # --- Constraints for optimization ---\n    # Constraint 1: The sum of the probabilities must be 1\n    constraints = ({\n        'type': 'eq',\n        'fun': lambda x: np.sum(x) - 1.0 # Ensure float comparison\n    })\n\n    # Bounds: Each probability value x[k] must be between 0 and 1\n    # Creates a list of max_s tuples, e.g., [(0, 1), (0, 1), ..., (0, 1)]\n    bounds = [(0, 1)] * max_s\n\n    # Initial guess: Use Dirichlet distribution to get a random distribution that sums to 1.\n    # Provides a starting point for the optimizer. np.ones(max_s) gives equal weights initially.\n    initial_guess = np.random.dirichlet(np.ones(max_s))\n\n    # --- Perform Optimization ---\n    try:\n        result = minimize(\n            objective,\n            initial_guess,\n            method='SLSQP',\n            bounds=bounds,\n            constraints=constraints,\n            # options={'disp': False} # Set True for detailed optimizer output\n        )\n\n        # Check if optimization was successful\n        if not result.success:\n            print(f\"Warning: Optimization failed! Message: {result.message}\")\n            # Optionally print result object for more details: print(result)\n            return None # Indicate failure\n\n        # The optimized probabilities (P(1) to P(max_s))\n        optimized_probs = result.x\n\n        # --- Post-process: Correct potential floating point inaccuracies ---\n        # Ensure probabilities are non-negative and sum *exactly* to 1\n        optimized_probs[optimized_probs &lt; 0] = 0 # Clamp small negatives to 0\n        current_sum = np.sum(optimized_probs)\n        if not np.isclose(current_sum, 1.0):\n            if current_sum &gt; 0: # Avoid division by zero\n                 optimized_probs /= current_sum # Normalize to sum to 1\n            else:\n                 print(\"Warning: Optimization resulted in zero sum probabilities after clamping negatives.\")\n                 # Handle this case - maybe return uniform distribution or None\n                 return None # Or return uniform: np.ones(max_s) / max_s\n\n    except Exception as e:\n        print(f\"An error occurred during optimization: {e}\")\n        return None\n\n    # --- Reorder the probabilities based on the index 'i' ---\n    # Split the probabilities P(1)...P(i) and P(i+1)...P(max_s)\n    # Note: Python slicing is exclusive of the end index, array indexing is 0-based.\n    # result.x[0] corresponds to P(1), result.x[i-1] to P(i).\n    # result.x[i] corresponds to P(i+1), result.x[max_s-1] to P(max_s).\n\n    first_part_probs = optimized_probs[:i]   # Probabilities P(1) to P(i)\n    second_part_probs = optimized_probs[i:]  # Probabilities P(i+1) to P(max_s)\n\n    # Sort the first part ascending, the second part descending\n    sorted_first_part = np.sort(first_part_probs)\n    sorted_second_part = np.sort(second_part_probs)[::-1] # [::-1] reverses\n\n    # --- Create final output array ---\n    # Array of size max_s + 1, initialized to zeros. Index 0 unused.\n    values = np.zeros(max_s + 1)\n\n    # Assign the sorted probabilities back into the correct slots (index 1 onwards)\n    values[1 : i + 1] = sorted_first_part      # Assign P(1)...P(i)\n    values[i + 1 : max_s + 1] = sorted_second_part # Assign P(i+1)...P(max_s)\n\n    # Final check on sum after potential normalization/sorting\n    if not np.isclose(np.sum(values[1:]), 1.0):\n         print(f\"Warning: Final distribution sum is {np.sum(values[1:])}, not 1.0. Check logic.\")\n\n    # Return the final array with the sorted probability distribution\n    return values\n\ni = 5  # First 5 highest values in ascending order, rest in descending order\ns = generate_weighted_list(max_s, l, i)\nprint(f\"Average generated service time: {np.dot(np.arange(len(s)), s)}\")\nconvolutions = compute_convolutions(s, N, q)\nX = np.array(bailey_welch_schedule(T, d, N, s))\n# Objective Function Calculation\nLARGE_PENALTY = 1e10 # Penalty for infeasible solutions\n\nAverage generated service time: 8.6135702858019\n\n\n\n\n6.4.6.3 3. Common Functions (Objective Evaluation and HED)\nObjective evaluation implements \\(C(\\mathbf{x})\\) from Kaandorp and Koole (2007). HED implementation follows Deshwal et al. (2023).\n\ndef evaluate_objective(U_np, X_vec, v_star, convolutions, d, w):\n    \"\"\"\n    Target function: Evaluates objective for a single binary numpy array U.\n    Returns a float.\n    \"\"\"\n    # Input validation (same as before)\n    if not isinstance(U_np, np.ndarray):\n        raise TypeError(\"Input U must be a numpy array\")\n    if U_np.ndim != 1:\n         raise ValueError(\"Input U must be 1-dimensional\")\n    if U_np.shape[0] != v_star.shape[0]:\n         raise ValueError(f\"Dimension mismatch: U length {U_np.shape[0]} != V* rows {v_star.shape[0]}.\")\n    if X_vec.shape[0] != v_star.shape[1]:\n         raise ValueError(\"Dimension mismatch: X length must match V* columns.\")\n    if not np.all((U_np == 0) | (U_np == 1)):\n         raise ValueError(\"Input U must be binary (0s and 1s).\")\n\n    # Calculate Y based on selected rows of V_star\n    V_sum = np.sum(v_star[U_np == 1, :], axis=0)\n    Y = X_vec + V_sum\n\n    # Check feasibility and calculate objective\n    if np.all(Y &gt;= 0):\n        ewt, esp = calculate_objective_serv_time_lookup(Y, d, convolutions)\n        objective_value = w * ewt + (1 - w) * esp\n        return objective_value\n    else:\n        # Infeasible solution\n        return LARGE_PENALTY\n\n# --- HED Implementation ---\n\ndef hamming_distance(u1, u2):\n    \"\"\"Calculates Hamming distance between two binary numpy arrays.\"\"\"\n    return np.sum(u1 != u2)\n\ndef generate_diverse_random_dictionary(T, m):\n    \"\"\"Generates the random dictionary A for HED.\"\"\"\n    dictionary_A = np.zeros((m, T), dtype=int)\n    for i in range(m):\n        # Sample theta for density of 1s in this dictionary vector\n        theta = np.random.uniform(0, 1)\n        row = (np.random.rand(T) &lt; theta).astype(int)\n        dictionary_A[i, :] = row\n    return dictionary_A\n\ndef embed_vector(U_np, dictionary_A):\n    \"\"\"Embeds a single binary vector U using HED.\"\"\"\n    m = dictionary_A.shape[0]\n    embedding_phi = np.zeros(m, dtype=float) # Use float for GP\n    for i in range(m):\n        embedding_phi[i] = hamming_distance(U_np, dictionary_A[i, :])\n    return embedding_phi\n\ndef embed_batch(U_batch_np, dictionary_A):\n    \"\"\"Embeds a batch of binary vectors U.\"\"\"\n    # Input U_batch_np is expected to be a NumPy array\n    m = dictionary_A.shape[0]\n    if U_batch_np.ndim == 1: # Handle single vector case\n        U_batch_np = U_batch_np.reshape(1, -1)\n\n    batch_size = U_batch_np.shape[0]\n    embeddings_np = np.zeros((batch_size, m), dtype=float) # Use float for GP\n\n    for j in range(batch_size):\n        embeddings_np[j, :] = embed_vector(U_batch_np[j, :], dictionary_A)\n\n    # Return NumPy array directly\n    return embeddings_np\n\n\n\n6.4.6.4 4. Experiment 1: CBO with Expected Improvement (EI)\nApplies the methodology from Deshwal et al. (2023) using EI.\n\n# --- BO Helper Functions ---\n\ndef get_fitted_model(train_X_embedded_scaled, train_Y, m):\n    \"\"\"\n    Fits a GaussianProcessRegressor model to the SCALED embedded data.\n    Assumes train_Y contains negative objective values for maximization.\n    \"\"\"\n    if train_Y.ndim &gt; 1 and train_Y.shape[1] == 1:\n        train_Y = train_Y.ravel() # sklearn GP expects 1D target array\n\n    # Define the kernel for the Gaussian Process\n    # Matern kernel is a common choice, nu=2.5 is smooth (twice differentiable)\n    # ConstantKernel handles the overall variance scaling\n    # WhiteKernel handles the observation noise\n    kernel = ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \\\n             Matern(length_scale=np.ones(m), # Enable ARD, initialize length scales to 1\n                    length_scale_bounds=(1e-2, 1e2),\n                    nu=2.5) + \\\n             WhiteKernel(noise_level=1e-10, # Small value for numerical stability\n                         noise_level_bounds=\"fixed\") # Bounds for noise optimization\n\n    # Instantiate the Gaussian Process Regressor\n    # alpha: Value added to the diagonal of the kernel matrix during fitting\n    #        for numerical stability (can also be seen as additional noise)\n    # n_restarts_optimizer: Restarts optimizer to find better hyperparameters\n    gp_model = GaussianProcessRegressor(\n        kernel=kernel,\n        alpha=1e-10, # Small value for numerical stability\n        n_restarts_optimizer=10, # More restarts -&gt; better hyperparams but slower\n        random_state=42 # For reproducibility of optimizer restarts\n    )\n\n    # Fit the GP model\n    gp_model.fit(train_X_embedded_scaled, train_Y)\n    return gp_model\n\ndef expected_improvement(mu, sigma, f_best, xi=0.01):\n    \"\"\"\n    Computes the Expected Improvement acquisition function.\n    Assumes maximization (f_best is the current maximum observed value).\n    mu, sigma: Predicted mean and standard deviation (NumPy arrays).\n    f_best: Current best observed function value (scalar).\n    xi: Exploration-exploitation trade-off parameter.\n    \"\"\"\n    # Ensure sigma is positive and non-zero to avoid division errors\n    sigma = np.maximum(sigma, 1e-9)\n    Z = (mu - f_best - xi) / sigma\n\n    ei = (mu - f_best - xi) * norm.cdf(Z) + sigma * norm.pdf(Z)\n\n    # Set EI to 0 where variance is negligible\n    ei[sigma &lt;= 1e-9] = 0.0\n    return ei\n\n# MODIFIED: Accepts the scaler and uses scikit-learn GP + EI\ndef optimize_acqf_discrete_via_embedding(gp_model, scaler, dictionary_A, T, q, num_candidates, current_best_neg_f_val):\n    \"\"\"\n    Optimizes acquisition function (Expected Improvement) by sampling random\n    binary candidates, embedding, SCALING, predicting with GP, and calculating EI.\n    Selects the top q candidates based on EI.\n    Returns candidates as a numpy array (q x T).\n    \"\"\"\n    m = dictionary_A.shape[0]\n\n    # 1. Generate Random Binary Candidates\n    candidate_u_vectors_np = np.random.randint(0, 2, size=(num_candidates, T))\n    # Optional: Ensure unique candidates if needed (adds overhead)\n    # candidate_u_vectors_np = np.unique(candidate_u_vectors_np, axis=0)\n    # num_candidates = candidate_u_vectors_np.shape[0] # Update count\n\n    # 2. Embed the Candidates\n    embedded_candidates_np = embed_batch(candidate_u_vectors_np, dictionary_A)\n\n    # 3. Scale the Embedded Candidates\n    # Handle potential warning if scaler expects float64 (already float here)\n    # Use the *fitted* scaler from the training data\n    embedded_candidates_scaled_np = scaler.transform(embedded_candidates_np)\n\n    # 4. Predict Mean and Std Dev using the GP Model\n    mu, std = gp_model.predict(embedded_candidates_scaled_np, return_std=True)\n\n    # 5. Calculate Acquisition Function (Expected Improvement)\n    # current_best_neg_f_val is the maximum of the (negative) objectives seen so far\n    acq_values = expected_improvement(mu, std, current_best_neg_f_val, xi=0.01)\n\n    # 6. Select Top Candidates\n    # Use np.argsort to find indices that would sort the array (ascending)\n    # Select the last q indices for the highest EI values\n    # If q=1, np.argmax(acq_values) is simpler but argsort works generally\n    top_indices = np.argsort(acq_values)[-q:]\n\n    # Ensure indices are returned in descending order of acquisition value (optional but nice)\n    top_indices = top_indices[::-1]\n\n    return candidate_u_vectors_np[top_indices, :]\n# --- BO Loop ---\n\n# Parameters\nN_INITIAL = 20\nN_ITERATIONS = 20\nBATCH_SIZE_q = 5\nNUM_CANDIDATES_Acqf = T*1024 # Might need more for higher T\nm = 128 # Dimension of the embedding space\n\n# Store evaluated points (using NumPy arrays)\nevaluated_U_np_list = [] # List to store evaluated U vectors (binary)\nevaluated_f_vals = []    # List to store raw objective values (lower is better)\ntrain_Y_list = []        # List to store NEGATED objective values for GP (higher is better)\n\n# 1. Initialization\nprint(f\"Generating {N_INITIAL} initial points...\")\ninitial_candidates = []\nwhile len(initial_candidates) &lt; N_INITIAL:\n    U_init = np.random.randint(0, 2, size=T)\n    # Ensure unique initial points\n    is_duplicate = any(np.array_equal(U_init, u) for u in initial_candidates)\n    if not is_duplicate:\n        initial_candidates.append(U_init)\n\nfor U_init in initial_candidates:\n    f_val = evaluate_objective(U_init, X, v_star, convolutions, d, w)\n    neg_f_val = -f_val\n\n    evaluated_U_np_list.append(U_init)\n    evaluated_f_vals.append(f_val)\n    train_Y_list.append(neg_f_val)\n\n# Convert lists to NumPy arrays for GP fitting\ntrain_Y = np.array(train_Y_list).reshape(-1, 1) # Keep as column vector initially\n\nbest_obj_so_far = min(evaluated_f_vals) if evaluated_f_vals else float('inf')\ninitial_best_obj_so_far_ei = best_obj_so_far\nprint(f\"Initial best objective value: {best_obj_so_far}\")\nif not np.isfinite(best_obj_so_far):\n     print(\"Warning: Initial best objective is infinite, possibly all initial points were infeasible.\")\n\n\n# 2. BO Iterations\nfor iteration in range(N_ITERATIONS):\n    start_time = time.time()\n    print(f\"\\n--- Iteration {iteration + 1}/{N_ITERATIONS} ---\")\n\n    # a. Generate dictionary A for HED\n    current_dictionary_A = generate_diverse_random_dictionary(T, m)\n\n    # b. Embed ALL evaluated U vectors so far\n    if not evaluated_U_np_list:\n        print(\"Warning: No points evaluated yet. Skipping iteration.\")\n        continue\n    evaluated_U_np_array = np.array(evaluated_U_np_list)\n    embedded_train_X = embed_batch(evaluated_U_np_array, current_dictionary_A)\n\n    # c. Scale the embedded training data\n    scaler = MinMaxScaler()\n    # Fit scaler only if there's data\n    if embedded_train_X.shape[0] &gt; 0:\n        # Fit and transform\n        embedded_train_X_scaled = scaler.fit_transform(embedded_train_X)\n    else:\n        # Handle case with no data (shouldn't happen after init)\n        embedded_train_X_scaled = embedded_train_X # Will be empty\n\n    # Ensure train_Y is a NumPy array for fitting\n    train_Y_for_fit = np.array(train_Y_list) # Use the list directly\n\n    # d. Fit GP Model using SCALED data\n    print(\"Fitting GP model...\")\n    if embedded_train_X_scaled.shape[0] &gt; 0 and train_Y_for_fit.shape[0] == embedded_train_X_scaled.shape[0]:\n        gp_model = get_fitted_model(embedded_train_X_scaled, train_Y_for_fit, m)\n        print(\"GP model fitted.\")\n    else:\n         print(\"Warning: Not enough data or data mismatch to fit GP model. Skipping iteration.\")\n         continue # Skip if no data or mismatch\n\n    # e. Determine current best value for Acquisition Function\n    # We are maximizing the negative objective in the GP\n    current_best_neg_f_val = np.max(train_Y_for_fit) if train_Y_for_fit.size &gt; 0 else -float('inf')\n\n    # Prevent potential issues if all points were infeasible (very large negative best_f)\n    if current_best_neg_f_val &lt;= -LARGE_PENALTY / 2 and np.isfinite(current_best_neg_f_val):\n         print(f\"Warning: Current best value ({current_best_neg_f_val:.2f}) is very low (likely from penalties). Acqf might behave unexpectedly.\")\n\n\n    # f. Optimize Acquisition Function (Expected Improvement)\n    print(\"Optimizing acquisition function...\")\n    next_U_candidates_np = optimize_acqf_discrete_via_embedding(\n        gp_model=gp_model,\n        scaler=scaler, # Pass the fitted scaler\n        dictionary_A=current_dictionary_A,\n        T=T,\n        q=BATCH_SIZE_q,\n        num_candidates=NUM_CANDIDATES_Acqf,\n        current_best_neg_f_val=current_best_neg_f_val\n    )\n    print(f\"Selected {next_U_candidates_np.shape[0]} candidate(s).\")\n\n    # g. Evaluate Objective for the selected candidate(s)\n    newly_evaluated_U = []\n    newly_evaluated_f = []\n    newly_evaluated_neg_f = []\n\n    for i in range(next_U_candidates_np.shape[0]):\n        next_U = next_U_candidates_np[i, :]\n\n        # Check if this candidate was already evaluated\n        # Use a tolerance for floating point comparisons if U were continuous\n        # For binary, exact comparison is fine\n        already_evaluated = any(np.array_equal(next_U, u) for u in evaluated_U_np_list)\n\n        if already_evaluated:\n            print(f\"  Candidate {i} was already evaluated. Skipping re-evaluation.\")\n            # TODO: Optionally, could try to generate a *different* candidate here\n            #       e.g., by running optimize_acqf again excluding this one,\n            #       or sampling randomly near it. For now, just skip.\n            continue # Skip to next candidate\n\n        # Evaluate the objective\n        next_f = evaluate_objective(next_U, X, v_star, convolutions, d, w)\n        next_neg_f = -next_f\n        print(f\"  Candidate {i}: Obj = {next_f:.4f}\")\n\n        # Add to temporary lists for this iteration\n        newly_evaluated_U.append(next_U)\n        newly_evaluated_f.append(next_f)\n        newly_evaluated_neg_f.append(next_neg_f)\n\n        # Update overall best objective found\n        if next_f &lt; best_obj_so_far:\n            best_obj_so_far = next_f\n\n    # h. Augment Dataset for next iteration\n    evaluated_U_np_list.extend(newly_evaluated_U)\n    evaluated_f_vals.extend(newly_evaluated_f)\n    train_Y_list.extend(newly_evaluated_neg_f) # Add negative values for next GP fit\n\n    # Convert train_Y_list back to array for potential use (though we rebuild it next iter)\n    train_Y = np.array(train_Y_list).reshape(-1, 1)\n\n    iter_time = time.time() - start_time\n    print(f\"Best objective value found so far: {best_obj_so_far:.4f}\")\n    print(f\"Total points evaluated: {len(evaluated_f_vals)}\")\n    print(f\"Iteration {iteration + 1} completed in {iter_time:.2f} seconds.\")\n\n\n# --- Results ---\nprint(\"\\n--- Optimization Finished ---\")\nif not evaluated_f_vals:\n    print(\"No points were successfully evaluated.\")\nelse:\n    # Find the best point among all evaluated points\n    final_best_idx_ei = np.argmin(evaluated_f_vals) # Index of minimum raw objective\n    final_best_U_ei = evaluated_U_np_list[final_best_idx_ei]\n    final_best_f_ei = evaluated_f_vals[final_best_idx_ei]\n    nr_evaluated_f_vals_ei = len(evaluated_f_vals) # Saved for reporting results\n    print(f\"Total evaluations: {len(evaluated_f_vals)}\") \n    print(f\"Best Objective Value Found: {final_best_f_ei}\")\n    # Ensure U is printed correctly if it's long\n    print(f\"Best U vector Found: {final_best_U_ei}\")\n    # print(f\"Best U vector Found (Indices of 1s): {np.where(final_best_U_ei == 1)[0]}\")\n\n\n    # Verification - Recalculate Y for the best U found\n    V_sum_best = np.sum(v_star[final_best_U_ei == 1, :], axis=0)\n    Y_best_ei = X + V_sum_best\n    is_feasible = np.all(Y_best_ei &gt;= 0)\n    if is_feasible:\n        ewt, esp = calculate_objective_serv_time_lookup(Y_best_ei, d, convolutions)\n        recalculated_obj = w * ewt + (1 - w) * esp\n        \n    else:\n        LARGE_PENALTY\n\n    print(f\"\\n--- Verification ---\")\n    print(f\"Is the best U feasible? {is_feasible}\")\n    if is_feasible:\n        print(f\"Resulting Y vector for best U: {Y_best_ei}\")\n        print(f\"Objective value (recalculated): {recalculated_obj:.4f}\")\n        if not np.isclose(final_best_f_ei, recalculated_obj):\n             print(f\"Warning: Stored best objective ({final_best_f_ei}) does not match recalculation ({recalculated_obj})!\")\n    elif final_best_f &lt; LARGE_PENALTY:\n         print(f\"Warning: Best objective ({final_best_f_ei}) is not the penalty value, but feasibility check failed.\")\n         print(f\"Resulting Y vector (infeasible): {Y_best_ei}\")\n    else:\n         print(\"Best solution found corresponds to an infeasible penalty value.\")\n\nGenerating 20 initial points...\nInitial best objective value: 88.0897003591152\n\n--- Iteration 1/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 10000000000.0000\n  Candidate 1: Obj = 10000000000.0000\n  Candidate 2: Obj = 10000000000.0000\n  Candidate 3: Obj = 10000000000.0000\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 88.0897\nTotal points evaluated: 25\nIteration 1 completed in 13.18 seconds.\n\n--- Iteration 2/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 10000000000.0000\n  Candidate 1: Obj = 10000000000.0000\n  Candidate 2: Obj = 10000000000.0000\n  Candidate 3: Obj = 89.2318\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 88.0897\nTotal points evaluated: 30\nIteration 2 completed in 13.03 seconds.\n\n--- Iteration 3/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 10000000000.0000\n  Candidate 1: Obj = 10000000000.0000\n  Candidate 2: Obj = 88.7158\n  Candidate 3: Obj = 10000000000.0000\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 88.0897\nTotal points evaluated: 35\nIteration 3 completed in 13.13 seconds.\n\n--- Iteration 4/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 10000000000.0000\n  Candidate 1: Obj = 10000000000.0000\n  Candidate 2: Obj = 10000000000.0000\n  Candidate 3: Obj = 10000000000.0000\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 88.0897\nTotal points evaluated: 40\nIteration 4 completed in 15.09 seconds.\n\n--- Iteration 5/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 88.7784\n  Candidate 1: Obj = 95.1604\n  Candidate 2: Obj = 89.7564\n  Candidate 3: Obj = 10000000000.0000\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 88.0897\nTotal points evaluated: 45\nIteration 5 completed in 14.31 seconds.\n\n--- Iteration 6/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 10000000000.0000\n  Candidate 1: Obj = 89.4412\n  Candidate 2: Obj = 10000000000.0000\n  Candidate 3: Obj = 89.0120\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 88.0897\nTotal points evaluated: 50\nIteration 6 completed in 14.96 seconds.\n\n--- Iteration 7/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 94.9024\n  Candidate 1: Obj = 95.9198\n  Candidate 2: Obj = 10000000000.0000\n  Candidate 3: Obj = 93.9183\n  Candidate 4: Obj = 89.3453\nBest objective value found so far: 88.0897\nTotal points evaluated: 55\nIteration 7 completed in 18.13 seconds.\n\n--- Iteration 8/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 10000000000.0000\n  Candidate 1: Obj = 10000000000.0000\n  Candidate 2: Obj = 94.7820\n  Candidate 3: Obj = 94.0897\n  Candidate 4: Obj = 95.3445\nBest objective value found so far: 88.0897\nTotal points evaluated: 60\nIteration 8 completed in 20.46 seconds.\n\n--- Iteration 9/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 94.7453\n  Candidate 1: Obj = 96.4062\n  Candidate 2: Obj = 95.8984\n  Candidate 3: Obj = 94.8890\n  Candidate 4: Obj = 88.7431\nBest objective value found so far: 88.0897\nTotal points evaluated: 65\nIteration 9 completed in 16.67 seconds.\n\n--- Iteration 10/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 95.7344\n  Candidate 1: Obj = 95.2253\n  Candidate 2: Obj = 94.6868\n  Candidate 3: Obj = 94.4514\n  Candidate 4: Obj = 94.5031\nBest objective value found so far: 88.0897\nTotal points evaluated: 70\nIteration 10 completed in 22.24 seconds.\n\n--- Iteration 11/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 95.1213\n  Candidate 1: Obj = 88.7654\n  Candidate 2: Obj = 95.9191\n  Candidate 3: Obj = 88.6929\n  Candidate 4: Obj = 90.2251\nBest objective value found so far: 88.0897\nTotal points evaluated: 75\nIteration 11 completed in 19.73 seconds.\n\n--- Iteration 12/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 94.8660\n  Candidate 1: Obj = 88.4310\n  Candidate 2: Obj = 94.7075\n  Candidate 3: Obj = 10000000000.0000\n  Candidate 4: Obj = 95.9448\nBest objective value found so far: 88.0897\nTotal points evaluated: 80\nIteration 12 completed in 20.19 seconds.\n\n--- Iteration 13/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 95.1996\n  Candidate 1: Obj = 95.5489\n  Candidate 2: Obj = 95.3606\n  Candidate 3: Obj = 10000000000.0000\n  Candidate 4: Obj = 95.5386\nBest objective value found so far: 88.0897\nTotal points evaluated: 85\nIteration 13 completed in 26.51 seconds.\n\n--- Iteration 14/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 95.6974\n  Candidate 1: Obj = 92.7296\n  Candidate 2: Obj = 94.2263\n  Candidate 3: Obj = 94.5919\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 88.0897\nTotal points evaluated: 90\nIteration 14 completed in 31.34 seconds.\n\n--- Iteration 15/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 87.9466\n  Candidate 1: Obj = 92.9106\n  Candidate 2: Obj = 88.5815\n  Candidate 3: Obj = 89.0310\n  Candidate 4: Obj = 88.5117\nBest objective value found so far: 87.9466\nTotal points evaluated: 95\nIteration 15 completed in 35.87 seconds.\n\n--- Iteration 16/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 94.9326\n  Candidate 1: Obj = 95.6298\n  Candidate 2: Obj = 95.5985\n  Candidate 3: Obj = 94.8712\n  Candidate 4: Obj = 90.4048\nBest objective value found so far: 87.9466\nTotal points evaluated: 100\nIteration 16 completed in 62.02 seconds.\n\n--- Iteration 17/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 93.7129\n  Candidate 1: Obj = 94.0030\n  Candidate 2: Obj = 95.3315\n  Candidate 3: Obj = 94.0290\n  Candidate 4: Obj = 93.8096\nBest objective value found so far: 87.9466\nTotal points evaluated: 105\nIteration 17 completed in 47.51 seconds.\n\n--- Iteration 18/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 94.5934\n  Candidate 1: Obj = 95.3891\n  Candidate 2: Obj = 94.3777\n  Candidate 3: Obj = 93.9556\n  Candidate 4: Obj = 94.1809\nBest objective value found so far: 87.9466\nTotal points evaluated: 110\nIteration 18 completed in 200.45 seconds.\n\n--- Iteration 19/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 89.5092\n  Candidate 1: Obj = 94.2203\n  Candidate 2: Obj = 93.6439\n  Candidate 3: Obj = 86.7328\n  Candidate 4: Obj = 94.1756\nBest objective value found so far: 86.7328\nTotal points evaluated: 115\nIteration 19 completed in 226.02 seconds.\n\n--- Iteration 20/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 10000000000.0000\n  Candidate 1: Obj = 10000000000.0000\n  Candidate 2: Obj = 10000000000.0000\n  Candidate 3: Obj = 86.0113\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 86.0113\nTotal points evaluated: 120\nIteration 20 completed in 19.78 seconds.\n\n--- Optimization Finished ---\nTotal evaluations: 120\nBest Objective Value Found: 86.01134637760948\nBest U vector Found: [0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1]\n\n--- Verification ---\nIs the best U feasible? True\nResulting Y vector for best U: [3 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 7]\nObjective value (recalculated): 86.0113\n\n\n\n\n6.4.6.5 5. Experiment 2: CBO with Lower Confidence Bound (LCB) - Fixed Kappa\nApplies the methodology from Deshwal et al. (2023) using LCB with fixed \\(\\kappa\\).\n\n# --- BO Helper Functions ---\n\n# --- get_fitted_model function remains the same ---\ndef get_fitted_model(train_X_embedded_scaled, train_Y, m):\n    # ... (implementation is unchanged) ...\n    if train_Y.ndim &gt; 1 and train_Y.shape[1] == 1: train_Y = train_Y.ravel()\n    kernel = ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \\\n             Matern(length_scale=np.ones(m), length_scale_bounds=(1e-2, 1e2), nu=2.5) + \\\n             WhiteKernel(noise_level=1e-10, # Small value for numerical stability\n                         noise_level_bounds=\"fixed\") # Bounds for noise optimization\n    gp_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-10, n_restarts_optimizer=10, random_state=42)\n    gp_model.fit(train_X_embedded_scaled, train_Y)\n    return gp_model\n\ndef lower_confidence_bound(mu, sigma, kappa=2.576):\n    \"\"\"\n    Computes the Lower Confidence Bound (LCB) acquisition function.\n    Assumes maximization of this value guides the search (since mu is neg objective).\n    Higher LCB means lower predicted objective or lower penalty for uncertainty.\n\n    mu, sigma: Predicted mean and standard deviation (NumPy arrays).\n    kappa: Controls the balance between exploitation (high mu -&gt; low original objective)\n           and exploration (low sigma).\n    \"\"\"\n    # Ensure sigma is non-negative\n    sigma = np.maximum(sigma, 0)\n    return mu - kappa * sigma # &lt;&lt;&lt; Sign flipped from UCB\n\ndef optimize_acqf_discrete_via_embedding(gp_model, scaler, dictionary_A, T, q, num_candidates, kappa):\n    \"\"\"\n    Optimizes LCB acquisition function by sampling random binary candidates,\n    embedding, SCALING, predicting with GP, and calculating LCB.\n    Selects the top q candidates based on LCB.\n    Returns candidates as a numpy array (q x T).\n    \"\"\"\n    m = dictionary_A.shape[0]\n\n    # 1. Generate Random Binary Candidates\n    candidate_u_vectors_np = np.random.randint(0, 2, size=(num_candidates, T))\n\n    # 2. Embed the Candidates\n    embedded_candidates_np = embed_batch(candidate_u_vectors_np, dictionary_A)\n\n    # 3. Scale the Embedded Candidates\n    embedded_candidates_scaled_np = scaler.transform(embedded_candidates_np)\n\n    # 4. Predict Mean and Std Dev using the GP Model\n    mu, std = gp_model.predict(embedded_candidates_scaled_np, return_std=True)\n\n    # 5. Calculate Acquisition Function (Lower Confidence Bound) &lt;&lt;&lt; CHANGED HERE\n    acq_values = lower_confidence_bound(mu, std, kappa=kappa) # Use LCB\n\n    # 6. Select Top Candidates (based on highest LCB) &lt;&lt;&lt; COMMENT UPDATED\n    # We maximize LCB = mu - kappa*sigma, where mu is neg_objective\n    top_indices = np.argsort(acq_values)[-q:]\n    top_indices = top_indices[::-1] # Ensure descending order of LCB\n\n    return candidate_u_vectors_np[top_indices, :]\n\n# --- BO Loop ---\n\n# Parameters\nKAPPA = 2.576 # Exploration parameter for LCB. Adjust as needed.\nN_INITIAL = 20\nN_ITERATIONS = 20\nBATCH_SIZE_q = 5\nNUM_CANDIDATES_Acqf = T*1024 # Might need more for higher T\nm = 128 # Dimension of the embedding space\n\n# Store evaluated points (using NumPy arrays)\nevaluated_U_np_list = [] # List to store evaluated U vectors (binary)\nevaluated_f_vals = []    # List to store raw objective values (lower is better)\ntrain_Y_list = []        # List to store NEGATED objective values for GP (higher is better)\n\n# 1. Initialization\nfor U_init in initial_candidates:\n    f_val = evaluate_objective(U_init, X, v_star, convolutions, d, w)\n    neg_f_val = -f_val\n    evaluated_U_np_list.append(U_init)\n    evaluated_f_vals.append(f_val)\n    train_Y_list.append(neg_f_val)\n  \n# Convert lists to NumPy arrays for GP fitting\ntrain_Y = np.array(train_Y_list).reshape(-1, 1) # Keep as column vector initially\n\nbest_obj_so_far = min(evaluated_f_vals) if evaluated_f_vals else float('inf')\ninitial_best_obj_so_far_lcb = best_obj_so_far # Saved for reporting results\nprint(f\"Initial best objective value: {best_obj_so_far}\")\nif not np.isfinite(best_obj_so_far):\n     print(\"Warning: Initial best objective is infinite, possibly all initial points were infeasible.\")\n\n# 2. BO Iterations\nfor iteration in range(N_ITERATIONS):\n    start_time = time.time()\n    print(f\"\\n--- Iteration {iteration + 1}/{N_ITERATIONS} ---\")\n\n    # a. Generate dictionary A (remains the same)\n    current_dictionary_A = generate_diverse_random_dictionary(T, m)\n\n    # b. Embed ALL evaluated U vectors (remains the same)\n    if not evaluated_U_np_list: continue\n    evaluated_U_np_array = np.array(evaluated_U_np_list)\n    embedded_train_X = embed_batch(evaluated_U_np_array, current_dictionary_A)\n\n    # c. Scale the embedded training data (remains the same)\n    scaler = MinMaxScaler()\n    if embedded_train_X.shape[0] &gt; 0: embedded_train_X_scaled = scaler.fit_transform(embedded_train_X)\n    else: embedded_train_X_scaled = embedded_train_X\n\n    # Ensure train_Y is NumPy array\n    train_Y_for_fit = np.array(train_Y_list)\n\n    # d. Fit GP Model (remains the same)\n    print(\"Fitting GP model...\")\n    if embedded_train_X_scaled.shape[0] &gt; 0 and train_Y_for_fit.shape[0] == embedded_train_X_scaled.shape[0]:\n        gp_model = get_fitted_model(embedded_train_X_scaled, train_Y_for_fit, m)\n        print(\"GP model fitted.\")\n    else:\n        print(\"Warning: Not enough data or data mismatch to fit GP model. Skipping iteration.\")\n        continue\n\n    # e. Determine current best neg value (useful for tracking, not directly used in LCB calculation)\n    current_best_neg_f_val = np.max(train_Y_for_fit) if train_Y_for_fit.size &gt; 0 else -float('inf')\n    if current_best_neg_f_val &lt;= -LARGE_PENALTY / 2 and np.isfinite(current_best_neg_f_val):\n        print(f\"Warning: Current best NEGATIVE objective value ({current_best_neg_f_val:.2f}) is very low (likely from penalties).\")\n\n    # f. Optimize Acquisition Function (LCB) &lt;&lt;&lt; MODIFIED CALL & COMMENT\n    print(\"Optimizing acquisition function (LCB)...\") # Comment updated\n    next_U_candidates_np = optimize_acqf_discrete_via_embedding(\n        gp_model=gp_model,\n        scaler=scaler,\n        dictionary_A=current_dictionary_A,\n        T=T,\n        q=BATCH_SIZE_q,\n        num_candidates=NUM_CANDIDATES_Acqf,\n        kappa=KAPPA # Pass kappa\n    )\n    print(f\"Selected {next_U_candidates_np.shape[0]} candidate(s).\")\n\n    # g. Evaluate Objective (remains the same)\n    newly_evaluated_U = []; newly_evaluated_f = []; newly_evaluated_neg_f = []\n    for i in range(next_U_candidates_np.shape[0]):\n        next_U = next_U_candidates_np[i, :]\n        already_evaluated = any(np.array_equal(next_U, u) for u in evaluated_U_np_list)\n        if already_evaluated: print(f\"  Candidate {i} was already evaluated. Skipping.\"); continue\n\n        next_f = evaluate_objective(next_U, X, v_star, convolutions, d, w)\n        next_neg_f = -next_f\n        print(f\"  Candidate {i}: Obj = {next_f:.4f}\")\n        newly_evaluated_U.append(next_U); newly_evaluated_f.append(next_f); newly_evaluated_neg_f.append(next_neg_f)\n        if next_f &lt; best_obj_so_far: best_obj_so_far = next_f\n\n    # h. Augment Dataset (remains the same)\n    evaluated_U_np_list.extend(newly_evaluated_U); evaluated_f_vals.extend(newly_evaluated_f); train_Y_list.extend(newly_evaluated_neg_f)\n    train_Y = np.array(train_Y_list).reshape(-1, 1)\n\n    iter_time = time.time() - start_time\n    print(f\"Best objective value found so far: {best_obj_so_far:.4f}\")\n    print(f\"Total points evaluated: {len(evaluated_f_vals)}\")\n    print(f\"Iteration {iteration + 1} completed in {iter_time:.2f} seconds.\")\n\n\n# --- Results ---\nprint(\"\\n--- Optimization Finished ---\")\nif not evaluated_f_vals: print(\"No points were successfully evaluated.\")\nelse:\n    final_best_idx_lcb = np.argmin(evaluated_f_vals)\n    final_best_U_lcb = evaluated_U_np_list[final_best_idx_lcb]\n    final_best_f_lcb = evaluated_f_vals[final_best_idx_lcb]\n    nr_evaluated_f_vals_lcb = len(evaluated_f_vals) # Saved for reporting results\n    print(f\"Total evaluations: {nr_evaluated_f_vals_lcb}\")\n    print(f\"Best Objective Value Found: {final_best_f_lcb}\")\n    print(f\"Best U vector Found: {final_best_U_lcb}\")\n\n    # Verification\n    V_sum_best = np.sum(v_star[final_best_U_lcb == 1, :], axis=0)\n    Y_best_lcb = X + V_sum_best\n    is_feasible = np.all(Y_best_lcb &gt;= 0)\n    recalculated_obj = LARGE_PENALTY\n    if is_feasible:\n        ewt, esp = calculate_objective_serv_time_lookup(Y_best_lcb, d, convolutions)\n        recalculated_obj = w * ewt + (1 - w) * esp\n\n    print(f\"\\n--- Verification ---\")\n    print(f\"Is the best U feasible? {is_feasible}\")\n    if is_feasible:\n        print(f\"Resulting Y vector for best U: {Y_best_lcb}\")\n        print(f\"Objective value (recalculated): {recalculated_obj:.4f}\")\n        if not np.isclose(final_best_f_lcb, recalculated_obj): print(f\"Warning: Stored best objective ({final_best_f:.4f}) does not match recalculation ({recalculated_obj:.4f})!\")\n    elif final_best_f_lcb &lt; LARGE_PENALTY: print(f\"Warning: Best objective ({final_best_f_lcb:.4f}) is not the penalty value, but feasibility check failed.\"); print(f\"Resulting Y vector (infeasible): {Y_best_lcb}\")\n    else: print(\"Best solution found corresponds to an infeasible penalty value.\")\n\nInitial best objective value: 88.0897003591152\n\n--- Iteration 1/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 10000000000.0000\n  Candidate 1: Obj = 10000000000.0000\n  Candidate 2: Obj = 10000000000.0000\n  Candidate 3: Obj = 10000000000.0000\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 88.0897\nTotal points evaluated: 25\nIteration 1 completed in 13.23 seconds.\n\n--- Iteration 2/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 88.2622\n  Candidate 1: Obj = 87.9853\n  Candidate 2: Obj = 87.6825\n  Candidate 3: Obj = 10000000000.0000\n  Candidate 4: Obj = 88.8805\nBest objective value found so far: 87.6825\nTotal points evaluated: 30\nIteration 2 completed in 13.35 seconds.\n\n--- Iteration 3/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 87.7538\n  Candidate 1: Obj = 94.7412\n  Candidate 2: Obj = 95.5317\n  Candidate 3: Obj = 94.5079\n  Candidate 4: Obj = 89.3976\nBest objective value found so far: 87.6825\nTotal points evaluated: 35\nIteration 3 completed in 15.22 seconds.\n\n--- Iteration 4/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 89.1307\n  Candidate 1: Obj = 89.5008\n  Candidate 2: Obj = 88.8866\n  Candidate 3: Obj = 89.5884\n  Candidate 4: Obj = 88.5348\nBest objective value found so far: 87.6825\nTotal points evaluated: 40\nIteration 4 completed in 15.37 seconds.\n\n--- Iteration 5/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 94.0832\n  Candidate 1: Obj = 88.0605\n  Candidate 2: Obj = 87.2391\n  Candidate 3: Obj = 88.5551\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 87.2391\nTotal points evaluated: 45\nIteration 5 completed in 15.01 seconds.\n\n--- Iteration 6/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 87.9236\n  Candidate 1: Obj = 10000000000.0000\n  Candidate 2: Obj = 88.9660\n  Candidate 3: Obj = 88.9521\n  Candidate 4: Obj = 93.5198\nBest objective value found so far: 87.2391\nTotal points evaluated: 50\nIteration 6 completed in 16.23 seconds.\n\n--- Iteration 7/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 94.9893\n  Candidate 1: Obj = 95.2831\n  Candidate 2: Obj = 95.0328\n  Candidate 3: Obj = 10000000000.0000\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 87.2391\nTotal points evaluated: 55\nIteration 7 completed in 16.45 seconds.\n\n--- Iteration 8/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 88.7784\n  Candidate 1: Obj = 94.6934\n  Candidate 2: Obj = 92.7892\n  Candidate 3: Obj = 94.3525\n  Candidate 4: Obj = 88.5963\nBest objective value found so far: 87.2391\nTotal points evaluated: 60\nIteration 8 completed in 17.73 seconds.\n\n--- Iteration 9/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 93.2346\n  Candidate 1: Obj = 87.6965\n  Candidate 2: Obj = 88.5723\n  Candidate 3: Obj = 89.4027\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 87.2391\nTotal points evaluated: 65\nIteration 9 completed in 28.52 seconds.\n\n--- Iteration 10/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 89.3736\n  Candidate 1: Obj = 88.8893\n  Candidate 2: Obj = 10000000000.0000\n  Candidate 3: Obj = 10000000000.0000\n  Candidate 4: Obj = 93.4544\nBest objective value found so far: 87.2391\nTotal points evaluated: 70\nIteration 10 completed in 24.80 seconds.\n\n--- Iteration 11/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 87.7868\n  Candidate 1: Obj = 87.5956\n  Candidate 2: Obj = 95.2076\n  Candidate 3: Obj = 88.6994\n  Candidate 4: Obj = 94.8456\nBest objective value found so far: 87.2391\nTotal points evaluated: 75\nIteration 11 completed in 24.01 seconds.\n\n--- Iteration 12/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 88.8117\n  Candidate 1: Obj = 88.7460\n  Candidate 2: Obj = 93.8252\n  Candidate 3: Obj = 88.7850\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 87.2391\nTotal points evaluated: 80\nIteration 12 completed in 32.39 seconds.\n\n--- Iteration 13/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 94.5939\n  Candidate 1: Obj = 93.0478\n  Candidate 2: Obj = 88.5613\n  Candidate 3: Obj = 92.9259\n  Candidate 4: Obj = 94.1999\nBest objective value found so far: 87.2391\nTotal points evaluated: 85\nIteration 13 completed in 52.90 seconds.\n\n--- Iteration 14/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 95.3989\n  Candidate 1: Obj = 94.2714\n  Candidate 2: Obj = 93.6165\n  Candidate 3: Obj = 89.8416\n  Candidate 4: Obj = 95.4340\nBest objective value found so far: 87.2391\nTotal points evaluated: 90\nIteration 14 completed in 59.17 seconds.\n\n--- Iteration 15/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 92.7154\n  Candidate 1: Obj = 90.8483\n  Candidate 2: Obj = 93.4692\n  Candidate 3: Obj = 95.4797\n  Candidate 4: Obj = 88.2388\nBest objective value found so far: 87.2391\nTotal points evaluated: 95\nIteration 15 completed in 39.66 seconds.\n\n--- Iteration 16/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 88.3254\n  Candidate 1: Obj = 89.8518\n  Candidate 2: Obj = 94.9013\n  Candidate 3: Obj = 88.3777\n  Candidate 4: Obj = 88.2221\nBest objective value found so far: 87.2391\nTotal points evaluated: 100\nIteration 16 completed in 955.47 seconds.\n\n--- Iteration 17/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 92.4377\n  Candidate 1: Obj = 92.9011\n  Candidate 2: Obj = 88.3895\n  Candidate 3: Obj = 93.7434\n  Candidate 4: Obj = 87.9331\nBest objective value found so far: 87.2391\nTotal points evaluated: 105\nIteration 17 completed in 53.98 seconds.\n\n--- Iteration 18/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 93.7456\n  Candidate 1: Obj = 96.1296\n  Candidate 2: Obj = 93.2844\n  Candidate 3: Obj = 93.6966\n  Candidate 4: Obj = 88.4894\nBest objective value found so far: 87.2391\nTotal points evaluated: 110\nIteration 18 completed in 80.71 seconds.\n\n--- Iteration 19/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 94.6237\n  Candidate 1: Obj = 95.2143\n  Candidate 2: Obj = 93.8329\n  Candidate 3: Obj = 95.7847\n  Candidate 4: Obj = 10000000000.0000\nBest objective value found so far: 87.2391\nTotal points evaluated: 115\nIteration 19 completed in 106.94 seconds.\n\n--- Iteration 20/20 ---\nFitting GP model...\nGP model fitted.\nOptimizing acquisition function (LCB)...\nSelected 5 candidate(s).\n  Candidate 0: Obj = 94.5182\n  Candidate 1: Obj = 10000000000.0000\n  Candidate 2: Obj = 88.4092\n  Candidate 3: Obj = 96.4800\n  Candidate 4: Obj = 88.1216\nBest objective value found so far: 87.2391\nTotal points evaluated: 120\nIteration 20 completed in 147.12 seconds.\n\n--- Optimization Finished ---\nTotal evaluations: 120\nBest Objective Value Found: 87.23910803625651\nBest U vector Found: [0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1]\n\n--- Verification ---\nIs the best U feasible? True\nResulting Y vector for best U: [2 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 7]\nObjective value (recalculated): 87.2391",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#results",
    "href": "combinatorial-bayes-optimization.html#results",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.5 Results",
    "text": "6.5 Results\nThe initial schedule, derived using the Bailey-Welch method (bailey1952study), serves as a baseline. The objective function \\(C(\\mathbf{x})\\) combines Expected Waiting Time (\\(EWT\\)) and Expected Staff Penalty (\\(ESP\\)). Lower values of \\(C(\\mathbf{x})\\) are preferable. Each experiment consisted of \\(N_{INITIAL} = 20\\) initial random evaluations followed by \\(N_{ITERATIONS} =20\\) Bayesian optimization iterations, with \\(BATCH\\_SIZE_q = 5\\) evaluations per iteration, totaling approximately \\(20 + 20 \\times 5 = 120\\) evaluations per experiment. The optimization operates on the binary perturbation vector \\(\\mathbf{U}\\), using the HED embedding (Deshwal et al. 2023).\nThe key performance metric is the best (minimum) objective function value found.\n\n6.5.1 Experiment 1: CBO with Expected Improvement (EI)\n\nInitial Best Objective (after random search): 88.0897003591152\nFinal Best Objective Found: 86.01134637760948\nTotal Evaluations: 120\nBest Perturbation Vector \\(\\mathbf{U}_{EI}^*\\): array([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\nResulting Optimal Schedule \\(\\mathbf{x}_{EI}^*\\): array([3, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 7])\n\n\n\n6.5.2 Experiment 2: CBO with Lower Confidence Bound (LCB) - \\(\\kappa =\\) 2.576\n\nInitial Best Objective (after random search): 88.0897003591152\nFinal Best Objective Found: 87.23910803625651\nTotal Evaluations: 120\nBest Perturbation Vector \\(\\mathbf{U}_{LCB\\_fixed}^*\\): array([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1])\nResulting Optimal Schedule \\(\\mathbf{x}_{LCB\\_fixed}^*\\): array([2, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 7])\n\n\n\n6.5.3 Summary of Best Objectives\n\n\n\n\n\n\n\nExperiment\nBest Objective \\(C(\\mathbf{x}^*)\\)\n\n\n\n\nCBO with EI\n86.01134637760948\n\n\nCBO with LCB (Fixed \\(\\kappa=\\) 2.576)\n87.23910803625651",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#discussion",
    "href": "combinatorial-bayes-optimization.html#discussion",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.6 Discussion",
    "text": "6.6 Discussion\nThe experiments aimed to compare two CBO strategies, leveraging the HED embedding technique (Deshwal et al. 2023), for optimizing the outpatient appointment scheduling problem formulated by Kaandorp and Koole (2007). Both methods successfully improved upon their respective initial random search results, demonstrating the applicability of BO with HED to this combinatorial problem.\n\nPerformance Comparison:\nHypothesis Evaluation:\n\nHypothesis 1\nHypothesis 2\n\nExploration vs. Exploitation:\nComputational Effort:\nLimitations and Future Work:\n\nThe optimality guarantee mentioned by Kaandorp and Koole (2007) applies to their specific local search algorithm operating directly on the schedule space \\(\\mathcal{F}\\), leveraging multimodularity. Our BO approach operates on the perturbation vector space \\(\\mathbf{U}\\) via HED embeddings. While BO aims for global optimization, it doesn’t inherit the same theoretical guarantee of finding the global optimum as the original local search, especially given the stochastic nature of GP modeling and acquisition function optimization.\nThe performance is likely sensitive to BO hyperparameters (dictionary size \\(m\\), \\(\\kappa\\) values, number of candidates for acquisition optimization).\nFurther investigation into different dictionary construction methods (e.g., binary wavelets as mentioned in Deshwal et al., 2023) or adaptive \\(\\kappa\\) schedules could be beneficial.\n\n\nIn conclusion, applying CBO with HED embeddings appears promising for this scheduling problem. The LCB acquisition function with a fixed, well-chosen \\(\\kappa\\) demonstrated the best performance in this study.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#timeline",
    "href": "combinatorial-bayes-optimization.html#timeline",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.7 Timeline",
    "text": "6.7 Timeline\n\nExperiment Setup and Code Implementation: 30-04-2025\nResults Analysis and Report Compilation: 07-05-2025",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#references",
    "href": "combinatorial-bayes-optimization.html#references",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.8 References",
    "text": "6.8 References\n\n\nBailey, Norman TJ. 1952. “A Study of Queues and Appointment\nSystems in Hospital Out-Patient Departments, with Special Reference to\nWaiting-Times.” Journal of the Royal Statistical Society\nSeries B: Statistical Methodology 14 (2): 185–99.\n\n\nDeshwal, Aryan, Sebastian Ament, Maximilian Balandat, Eytan Bakshy,\nJanardhan Rao Doppa, and David Eriksson. 2023. “Bayesian\nOptimization over\nHigh-Dimensional Combinatorial\nSpaces via Dictionary-Based\nEmbeddings.” In Proceedings of The\n26th International Conference on\nArtificial Intelligence and\nStatistics, 7021–39. PMLR. https://proceedings.mlr.press/v206/deshwal23a.html.\n\n\nKaandorp, Guido C., and Ger Koole. 2007. “Optimal Outpatient\nAppointment Scheduling.” Health Care Management Science\n10 (3): 217–29. https://doi.org/10.1007/s10729-007-9015-x.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "7  References",
    "section": "",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "service-time-with-no-shows.html",
    "href": "service-time-with-no-shows.html",
    "title": "service_time_with_no_shows",
    "section": "",
    "text": "Function Documentation\nservice_time_with_no_shows(s: List[float], q: float) -&gt; List[float]",
    "crumbs": [
      "Function documentation",
      "`service_time_with_no_shows`"
    ]
  },
  {
    "objectID": "service-time-with-no-shows.html#function-documentation",
    "href": "service-time-with-no-shows.html#function-documentation",
    "title": "service_time_with_no_shows",
    "section": "",
    "text": "Description\nAdjusts a distribution of service times to account for no-shows. The function scales the original service time distribution by the probability of a patient showing up (i.e., 1 - q) and then adds the no-show probability q to the service time for zero time slots.\n\n\nParameters\n\ns (List[float]): The original service time probability distribution. This list represents the probabilities associated with different service times.\nq (float): The probability of no-shows. This value should be between 0 and 1.\n\n\n\nReturns\n\nList[float]: The adjusted service time probability distribution where the no-show probability has been incorporated into the probability of zero service time.\n\n\n\nExample\n\nfrom functions import service_time_with_no_shows\n\n# Example usage\noriginal_distribution = [0.0, 0.5, 0.3, 0.2]\nno_show_probability = 0.1\nadjusted_distribution = service_time_with_no_shows(original_distribution, no_show_probability)\nprint(\"Adjusted distribution:\", adjusted_distribution)\n\nAdjusted distribution: [0.1, 0.45, 0.27, 0.18000000000000002]\n\n\n\nimport unittest\n\nclass TestServiceTimeWithNoShows(unittest.TestCase):\n    def test_adjust_distribution(self):\n        # Test with a known distribution and no-show probability\n        original_distribution = [0.0, 0.5, 0.3, 0.2]\n        no_show_probability = 0.1\n        \n        # Expected adjustment: second element 0.1, \n        # other elements: multiplied by 0.9\n        expected_distribution = [0.1, 0.45, 0.27, 0.18]\n        \n        result = service_time_with_no_shows(original_distribution, no_show_probability)\n        \n        # Using almost equal check due to floating point arithmetic\n        for r, e in zip(result, expected_distribution):\n            self.assertAlmostEqual(r, e, places=5)\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`service_time_with_no_shows`"
    ]
  },
  {
    "objectID": "compute-convolutions.html",
    "href": "compute-convolutions.html",
    "title": "compute_convolutions",
    "section": "",
    "text": "Function Documentation\ncompute_convolutions(probabilities: List[float], N: int, q: float = 0.0) -&gt; Dict[int, np.ndarray]",
    "crumbs": [
      "Function documentation",
      "`compute_convolutions`"
    ]
  },
  {
    "objectID": "compute-convolutions.html#function-documentation",
    "href": "compute-convolutions.html#function-documentation",
    "title": "compute_convolutions",
    "section": "",
    "text": "Description\nComputes the k-fold convolution of a given probability mass function (PMF) for k from 1 up to N. Before computing the convolutions, the PMF is adjusted for no-shows using the provided no-show probability q via the service_time_with_no_shows function. Convolution is performed using NumPy’s np.convolve.\n\n\nParameters\n\nprobabilities (List[float]): The original PMF represented as a list where the index corresponds to a value (for instance, a service time) and the value at that index is its probability. This function is generic and does not have to be used solely for service times.\nN (int): The maximum number of convolutions to compute.\nq (float, optional): The probability of a no-show. Defaults to 0.0.\n\n\n\nReturns\n\nDict[int, np.ndarray]: A dictionary where each key k (with 1 ≤ k ≤ N) corresponds to the PMF resulting from the k-fold convolution of the adjusted PMF.\n\n\n\nExample\n\nimport numpy as np\nfrom functions import compute_convolutions, service_time_with_no_shows\n\n# Example usage\noriginal_pmf = [0.0, 0.5, 0.3, 0.2]\nN = 3\nno_show_probability = 0.1\n\nconvs = compute_convolutions(original_pmf, N, no_show_probability)\nfor k, pmf in convs.items():\n    print(f\"{k}-fold convolution: {pmf}\")\n\n1-fold convolution: [0.1  0.45 0.27 0.18]\n2-fold convolution: [0.01   0.09   0.2565 0.279  0.2349 0.0972 0.0324]\n3-fold convolution: [0.001    0.0135   0.06885  0.169425 0.234495 0.236925 0.160623 0.083106\n 0.026244 0.005832]\n\n\n\nimport unittest\n\nclass TestComputeConvolutions(unittest.TestCase):\n    def test_single_convolution(self):\n        # When N = 1, the result should be the adjusted PMF\n        original_pmf = [0.0, 0.5, 0.3, 0.2]\n        no_show_probability = 0.1\n        N = 1\n        expected = np.array(service_time_with_no_shows(original_pmf, no_show_probability))\n        result = compute_convolutions(original_pmf, N, no_show_probability)\n        self.assertTrue(np.allclose(result[1], expected), \"Single convolution test failed\")\n\n    def test_multiple_convolutions(self):\n        # Test for N = 3 using a simple PMF\n        original_pmf = [0.0, 0.5, 0.3, 0.2]\n        no_show_probability = 0.0  # No adjustment for simplicity\n        N = 3\n        result = compute_convolutions(original_pmf, N, no_show_probability)\n\n        # For N=1, result is the original pmf\n        self.assertTrue(np.allclose(result[1], np.array(original_pmf)))\n\n        # For higher convolutions, ensure the sum of probabilities remains 1 (within numerical precision)\n        for k in range(1, N + 1):\n            self.assertAlmostEqual(np.sum(result[k]), 1.0, places=5, msg=f\"Sum of probabilities for {k}-fold convolution is not 1\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.002s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`compute_convolutions`"
    ]
  },
  {
    "objectID": "calculate-objective-serv-time-lookup.html",
    "href": "calculate-objective-serv-time-lookup.html",
    "title": "calculate_objective_serv_time_lookup",
    "section": "",
    "text": "Function Documentation\ncalculate_objective_serv_time_lookup(schedule: List[int], d: int, convolutions: dict) -&gt; Tuple[float, float]",
    "crumbs": [
      "Function documentation",
      "`calculate_objective_serv_time_lookup`"
    ]
  },
  {
    "objectID": "calculate-objective-serv-time-lookup.html#function-documentation",
    "href": "calculate-objective-serv-time-lookup.html#function-documentation",
    "title": "calculate_objective_serv_time_lookup",
    "section": "",
    "text": "Description\nThis notebook provides documentation for the function calculate_objective_serv_time_lookup, which calculates an objective value (in terms of expected waiting time and expected spillover) based on a given schedule and pre-computed convolutions of a probability mass function (PMF).\nThe function uses the following inputs:\n\nschedule: A list of integers representing the number of patients scheduled in each time slot.\nd: An integer indicating the duration threshold for a time slot.\nconvolutions: A dictionary of precomputed convolutions of the service time PMF. The key 1 should correspond to the adjusted service time distribution (for example, one adjusted for no-shows), while keys greater than 1 are used for multiple patients in a time slot.\n\nThe function returns a tuple:\n\newt: The sum of expected waiting times over the schedule.\nesp: The expected spillover time (or overtime) after the final time slot.",
    "crumbs": [
      "Function documentation",
      "`calculate_objective_serv_time_lookup`"
    ]
  },
  {
    "objectID": "calculate-objective-serv-time-lookup.html#example-usage",
    "href": "calculate-objective-serv-time-lookup.html#example-usage",
    "title": "calculate_objective_serv_time_lookup",
    "section": "Example Usage",
    "text": "Example Usage\nA trivial example using a precomputed convolution dictionary with a degenerate PMF (i.e. always zero service time) is provided in the unit tests below.\n\nimport numpy as np\nfrom typing import List, Dict, Tuple\nfrom functions import service_time_with_no_shows, compute_convolutions, calculate_objective_serv_time_lookup\n\n# For demonstration purposes, we use a trivial convolution dictionary.\noriginal_distribution = [0.0, 0.5, 0.3, 0.2]\nno_show_probability = 0.1\nadjusted_distribution = service_time_with_no_shows(original_distribution, no_show_probability)\nschedule_example = [2, 0, 0, 0, 0, 0, 1]\nN = sum(schedule_example)\nconvolutions_example = compute_convolutions(original_distribution, N, no_show_probability)\nd_example = 1\newt, esp = calculate_objective_serv_time_lookup(schedule_example, d_example, convolutions_example)\nprint(\"Adjusted Service Time Distribution: \", adjusted_distribution)\nprint(\"Expected Adjusted Service Time: \", np.dot(range(len(adjusted_distribution)), adjusted_distribution))\nprint(\"Expected Waiting Time:\", ewt)\nprint(\"Expected Spillover:\", esp)\n\nAdjusted Service Time Distribution:  [0.1, 0.45, 0.27, 0.18000000000000002]\nExpected Adjusted Service Time:  1.53\nExpected Waiting Time: 1.53\nExpected Spillover: 0.6300000000000001\n\n\n\nimport unittest\n\nclass TestCalculateObjectiveServTimeLookup(unittest.TestCase):\n    def setUp(self):\n        # Create a convolution dictionary\n        self.convolutions = convolutions_example\n        self.d = d_example\n\n    def test_single_time_slot(self):\n        # With one patient there will be no waiting and spillover (overtime) can be calculated by hand.\n        schedule = [1]\n        ewt, esp = calculate_objective_serv_time_lookup(schedule, self.d, self.convolutions)\n        self.assertAlmostEqual(ewt, 0.0, places=5, msg=\"Expected waiting time should be 0\")\n        self.assertAlmostEqual(esp, 0.6300000000000001, places=5, msg=\"Expected spillover should be 0\")\n\n    def test_zero_patients(self):\n        # If no patients are scheduled in a time slot, the process simply advances in time.\n        schedule = [0]\n        ewt, esp = calculate_objective_serv_time_lookup(schedule, self.d, self.convolutions)\n        self.assertAlmostEqual(ewt, 0.0, places=5, msg=\"Expected waiting time should be 0 when no patients\")\n        self.assertAlmostEqual(esp, 0.0, places=5, msg=\"Expected spillover should be 0 when no patients\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.001s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`calculate_objective_serv_time_lookup`"
    ]
  },
  {
    "objectID": "get-neighborhood.html",
    "href": "get-neighborhood.html",
    "title": "get_neighborhood",
    "section": "",
    "text": "Function Documentation\nget_neighborhood(x: Union[List[int], np.ndarray], v_star: np.ndarray, ids: List[List[int]], verbose: bool = False) -&gt; np.ndarray",
    "crumbs": [
      "Function documentation",
      "`get_neighborhood`"
    ]
  },
  {
    "objectID": "get-neighborhood.html#function-documentation",
    "href": "get-neighborhood.html#function-documentation",
    "title": "get_neighborhood",
    "section": "",
    "text": "Description\nThe get_neighborhood function computes a set of neighbor solutions by adding together selected rows from the array v_star to an initial solution vector x. The selection of rows is determined by the list of index lists ids, where each inner list represents a combination of indices. After generating the candidate neighbors, the function filters out any that contain negative values. An optional verbose flag provides debugging output during execution.\n\n\nParameters\n\nx (Union[List[int], np.ndarray]):\nThe current solution vector. Can be provided as a list of integers or as a NumPy array.\nv_star (np.ndarray):\nA 2D NumPy array where each row is an adjustment vector. These vectors are used to modify the current solution to explore its neighborhood.\nids (List[List[int]]):\nA list of index lists, where each inner list specifies which rows from v_star to sum together. Each combination represents a potential adjustment to the current solution.\nverbose (bool, optional):\nA flag indicating whether to print debugging information (e.g., intermediate computations, progress messages). Defaults to False.\n\n\n\nReturns\n\nnp.ndarray:\nA 2D NumPy array where each row is a neighbor solution (i.e., the result of adding a valid combination of adjustment vectors from v_star to x). Only neighbors with all non-negative entries are included in the output.\n\n\n\nExample\n\nimport numpy as np\nfrom functions import get_neighborhood, get_v_star, powerset\n\n# Define an initial solution vector\nx = [3, 2, 1]\n\n# Generate adjustment vectors using get_v_star\n# For instance, create a set of cyclic adjustment vectors of length 3\nv_star = get_v_star(3)\n\n# Generate combinations of indices (e.g., using a powerset for switching 1 patient)\nids = powerset(range(3), size=1)\n\n# Generate the neighborhood (neighbors with non-negative entries only)\nneighbors = get_neighborhood(x, v_star, ids, echo=True)\nprint(\"Neighbor solutions:\")\nprint(neighbors)\n\nPrinting every 50th result\nv_star[0]: [-1  0  1]\nx, x', delta:\n[3 2 1],\n[2 2 2],\n[-1  0  1]\n-----------------\nv_star[1]: [ 1 -1  0]\nv_star[2]: [ 0  1 -1]\nSize of raw neighborhood: 3\nFiltered out: 0 schedules with negative values.\nSize of filtered neighborhood: 3\nNeighbor solutions:\n[[2 2 2]\n [4 1 1]\n [3 3 0]]\n\n\n\nimport unittest\nimport numpy as np\nfrom functions import get_neighborhood, get_v_star, powerset\n\nclass TestGetNeighborhood(unittest.TestCase):\n    def test_non_negative_neighbors(self):\n        # Test with a simple solution vector and adjustment vectors\n        x = [3, 2, 1]\n        v_star = get_v_star(3)\n        ids = powerset(range(3), size=1)\n        \n        neighbors = get_neighborhood(x, v_star, ids, echo=False)\n        \n        # Ensure that no neighbor has negative entries\n        self.assertTrue(np.all(neighbors &gt;= 0), \"Some neighbor solutions contain negative values\")\n    \n    def test_neighborhood_shape(self):\n        # Test that the neighborhood returns a NumPy array with the proper dimensions\n        x = [3, 2, 1]\n        v_star = get_v_star(3)\n        ids = powerset(range(3), size=1)\n        neighbors = get_neighborhood(x, v_star, ids, echo=False)\n        self.assertIsInstance(neighbors, np.ndarray, \"Neighborhood is not a NumPy array\")\n        # The number of rows should equal the number of valid combinations in ids (after filtering negatives)\n        self.assertLessEqual(neighbors.shape[0], len(ids), \"Neighborhood size is larger than expected\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.002s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`get_neighborhood`"
    ]
  },
  {
    "objectID": "local-search.html",
    "href": "local-search.html",
    "title": "local_search",
    "section": "",
    "text": "Function Documentation\nlocal_search(x: Union[List[int], np.ndarray], d: int, convolutions: Dict[int, np.ndarray], w: float, v_star: np.ndarray, size: int = 2, echo: bool = False) -&gt; Tuple[np.ndarray, float]",
    "crumbs": [
      "Function documentation",
      "`local_search`"
    ]
  },
  {
    "objectID": "local-search.html#function-documentation",
    "href": "local-search.html#function-documentation",
    "title": "local_search",
    "section": "",
    "text": "Description\nThe local_search function optimizes a schedule by iteratively exploring its neighborhood. Starting with an initial solution x, the function computes its objective value using the precomputed convolutions of the service time probability mass function. The neighborhood is generated by combining adjustment vectors from v_star (using a powerset-based approach) and filtering out candidates that contain negative values. The search continues until no further improvement is found for neighborhoods up to the specified size. The objective function combines expected average waiting time per patient and spillover time weighted by w.\n\n\nParameters\n\nx (Union[List[int], np.ndarray]):\nThe initial solution vector representing the schedule. It can be provided as a list of integers or as a NumPy array.\nd (int):\nThe duration threshold for a time slot. It is used to adjust the service process and waiting time distribution.\nconvolutions (Dict[int, np.ndarray]):\nA dictionary containing precomputed convolutions of the service time PMF. The key 1 represents the adjusted service time distribution, and other keys represent the convolution for the corresponding number of scheduled patients.\nw (float):\nThe weighting factor for combining the two performance objectives: expected waiting time and expected spillover time.\nv_star (np.ndarray):\nA 2D NumPy array of adjustment vectors. Each row in v_star is used to modify the current solution vector in order to generate its neighborhood.\nsize (int, optional):\nThe maximum number of patients to switch (i.e., the size of the neighborhood to explore) during the local search. Defaults to 2.\necho (bool, optional):\nA flag that, when set to True, prints progress and debugging messages during the search process. Defaults to False.\n\n\n\nReturns\n\nTuple[np.ndarray, float]:\nA tuple containing:\n\nThe best solution found as a 1D NumPy array.\nThe corresponding cost (objective value) as a float.\n\n\n\n\nExample\n\nimport numpy as np\nfrom functions import local_search, calculate_objective_serv_time_lookup, compute_convolutions, get_v_star, powerset\n\nfrom typing import List, Dict, Tuple, Union\n\ndef ways_to_distribute(N: int, T: int) -&gt; List[List[int]]:\n    \"\"\"\n    Compute all possible ways to distribute N identical items into T bins.\n    \n    Each distribution is represented as a list of T nonnegative integers whose sum is N.\n    \n    Parameters:\n        N (int): Total number of identical items.\n        T (int): Number of bins.\n        \n    Returns:\n        List[List[int]]: A list of distributions. Each distribution is a list of T integers that sum to N.\n        \n    Example:\n        &gt;&gt;&gt; ways_to_distribute(3, 2)\n        [[0, 3], [1, 2], [2, 1], [3, 0]]\n    \"\"\"\n    # Base case: only one bin left, all items must go into it.\n    if T == 1:\n        return [[N]]\n    \n    distributions = []\n    # Iterate over possible numbers of items in the first bin\n    for i in range(N + 1):\n        # Recursively distribute the remaining items among the remaining bins.\n        for distribution in ways_to_distribute(N - i, T - 1):\n            distributions.append([i] + distribution)\n            \n    return distributions\n  \ndef choose_best_solution(solutions: List[np.ndarray], d: int, convs: Dict[int, np.ndarray], w: float, v_star: np.ndarray) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Choose the best solution from a list of solutions based on the objective function.\n    \n    Parameters:\n        solutions (List[np.ndarray]): A list of solution vectors.\n        d (int): Duration threshold for a time slot.\n        convs (Dict[int, np.ndarray]): Precomputed convolutions of the service time PMF.\n        w (float): Weighting factor for the objective function.\n        \n    Returns:\n        Tuple[np.ndarray, float]: The best solution and its corresponding cost.\n    \"\"\"\n    best_solution = None\n    best_cost = float('inf')\n    \n    for solution in solutions:\n        waiting_time, spillover = calculate_objective_serv_time_lookup(solution, d, convs)\n        cost = w * waiting_time /N + (1 - w) * spillover\n        if cost &lt; best_cost:\n            best_solution = solution\n            best_cost = cost\n            \n    return np.array(best_solution), best_cost\n\n# Example schedule: initial solution vector\nx_initial = [3, 2, 1, 0]\nT = len(x_initial)\nN = sum(x_initial)\n\n# Duration threshold for a time slot\nd = 5\n\n# Example probability mass function and no-show probability\nservice_time = np.zeros(11)\nservice_time[3] = 0.2\nservice_time[5] = 0.3\nservice_time[8] = 0.5\nq = 0.1\n\n# Compute convolutions (precomputed service time distributions)\nconvs = compute_convolutions(service_time, N=N, q=q)\n\n# Weighting factor for the objective function\nw = 0.5\n\n# Generate adjustment vectors for the schedule (v_star)\nv_star = get_v_star(len(x_initial))\n\n# Perform local search to optimize the schedule\nbest_solution, best_cost = local_search(x_initial, d, convs, w, v_star, size=T, echo=True)\n\nprint(\"Best Solution:\", best_solution)\nprint(\"Best Cost:\", best_cost)\n\nInitial solution: [3 2 1 0], cost: 37.71594467672401\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 3\nFound better solution: [2 2 1 1], cost: 30.52386358592401\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 4\nFound better solution: [1 2 1 2], cost: 25.53066071370001\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 4\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 6\nFound better solution: [1 1 1 3], cost: 22.474543162500005\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 4\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 6\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 4\nBest Solution: [1 1 1 3]\nBest Cost: 22.474543162500005\n\n\n\nimport unittest\nimport numpy as np\nfrom functions import local_search, compute_convolutions, get_v_star\n\nclass TestLocalSearch(unittest.TestCase):\n    def test_local_search_improvement(self):\n        # Set up a simple test with a known schedule and parameters\n        x_initial = [3, 2, 1, 0]\n        T = len(x_initial)\n        N = sum(x_initial)\n        d = 5\n        service_time = np.zeros(11)\n        service_time[3] = 0.2\n        service_time[5] = 0.3\n        service_time[8] = 0.5\n        q = 0.1\n        convs = compute_convolutions(service_time, N=N, q=q)\n        w = 0.5\n        v_star = get_v_star(len(x_initial))\n        \n        # Perform local search\n        best_solution, best_cost = local_search(x_initial, d, convs, w, v_star, size=T, echo=False)\n        print(\"Best Solution:\", best_solution, \"Best Cost:\", best_cost)\n        \n        # Iterate over all solutions and choose best solution\n        solutions = ways_to_distribute(N, T)\n        best_solution_brute, best_cost_brute = choose_best_solution(solutions, d, convs, w, v_star)\n        print(\"Best Brute-force Solution:\", best_solution_brute, \"Best Brute-force Cost:\", best_cost_brute)\n        \n        # Verify that the local search solution is equal to the brute-force solution\n        self.assertTrue(np.array_equal(best_solution, best_solution_brute), \"The local search solution should match the brute-force solution.\")\n        \n        # Verify that the returned solution has the same length as the initial schedule\n        self.assertEqual(len(best_solution), len(x_initial), \"The optimized solution should have the same length as the initial solution.\")\n        \n        # Check that the cost is a float and that a solution is returned\n        self.assertIsInstance(best_cost, float, \"Cost should be a float value.\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\nF\n======================================================================\nFAIL: test_local_search_improvement (__main__.TestLocalSearch.test_local_search_improvement)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/var/folders/gf/gtt1mww524x0q33rqlwsmjw80000gn/T/ipykernel_67823/3482521855.py\", line 31, in test_local_search_improvement\n    self.assertTrue(np.array_equal(best_solution, best_solution_brute), \"The local search solution should match the brute-force solution.\")\nAssertionError: False is not true : The local search solution should match the brute-force solution.\n\n----------------------------------------------------------------------\nRan 1 test in 0.014s\n\nFAILED (failures=1)\n\n\nInitial solution: [3 2 1 0], cost: 37.71594467672401\nBest Solution: [1 1 1 3] Best Cost: 22.474543162500005\nBest Brute-force Solution: [2 1 1 2] Best Brute-force Cost: 9.894705450000002",
    "crumbs": [
      "Function documentation",
      "`local_search`"
    ]
  }
]