[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Appointment Scheduling Experiments",
    "section": "",
    "text": "To Do",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#to-do",
    "href": "index.html#to-do",
    "title": "Appointment Scheduling Experiments",
    "section": "",
    "text": "Add a brief introduction to the project\nEvaluator Functions\n\nDocument and test the evaluator functions\nRun experiments using the evaluator functions\n\nSearcher Functions\n\nDocument and test the search functions\nRun experiments using the search functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "function-testing.html",
    "href": "function-testing.html",
    "title": "2  Evaluator functions testing",
    "section": "",
    "text": "2.1 Objective\nIn this experiment we will test whether the functions for calculating the objective values work properly and efficiently.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#background",
    "href": "function-testing.html#background",
    "title": "2  Evaluator functions testing",
    "section": "2.2 Background",
    "text": "2.2 Background\nFor developing new methods for optimizing appointment schedules it is necessary that the function for calculating objective values works properly. It is also important that the function is efficient, as it will be used in optimization algorithms that will be run many times.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#hypothesis",
    "href": "function-testing.html#hypothesis",
    "title": "2  Evaluator functions testing",
    "section": "2.3 Hypothesis",
    "text": "2.3 Hypothesis\nThe functions for calculating that have been developed are working fast and generate correct results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#methodology",
    "href": "function-testing.html#methodology",
    "title": "2  Evaluator functions testing",
    "section": "2.4 Methodology",
    "text": "2.4 Methodology\n\n2.4.1 Tools and Materials\nFor testing the correct working of the functions used to calculate objective values we will compare the exact calculation to results from Monte Carlo (MC) simulations. The MC simulations allow modeling the system and replicating closely the actual process of patients arriving and being served. The exact calculation is based on the convolution of the service time distribution and the number of patients arriving at each time slot.\n\n\n2.4.2 Experimental Design\nWe will define some typical instances of schedules and calculate the objective values for them both using the exact method as well as through MC simulations. We will then compare the results.\n\n\n2.4.3 Variables\n\nIndependent Variables:\n\nDifferent instances of appointment schedules.\n\nDependent Variables:\n\nObjective value results from exact calculations and simulations.\nSpeed indicators\n\n\n\n\n2.4.4 Setup\nWe have defined the following test cases:\n\nimport numpy as np\nimport pandas as pd\nimport time\nimport plotly.graph_objects as go\nfrom functions import service_time_with_no_shows, compute_convolutions, compute_convolutions_fft, calculate_objective_serv_time_lookup\n\n# Parameters\nd = 5\nq = 0.1\n    \n# Create service time distribution\nservice_time = np.zeros(11)\nservice_time[3] = 0.2\nservice_time[5] = 0.3\nservice_time[8] = 0.5\n\naverage_service_time = np.dot(range(len(service_time)), service_time)\nprint(f\"Average service time: {average_service_time}\")\n    \n# Different schedule patterns with the same total number of patients (except for test schedule)\nschedules = [\n    (\"Calibration\", [2, 0, 0, 0, 0, 1]),\n    (\"Uniform\", [2, 2, 2, 2]),\n    (\"Decreasing\", [5, 2, 1, 0]),\n    (\"Increasing\", [0, 1, 2, 5]),\n    (\"Front-heavy\", [4, 4, 0, 0]),\n    (\"Back-heavy\", [0, 0, 4, 4]),\n    (\"Alternating\", [4, 0, 4, 0]),\n    (\"Bailey-rule\", [2, 1, 1, 1, 1, 1, 1])  # Schedule 2 initially, then 1 each slot\n]\n\n# Set number of simulations for Monte Carlo simulation\nnr_simulations = 1000\n\n# Create dictionary for storing results\nresults_dict = {'schedule_name': [], 'average_waiting_time': [], 'average_overtime': [], 'expected_waiting_time': [], 'expected_overtime': [], 'average_computation_time': []}\nresults_dict['schedule_name'] = [s[0] for s in schedules]\n\nAverage service time: 6.1\n\n\nThe “Calibration” test set is used to calibrate the simulation results with the exact results. For this schedule, the exact results can be easily calculated by hand. The average service time after correcting for no-shows for one patient is 5.49 (see below) and only the second patient in this example will experience waiting times. So the average expected waiting time is 5.49 / 3 = 1.83.\nThe expected overtime is is the expected spillover time caused by the last patient in the schedule. As there are no patients before the last patient in the last interval, the spillover time distribution is simply distribution of the event that the (adjusted) service time will exceed the interval time. For the case that overtime is 3 (8 - 5), the probability is 0.45 and for other values it is 0.55. So the expected overtime is 0.45 * 3 + 0.55 * 0 = 1.35.\nThe other test sets are used to examine the performance of the functions for different schedule patterns.\n\n\n2.4.5 Sample Size and Selection\nSample Size: - For each schedule instance we will run 1000 simulations.\nSample Selection: - During each simulation for each patient a random service time will be sampled from the distribution (adjusted for no-shows).\n\n\n2.4.6 Experimental Procedure\n\n2.4.6.1 Step 1: Adjust the service time distribution for no-shows.\n\n# Adjust service time distribution for no-shows and compare to original\nservice_time_no_shows = service_time_with_no_shows(service_time, q)\nprint(f\"Service time distribution with no-shows: {service_time_no_shows}\")\n\naverage_service_time_no_shows = np.dot(range(len(service_time_no_shows)), service_time_no_shows)\nprint(f\"Average service time with no-shows: {average_service_time_no_shows}\")\n\nService time distribution with no-shows: [0.1, 0.0, 0.0, 0.18000000000000002, 0.0, 0.27, 0.0, 0.0, 0.45, 0.0, 0.0]\nAverage service time with no-shows: 5.49\n\n\n\n\n2.4.6.2 Step 2: Monte carlo simulation\nFor each schedule instance:\n\nCalculate \\(N\\) and \\(T\\), the total number of patients and the total time of the schedule.\nFor each simulation:\n\nSample random service times for each of the \\(N\\) patient from service times distribution with no-shows.\nCalculate the the average waiting time and the overtime for the schedule using a Lindley recursion, starting at \\(t = 0\\) and ending at \\(t = T - 1\\).\n\n\n\nimport numpy as np\nfrom typing import List, Tuple, Union\n\ndef simulate_schedule(\n    schedule: List[int],\n    service_time_no_shows: Union[List[float], np.ndarray],\n    d: int,\n    nr_simulations: int\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Runs a Monte Carlo simulation for a single schedule.\n\n    This function simulates patient scheduling over multiple time slots and computes the average waiting \n    time per patient and the average overtime across all simulation iterations. Each time slot has a duration \n    threshold `d`. Service times for patients are sampled based on the provided probability mass function.\n\n    Parameters:\n        schedule (List[int]): A list where each element represents the number of patients scheduled in each time slot.\n        service_time_no_shows (Union[List[float], np.ndarray]): A probability mass function (PMF) for service times.\n        d (int): The duration threshold for a time slot.\n        nr_simulations (int): The number of simulation iterations to run.\n\n    Returns:\n        Tuple[float, float]: A tuple containing:\n            - The average waiting time per patient across simulations.\n            - The average overtime across simulations.\n    \"\"\"\n    N: int = sum(schedule)  # Total number of patients\n    T: int = len(schedule)  # Total number of time slots\n\n    total_waiting_time: float = 0.0\n    total_overtime: float = 0.0\n\n    for _ in range(nr_simulations):\n        cum_waiting_time: float = 0.0\n\n        # --- Process the first time slot ---\n        num_patients: int = schedule[0]\n        # Generate random service times for the first slot\n        sampled: np.ndarray = np.random.choice(\n            range(len(service_time_no_shows)),\n            size=num_patients,\n            p=service_time_no_shows\n        )\n\n        if num_patients == 0:\n            waiting_time: float = 0.0\n            spillover_time: float = 0.0\n        elif num_patients == 1:\n            waiting_time = 0.0\n            spillover_time = max(0, sampled[0])\n        else:\n            # For more than one patient, the waiting time is the cumulative sum\n            # of the service times for all but the last patient.\n            waiting_time = float(sum(np.cumsum(sampled[:-1])))\n            spillover_time = max(0, sum(sampled) - d)\n        cum_waiting_time += waiting_time\n\n        # --- Process the remaining time slots ---\n        for t in range(1, T):\n            num_patients = schedule[t]\n            # Generate random service times for time slot t\n            sampled = np.random.choice(\n                range(len(service_time_no_shows)),\n                size=num_patients,\n                p=service_time_no_shows\n            )\n            if num_patients == 0:\n                waiting_time = 0.0\n                spillover_time = max(0, spillover_time - d)\n            elif num_patients == 1:\n                waiting_time = spillover_time\n                spillover_time = max(0, spillover_time + sampled[0] - d)\n            else:\n                # Each patient waits the current spillover,\n                # plus additional waiting due to the service times of those ahead.\n                waiting_time = spillover_time * num_patients + sum(np.cumsum(sampled[:-1]))\n                spillover_time = max(0, spillover_time + sum(sampled) - d)\n            cum_waiting_time += waiting_time\n\n        # Accumulate normalized waiting time (per patient) and overtime\n        total_waiting_time += cum_waiting_time / N\n        total_overtime += spillover_time\n\n    avg_waiting_time: float = total_waiting_time / nr_simulations\n    avg_overtime: float = total_overtime / nr_simulations\n\n    return avg_waiting_time, avg_overtime\n\n\n# Loop through the schedules\nfor schedule_name, schedule in schedules:\n    N = sum(schedule)\n    T = len(schedule)\n    print(f\"Schedule: {schedule_name} {schedule}, N: {N}, T: {T}\")\n    \n    avg_waiting_time, avg_overtime = simulate_schedule(schedule, service_time_no_shows, d, nr_simulations)\n    \n    print(f\"Average waiting time: {avg_waiting_time}, average overtime: {avg_overtime}\")\n    results_dict['average_waiting_time'].append(avg_waiting_time)\n    results_dict['average_overtime'].append(avg_overtime)\n\nSchedule: Calibration [2, 0, 0, 0, 0, 1], N: 3, T: 6\nAverage waiting time: 1.8486666666666853, average overtime: 1.392\nSchedule: Uniform [2, 2, 2, 2], N: 8, T: 4\nAverage waiting time: 12.0175, average overtime: 24.247\nSchedule: Decreasing [5, 2, 1, 0], N: 8, T: 4\nAverage waiting time: 16.872375, average overtime: 24.117\nSchedule: Increasing [0, 1, 2, 5], N: 8, T: 4\nAverage waiting time: 12.272375, average overtime: 29.515\nSchedule: Front-heavy [4, 4, 0, 0], N: 8, T: 4\nAverage waiting time: 16.85025, average overtime: 24.151\nSchedule: Back-heavy [0, 0, 4, 4], N: 8, T: 4\nAverage waiting time: 16.67225, average overtime: 33.73\nSchedule: Alternating [4, 0, 4, 0], N: 8, T: 4\nAverage waiting time: 14.208375, average overtime: 24.128\nSchedule: Bailey-rule [2, 1, 1, 1, 1, 1, 1], N: 8, T: 7\nAverage waiting time: 6.508875, average overtime: 9.837\n\n\n\n\n2.4.6.3 Step 3: Exact calculation\nFor each schedule instance run 10 evaluations of the objective value using the exact method and calculate the average waiting time and overtime.\n\n# Loop through the schedules, run 10 evaluations, calculate average waiting time and overtime for each schedule, calculate average computation times and store the results in the results dictionary\n\nfor schedule_name, schedule in schedules:\n    N = sum(schedule)\n    T = len(schedule)\n    print(f\"Schedule: {schedule_name} {schedule}, N: {N}, T: {T}\")\n    convolutions = compute_convolutions(service_time, N, q)\n    \n    total_time = 0\n    # Exact calculation over 10 evaluations\n    for i in range(10):\n        start_time = time.time()\n        # Calculate the objective value using the exact method\n        waiting_time, overtime = calculate_objective_serv_time_lookup(schedule, d, convolutions)\n        elapsed_time = time.time() - start_time\n        total_time += elapsed_time\n        \n    avg_time = total_time / 10\n    print(f\"Expected waiting time: {waiting_time / N}, Expected overtime: {overtime}\")\n    results_dict['expected_waiting_time'].append(waiting_time / N)\n    results_dict['expected_overtime'].append(overtime)\n    results_dict['average_computation_time'].append(avg_time)\n\nSchedule: Calibration [2, 0, 0, 0, 0, 1], N: 3, T: 6\nExpected waiting time: 1.83, Expected overtime: 1.35\nSchedule: Uniform [2, 2, 2, 2], N: 8, T: 4\nExpected waiting time: 11.816608810500004, Expected overtime: 24.064827562971374\nSchedule: Decreasing [5, 2, 1, 0], N: 8, T: 4\nExpected waiting time: 16.715096633250006, Expected overtime: 23.92331748953545\nSchedule: Increasing [0, 1, 2, 5], N: 8, T: 4\nExpected waiting time: 12.5150625, Expected overtime: 29.856120798600017\nSchedule: Front-heavy [4, 4, 0, 0], N: 8, T: 4\nExpected waiting time: 16.715970000000002, Expected overtime: 23.92482686022145\nSchedule: Back-heavy [0, 0, 4, 4], N: 8, T: 4\nExpected waiting time: 16.715970000000002, Expected overtime: 33.92194762296002\nSchedule: Alternating [4, 0, 4, 0], N: 8, T: 4\nExpected waiting time: 14.233406400000002, Expected overtime: 23.95841024194273\nSchedule: Bailey-rule [2, 1, 1, 1, 1, 1, 1], N: 8, T: 7\nExpected waiting time: 6.439653759761102, Expected overtime: 9.820826086143853",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#results",
    "href": "function-testing.html#results",
    "title": "2  Evaluator functions testing",
    "section": "2.5 Results",
    "text": "2.5 Results\nComparison of the results of the exact calculations with the results of the Monte Carlo simulations.\n\ndf_results = pd.DataFrame.from_dict(results_dict)\ndf_results\n\n\n\n\n\n\n\n\n\nschedule_name\naverage_waiting_time\naverage_overtime\nexpected_waiting_time\nexpected_overtime\naverage_computation_time\n\n\n\n\n0\nCalibration\n1.848667\n1.392\n1.830000\n1.350000\n0.000098\n\n\n1\nUniform\n12.017500\n24.247\n11.816609\n24.064828\n0.000081\n\n\n2\nDecreasing\n16.872375\n24.117\n16.715097\n23.923317\n0.000069\n\n\n3\nIncreasing\n12.272375\n29.515\n12.515063\n29.856121\n0.000062\n\n\n4\nFront-heavy\n16.850250\n24.151\n16.715970\n23.924827\n0.000054\n\n\n5\nBack-heavy\n16.672250\n33.730\n16.715970\n33.921948\n0.000056\n\n\n6\nAlternating\n14.208375\n24.128\n14.233406\n23.958410\n0.000053\n\n\n7\nBailey-rule\n6.508875\n9.837\n6.439654\n9.820826\n0.000138\n\n\n\n\n\n\n\n\n\n# Extract schedule names from the dataframe\nschedule_names = df_results['schedule_name'].tolist()\n\n# Create new x-values for simulation and exact results\nx_sim = [f\"{s}&lt;br&gt;Simulation\" for s in schedule_names]\nx_exact = [f\"{s}&lt;br&gt;Exact\" for s in schedule_names]\n\n# Extract values from the dataframe\nsim_wait = df_results['average_waiting_time'].tolist()\nsim_over = df_results['average_overtime'].tolist()\nexact_wait = df_results['expected_waiting_time'].tolist()\nexact_over = df_results['expected_overtime'].tolist()\n\n# Create a combined category list with an empty category between the two groups\ncategories = x_sim + [\"\"] + x_exact\n\n# Create the figure\nfig = go.Figure()\n\n# Simulation bar traces (stacked)\nfig.add_trace(go.Bar(\n    x=x_sim,\n    y=sim_wait,\n    name='Waiting Time',\n    marker_color='blue'\n))\nfig.add_trace(go.Bar(\n    x=x_sim,\n    y=sim_over,\n    name='Overtime',\n    marker_color='red'\n))\n\n# Exact bar traces (stacked)\nfig.add_trace(go.Bar(\n    x=x_exact,\n    y=exact_wait,\n    name='Waiting Time',\n    marker_color='blue',\n    showlegend=False  # legend already shown for waiting time above\n))\nfig.add_trace(go.Bar(\n    x=x_exact,\n    y=exact_over,\n    name='Overtime',\n    marker_color='red',\n    showlegend=False  # legend already shown for overtime above\n))\n\n# Update x-axis to use the full category array (which includes the gap)\nfig.update_xaxes(\n    tickangle=45,\n    categoryorder='array',\n    categoryarray=categories\n)\n\n# Optionally, adjust the vertical dotted line.\n# For example, you can remove it if the gap is sufficient or reposition it.\nfig.update_layout(\n    title=\"Comparison of Simulation vs. Exact Results\",\n    xaxis_title=\"Schedule Type\",\n    yaxis_title=\"Time\",\n    barmode='stack',\n    shapes=[\n        dict(\n            type=\"line\",\n            xref=\"paper\", x0=0.5, x1=0.5,\n            yref=\"paper\", y0=0, y1=1,\n            line=dict(\n                color=\"black\",\n                width=2,\n                dash=\"dot\"\n            )\n        )\n    ]\n)\n\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#discussion",
    "href": "function-testing.html#discussion",
    "title": "2  Evaluator functions testing",
    "section": "2.6 Discussion",
    "text": "2.6 Discussion\nThe results show that the exact calculations and the Monte Carlo simulations are in good agreement. The average waiting times and overtimes are very close for both methods. The computation times for the exact calculations are also reasonable, indicating that the functions are efficient - at least for these limited instances.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#timeline",
    "href": "function-testing.html#timeline",
    "title": "2  Evaluator functions testing",
    "section": "2.7 Timeline",
    "text": "2.7 Timeline\nThis experiment has been started on 07-03-2025 and is expected to be finished on 14-03-2025.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html",
    "href": "xgboost-pairwise-ranking-large.html",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "",
    "text": "3.1 Objective\nObjective: Testing the performance of an XGBoost model trained for ranking pairwise schedules.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#background",
    "href": "xgboost-pairwise-ranking-large.html#background",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.2 Background",
    "text": "3.2 Background\nIn this experiment we develop a Machine Learning model using XGBoost that can evaluate two neighboring schedules and rank them according to preference. This ranking model can be applied to quickly guide the search process towards a ‘good enough’ solution.\nThe choice of using an ordinal model instead of a cardinal model is based on the consideration that it is significantly easier to determine whether alternative A is superior to B than to quantify the exact difference between A and B. This makes intuitive sense when considering the scenario of holding two identical-looking packages and deciding which one is heavier, as opposed to estimating the precise weight difference between them. (ho_ordinal_2000?).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#hypothesis",
    "href": "xgboost-pairwise-ranking-large.html#hypothesis",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.3 Hypothesis",
    "text": "3.3 Hypothesis\nAn XGBoost ranking model achieves superior computational efficiency compared to evaluating each element of a pair individually, leading to faster overall performance in ranking tasks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#methodology",
    "href": "xgboost-pairwise-ranking-large.html#methodology",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.4 Methodology",
    "text": "3.4 Methodology\n\n3.4.1 Tools and Materials\nWe use packages from Scikit-learn to prepare training data and evaluate the model and the XGBClassifier interface from the XGBoost library.\n\nimport time\nimport math\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.base import clone\nimport xgboost as xgb\nfrom xgboost.callback import TrainingCallback\nimport plotly.graph_objects as go\nimport pickle\nimport random\nfrom scipy.optimize import minimize\nfrom itertools import combinations\n\n\n\n3.4.2 Experimental Design\nTo compare an XGBoost Machine Learning model with a simple evaluation of each individual element of the pair, we will use a pairwise ranking approach. The objective is to rank two neighboring schedules according to preference.\n\nfrom functions import compute_convolutions\n\nN = 22 # Number of patients\nT = 20 # Number of intervals\nd = 5 # Length of each interval\nmax_s = 20 # Maximum service time\nq = 0.20 # Probability of a scheduled patient not showing up\nw = 0.1 # Weight for the waiting time in objective function\nl = 10\nnum_schedules = 100000 # Number of schedules to sample\n\n# Create service time distribution\ndef generate_weighted_list(max_s, l, i):\n    # Initialize an array of T+1 values, starting with zero\n    values = np.zeros(T + 1)\n    \n    # Objective function: Sum of squared differences between current weighted average and the desired l\n    def objective(x):\n        weighted_avg = np.dot(np.arange(1, T + 1), x) / np.sum(x)\n        return (weighted_avg - l) ** 2\n\n    # Constraint: The sum of the values from index 1 to T must be 1\n    constraints = ({\n        'type': 'eq',\n        'fun': lambda x: np.sum(x) - 1\n    })\n    \n    # Bounds: Each value should be between 0 and 1\n    bounds = [(0, 1)] * T\n\n    # Initial guess: Random distribution that sums to 1\n    initial_guess = np.random.dirichlet(np.ones(T))\n\n    # Optimization: Minimize the objective function subject to the sum and bounds constraints\n    result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n\n    # Set the values in the array (index 0 remains 0)\n    values[1:] = result.x\n\n    # Now we need to reorder the values as per the new requirement\n    first_part = np.sort(values[1:i+1])  # Sort the first 'i' values in ascending order\n    second_part = np.sort(values[i+1:])[::-1]  # Sort the remaining 'T-i' values in descending order\n    \n    # Combine the sorted parts back together\n    values[1:i+1] = first_part\n    values[i+1:] = second_part\n    \n    return values\n\ni = 5  # First 5 highest values in ascending order, rest in descending order\ns = generate_weighted_list(max_s, l, i)\nprint(s)\nprint(\"Sum:\", np.sum(s[1:]))  # This should be 1\nprint(\"Weighted service time:\", np.dot(np.arange(1, T + 1), s[1:]))  # This should be close to l\n\nconvolutions = compute_convolutions(s, N, q)\nfile_path_parameters = f\"datasets/parameters_{N}_{T}_{l}.pkl\"\nwith open(file_path_parameters, 'wb') as f:\n    pickle.dump({\n      'N': N,\n      'T': T,\n      'd': d,\n      'max_s': max_s,\n      'q': q,\n      'w': w,\n      'l': l,\n      'num_schedules': num_schedules,\n      'convolutions': convolutions\n      }, f)\n    print(f\"Data saved successfully to '{file_path_parameters}'\")\n\n[0.         0.00543629 0.03030549 0.04163197 0.11719454 0.11859203\n 0.11731255 0.11310677 0.10304861 0.05397991 0.05117193 0.03973443\n 0.03821059 0.03796436 0.02880373 0.02842576 0.02524925 0.01903744\n 0.01432856 0.01035653 0.00610927]\nSum: 1.0000000000096703\nWeighted service time: 8.093510567287902\nData saved successfully to 'datasets/parameters_22_20_10.pkl'\n\n\nWe will create a random set of pairs of neighboring schedules with \\(N = 22\\) patients and \\(T = 20\\) intervals of length \\(d = 5\\).\nA neighbor of a schedule x is considered a schedule x’ where single patients have been shifted one interval to the left. Eg: ([2,1,1,2], [1,2,0,3]) are neighbors and ([2,1,1,2], [2,1,3,0]) are not, because [1,2,0,3] - [2,1,1,2] = [-1, 1, -1, 1] and [2,1,3,0] - [2,1,1,2] = [0, 0, 2, -2].\nService times will have a discrete distribution. The probability of a scheduled patient not showing up will be \\(q = 0.2\\).\nThe objective function will be the weighted average of the total waiting time of all patients and overtime. The model will be trained to predict which of the two neighboring schedules has the lowest objective value. The prediction time will be recorded. Then the same schedules will be evaluated by computing the objective value and then ranked.\n\n\n3.4.3 Variables\n\nIndependent Variables: A list of tuples with pairs of neighboring schedules.\nDependent Variables: A list with rankings for each tuple of pairwise schedules. Eg: If the rank for ([2,1,1], [1,1,2]) equals 0 this means that the schedule with index 0 ([2,1,1]) has the lowest objective value.\n\n\n\n3.4.4 Data Collection\nThe data set will be generated using simulation in which random samples will be drawn from the population of all possible schedules. For each sample a random neighboring schedule will be created.\n\n\n3.4.5 Sample Size and Selection\nSample Size: The total population size equals \\({{N + T -1}\\choose{N}} \\approx\\) 244663.0 mln. For this experiment we will be using a relatively small sample of 100000 pairs of schedules.\nSample Selection: The samples will be drawn from a lexicographic order of possible schedules in order to accurately reflect the combinatorial nature of the problem and to ensure unbiased sampling from the entire combinatorial space.\n\n\n3.4.6 Experimental Procedure\nThe experiment involves multiple steps, beginning with data preparation and concluding with model evaluation.The diagram below illustrates the sequence of steps.\n\n\n\n\n\ngraph TD\n    A[\"From population\"] --&gt;|\"Sample\"| B[\"Random subset\"]\n    B --&gt; |Create neighbors| C[\"Features: Schedule pairs\"]\n    C --&gt; |Calculate objectives| D[\"Objective values\"]\n    D --&gt; |Rank objectives| E[\"Labels: Rankings\"]\n    E --&gt; |\"Split dataset\"| F[\"Training set\"]\n    E --&gt; |\"Split dataset\"| G[\"Test set\"]\n    F --&gt; |\"Train\"| H[\"Model\"]\n    H[\"Model\"] --&gt; |\"Apply\"| G[\"Test set\"]\n    G[\"Test set\"] --&gt; |\"Evaluate\"| I[\"Performance\"]\n\n\n\n\n\n\nStep 1: Randomly select a subset of schedules.\n\nfrom functions import create_random_schedules\n\nstart = time.time()\n# schedules = random_combination_with_replacement(T, N, num_schedules)\nschedules = create_random_schedules(T, N, num_schedules)\nprint(f\"Sampled: {len(schedules):,} schedules\\n\")\nh = random.choices(range(len(schedules)), k=7)\nprint(f\"Sampled schedules: {h}\")\nfor i in h:\n    print(f\"Schedule: {schedules[i]}\")\nend = time.time()\ndata_prep_time = end - start\n\nprint(f\"\\nProcessing time: {data_prep_time} seconds\\n\")\n\nSampled: 100,000 schedules\n\nSampled schedules: [85534, 49568, 65853, 46331, 77732, 38614, 7289]\nSchedule: [0, 1, 2, 2, 1, 0, 4, 3, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 0]\nSchedule: [0, 2, 0, 3, 0, 1, 2, 1, 2, 1, 1, 0, 1, 2, 0, 1, 0, 2, 1, 2]\nSchedule: [1, 1, 0, 1, 1, 2, 1, 2, 1, 0, 0, 0, 2, 2, 0, 2, 1, 0, 1, 4]\nSchedule: [1, 2, 0, 4, 2, 1, 0, 0, 2, 2, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1]\nSchedule: [0, 3, 2, 2, 1, 2, 1, 1, 1, 0, 0, 1, 2, 0, 1, 1, 0, 2, 2, 0]\nSchedule: [2, 1, 1, 0, 0, 5, 2, 0, 1, 3, 0, 1, 3, 1, 0, 0, 0, 0, 1, 1]\nSchedule: [1, 1, 1, 2, 4, 0, 0, 0, 1, 0, 1, 0, 2, 0, 1, 2, 1, 1, 3, 1]\n\nProcessing time: 0.6191208362579346 seconds\n\n\n\nStep 2: Create pairs of neighboring schedules.\n\nfrom functions import get_v_star\n\ndef create_neighbors_list(s: list[int], v_star: np.ndarray) -&gt; (list[int], list[int]):\n    \"\"\"\n    Create a set of pairs of schedules that are from the same neighborhood.\n    \n    Parameters:\n      s (list[int]): A list of integers with |s| = T and sum N.\n      v_star (np.ndarray): Precomputed vectors V* of length T.\n      \n    Returns:\n      tuple(list[int], list[int]): A pair of schedules.\n    \"\"\"\n    T = len(s)\n\n    # Precompute binomial coefficients (weights for random.choices)\n    binom_coeff = [math.comb(T, i) for i in range(1, T)]\n\n    # Choose a random value of i with the corresponding probability\n    i = random.choices(range(1, T), weights=binom_coeff)[0]\n\n    # Instead of generating the full list of combinations, sample one directly\n    j = random.sample(range(T), i)\n    \n    s_p = s.copy()\n    for k in j:\n        s_temp = np.array(s_p) + v_star[k]\n        s_temp = s_temp.astype(int)\n        if np.all(s_temp &gt;= 0):\n            s_p = s_temp.astype(int).tolist()\n        \n    return s, s_p\n\nstart = time.time()\nv_star = get_v_star(T)\nneighbors_list = [create_neighbors_list(schedule, v_star) for schedule in schedules] # This can be done in parellel to improve speed\nend = time.time()\nfor i in h:\n    original_schedule = neighbors_list[i][0]\n    neighbor_schedule = neighbors_list[i][1]\n    difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]\n    print(f\"Neighbors\\n{original_schedule}\\n{neighbor_schedule}\\n{difference}\")\ntraining_set_feat_time = end - start\nprint(f\"\\nProcessing time: {training_set_feat_time} seconds\\n\")\n\nNeighbors\n[0, 1, 2, 2, 1, 0, 4, 3, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 0]\n[0, 1, 2, 3, 0, 1, 4, 2, 1, 1, 2, 0, 0, 1, 0, 1, 1, 1, 1, 0]\n[0, 0, 0, -1, 1, -1, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0]\nNeighbors\n[0, 2, 0, 3, 0, 1, 2, 1, 2, 1, 1, 0, 1, 2, 0, 1, 0, 2, 1, 2]\n[1, 1, 1, 2, 0, 1, 3, 1, 2, 1, 0, 0, 2, 1, 1, 0, 1, 2, 0, 2]\n[-1, 1, -1, 1, 0, 0, -1, 0, 0, 0, 1, 0, -1, 1, -1, 1, -1, 0, 1, 0]\nNeighbors\n[1, 1, 0, 1, 1, 2, 1, 2, 1, 0, 0, 0, 2, 2, 0, 2, 1, 0, 1, 4]\n[1, 0, 0, 2, 1, 1, 2, 2, 0, 0, 0, 0, 2, 3, 0, 2, 0, 0, 1, 5]\n[0, 1, 0, -1, 0, 1, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, -1]\nNeighbors\n[1, 2, 0, 4, 2, 1, 0, 0, 2, 2, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1]\n[1, 2, 0, 5, 1, 1, 1, 0, 1, 3, 0, 1, 0, 0, 1, 3, 1, 0, 1, 0]\n[0, 0, 0, -1, 1, 0, -1, 0, 1, -1, 1, 0, 0, 0, 0, -1, 0, 1, -1, 1]\nNeighbors\n[0, 3, 2, 2, 1, 2, 1, 1, 1, 0, 0, 1, 2, 0, 1, 1, 0, 2, 2, 0]\n[1, 2, 2, 2, 2, 2, 1, 1, 0, 0, 1, 0, 2, 0, 2, 0, 1, 1, 2, 0]\n[-1, 1, 0, 0, -1, 0, 0, 0, 1, 0, -1, 1, 0, 0, -1, 1, -1, 1, 0, 0]\nNeighbors\n[2, 1, 1, 0, 0, 5, 2, 0, 1, 3, 0, 1, 3, 1, 0, 0, 0, 0, 1, 1]\n[3, 1, 0, 0, 1, 4, 2, 0, 1, 3, 0, 2, 2, 1, 0, 0, 0, 1, 1, 0]\n[-1, 0, 1, 0, -1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, -1, 0, 1]\nNeighbors\n[1, 1, 1, 2, 4, 0, 0, 0, 1, 0, 1, 0, 2, 0, 1, 2, 1, 1, 3, 1]\n[0, 1, 2, 2, 3, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 3, 0, 1, 3, 2]\n[1, 0, -1, 0, 1, 0, 0, -1, 1, -1, 1, -1, 1, 0, 0, -1, 1, 0, 0, -1]\n\nProcessing time: 9.164121150970459 seconds\n\n\n\nStep 3: For each schedule in each pair calculate the objective. For each pair save the index of the schedule that has the lowest objective value.\n\nfrom functions import calculate_objective_serv_time_lookup\n\nobjectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[0], d, convolutions)]\n]\nstart = time.time()\nobjectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[1], d, convolutions)]\n]\nend = time.time()\ntraining_set_lab_time = end - start\nobjectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]\nrankings = np.argmin(objectives, axis=1).tolist()\nfor i in range(5):\n    print(f\"Objectives: {objectives[i]}, Ranking: {rankings[i]}\")\n\nprint(f\"\\nProcessing time: {training_set_lab_time} seconds\\n\")\n\n# Saving neighbors_list and objectives to a pickle file\n\nfile_path_neighbors = f\"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl\"\nwith open(file_path_neighbors, 'wb') as f:\n    pickle.dump({'neighbors_list': neighbors_list, 'objectives': objectives, 'rankings': rankings}, f)\n    print(f\"Data saved successfully to '{file_path_neighbors}'\")\n\nObjectives: [88.17925506905414, 97.97325997172206], Ranking: 0\nObjectives: [97.91182775072866, 97.8591016726144], Ranking: 1\nObjectives: [126.1522161894033, 127.1647578693569], Ranking: 0\nObjectives: [101.79321072681091, 98.59424287048375], Ranking: 1\nObjectives: [121.71994028846842, 123.49965359375256], Ranking: 0\n\nProcessing time: 48.1225049495697 seconds\n\nData saved successfully to 'datasets/neighbors_and_objectives_22_20_10.pkl'\n\n\nStep 4: Create training and test sets.\n\n# Prepare the dataset\nX = []\nfor neighbors in neighbors_list:\n    X.append(neighbors[0] + neighbors[1])\n\nX = np.array(X)\ny = np.array(rankings)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 5: Train the XGBoost model.\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B[Initialize StratifiedKFold]\n    B --&gt; C[Initialize XGBClassifier]\n    C --&gt; D[Set results as empty list]\n    D --&gt; E[Loop through each split of cv split]\n    E --&gt; F[Get train and test indices]\n    F --&gt; G[Split X and y into X_train, X_test, y_train, y_test]\n    G --&gt; H[Clone the classifier]\n    H --&gt; I[Call fit_and_score function]\n    I --&gt; J[Fit the estimator]\n    J --&gt; K[Score on training set]\n    J --&gt; L[Score on test set]\n    K --&gt; M[Return estimator, train_score, test_score]\n    L --&gt; M\n    M --&gt; N[Append the results]\n    N --&gt; E\n    E --&gt; O[Loop ends]\n    O --&gt; P[Print results]\n    P --&gt; Q[End]\n\n\n\n\n\n\n\nclass CustomCallback(TrainingCallback):\n    def __init__(self, period=10):\n        self.period = period\n\n    def after_iteration(self, model, epoch, evals_log):\n        if (epoch + 1) % self.period == 0:\n            print(f\"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}\")\n        return False\n    \ndef fit_and_score(estimator, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0\n    )\n\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return estimator, train_score, test_score\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n\n# Initialize the XGBClassifier without early stopping here\n# Load the best trial parameters from a JSON file.\nwith open(\"model_params.json\", \"r\") as f:\n    model_params = json.load(f)\n    \n# Initialize the EarlyStopping callback with validation dataset\nearly_stop = xgb.callback.EarlyStopping(\n    rounds=10, metric_name='logloss', data_name='validation_0', save_best=True\n)\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n    early_stopping_rounds=9,\n    #callbacks=[CustomCallback(period=50), early_stop],\n    callbacks=[CustomCallback(period=50)],\n)\nprint(\"Params: \")\nfor key, value in model_params.items():\n    print(f\" {key}: {value}\")\n\nstart = time.time()\nresults = []\n\nfor train_idx, test_idx in cv.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    est, train_score, test_score = fit_and_score(\n        clone(clf), X_train, X_test, y_train, y_test\n    )\n    results.append((est, train_score, test_score))\nend = time.time()\ntraining_time = end - start\nprint(f\"\\nTraining time: {training_time} seconds\\n\")\n\nParams: \n max_depth: 6\n min_child_weight: 1\n gamma: 0.1\n subsample: 0.8\n colsample_bytree: 0.8\n learning_rate: 0.1\n n_estimators: 100\nEpoch 49, Evaluation log: 0.37537274533435705\nEpoch 99, Evaluation log: 0.3202946408016607\nEpoch 49, Evaluation log: 0.36902658584490416\nEpoch 99, Evaluation log: 0.31779550507897514\nEpoch 49, Evaluation log: 0.37383715238538573\nEpoch 99, Evaluation log: 0.32173826755817864\nEpoch 49, Evaluation log: 0.37095330382874236\nEpoch 99, Evaluation log: 0.32059324672384537\nEpoch 49, Evaluation log: 0.3741410843353253\nEpoch 99, Evaluation log: 0.3237181565931067\n\nTraining time: 2.9833967685699463 seconds\n\n\n\nStep 6: To evaluate the performance of the XGBoost ranking model, we will use Stratified K-Fold Cross-Validation with 5 splits, ensuring each fold maintains the same class distribution as the original dataset. Using StratifiedKFold(n_splits=5, shuffle=True, random_state=94), the dataset will be divided into five folds. In each iteration, the model will be trained on four folds and evaluated on the remaining fold. A custom callback, CustomCallback(period=10), will print the evaluation log every 10 epochs.\nThe fit_and_score function will fit the model and score it on both the training and test sets, storing the results for each fold. This provides insight into the model’s performance across different subsets of the data, helps in understanding how well the model generalizes to unseen data and identifies potential overfitting or underfitting issues. The overall processing time for the cross-validation will also be recorded.\n\n# Print results\nfor i, (est, train_score, test_score) in enumerate(results):\n    print(f\"Fold {i+1} - Train Score (Accuracy): {train_score:.4f}, Test Score (Accuracy): {test_score:.4f}\")\n\nFold 1 - Train Score (Accuracy): 0.8820, Test Score (Accuracy): 0.8743\nFold 2 - Train Score (Accuracy): 0.8824, Test Score (Accuracy): 0.8751\nFold 3 - Train Score (Accuracy): 0.8842, Test Score (Accuracy): 0.8726\nFold 4 - Train Score (Accuracy): 0.8849, Test Score (Accuracy): 0.8709\nFold 5 - Train Score (Accuracy): 0.8820, Test Score (Accuracy): 0.8713\n\n\nTraining the model on the entire dataset provides a final model that has learned from all available data. Recording the training time helps in understanding the computational efficiency and scalability of the model with the given hyperparameters.\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nclf.save_model('models/classifier_large_instance.json')\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\\n\")\n\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\nTraining accuracy: 88.11%\n\n\nTraining time: 0.46138501167297363 seconds\n\n\n\n\n\n3.4.7 Validation\nGenerating test schedules and calculating their objectives and rankings allows us to create a new dataset for evaluating the model’s performance on unseen data.\n\nnum_test_schedules = 1000\n\n#test_schedules = random_combination_with_replacement(T, N, num_test_schedules)\ntest_schedules = create_random_schedules(T, N, num_test_schedules)\n\ntest_neighbors = [create_neighbors_list(test_schedule, v_star) for test_schedule in test_schedules] # This can be done in parellel to improve speed\n\nprint(f\"Sampled: {len(test_schedules)} schedules\\n\")\n\ntest_objectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[0], d, convolutions)]\n]\n# Start time measurement for the evaluation\nstart = time.time()\ntest_objectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[1], d, convolutions)]\n]\ntest_rankings = [0 if test_obj &lt; test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]\nend = time.time()\nevaluation_time = end - start\n\n# Combine the objectives for each pair for later processing\ntest_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]\n\nprint(f\"\\nEvaluation time: {evaluation_time} seconds\\n\")\n\nfor i in range(6):\n    print(f\"Neighbors: {test_neighbors[i]},\\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\\n\")\n\nSampled: 1000 schedules\n\n\nEvaluation time: 0.5211591720581055 seconds\n\nNeighbors: ([0, 2, 2, 0, 0, 1, 3, 3, 2, 2, 0, 0, 1, 0, 1, 1, 1, 0, 1, 2], [1, 2, 1, 0, 0, 2, 3, 3, 2, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 2]),\nObjectives: [110.75226450345767, 105.5293674787008], Ranking: 1\n\nNeighbors: ([0, 2, 1, 1, 3, 2, 0, 0, 1, 1, 1, 0, 1, 1, 2, 2, 0, 0, 0, 4], [1, 1, 1, 2, 3, 1, 0, 0, 2, 1, 0, 1, 1, 0, 2, 2, 0, 1, 0, 3]),\nObjectives: [99.9391699307969, 94.16440194650053], Ranking: 1\n\nNeighbors: ([1, 0, 3, 0, 0, 2, 2, 0, 5, 4, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0], [1, 0, 3, 0, 0, 3, 1, 0, 5, 5, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0]),\nObjectives: [122.34851422377199, 123.69654835822257], Ranking: 0\n\nNeighbors: ([2, 1, 3, 2, 1, 0, 2, 0, 3, 0, 0, 2, 0, 0, 2, 0, 2, 1, 1, 0], [2, 2, 2, 3, 1, 0, 1, 0, 3, 0, 1, 1, 0, 0, 2, 1, 2, 1, 0, 0]),\nObjectives: [105.10528129650763, 108.43029352470786], Ranking: 0\n\nNeighbors: ([2, 0, 2, 1, 0, 2, 1, 0, 1, 3, 3, 0, 0, 1, 0, 0, 1, 2, 1, 2], [1, 1, 2, 0, 0, 2, 2, 0, 0, 4, 2, 0, 1, 0, 0, 0, 2, 1, 1, 3]),\nObjectives: [93.32367885774698, 93.1968800740998], Ranking: 1\n\nNeighbors: ([0, 1, 3, 0, 0, 0, 2, 1, 3, 1, 1, 0, 2, 1, 0, 1, 1, 1, 3, 1], [1, 0, 3, 0, 0, 1, 1, 2, 2, 2, 0, 0, 2, 1, 0, 2, 1, 0, 3, 1]),\nObjectives: [102.08587233729006, 97.05508233059925], Ranking: 1\n\n\n\nMaking predictions on new data and comparing them to the actual rankings provides an evaluation of the model’s performance in practical applications. Recording the prediction time helps in understanding the model’s efficiency during inference.\n\ninput_X = test_neighbors\nX_new = []\nfor test_neighbor in input_X:\n    X_new.append(test_neighbor[0] + test_neighbor[1])\n    \n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.00497889518737793 seconds\n\ntest_rankings = [1 1 0 0 1 1], \ny_pred = [1 1 0 0 0 1], \ny_pred_proba = \n[[0.03129494 0.96870506]\n [0.01167864 0.98832136]\n [0.84496987 0.15503016]\n [0.8883002  0.11169982]\n [0.50500226 0.49499774]\n [0.03031099 0.969689  ]]\n\n\nCalculating the ambiguousness of the predicted probabilities helps in understanding the model’s confidence in its predictions. High ambiguousness indicates uncertain predictions, while low ambiguousness indicates confident predictions.\nAmbiguousness is calculated using the formula for entropy:\n\\[\nH(X) = - \\sum_{i} p(x_i) \\log_b p(x_i)\n\\]\nWhere in our case:\n\n\\(H(X)\\) is the ambiguousness of the random variable \\(X\\) - the set of probability scores for the predicted rankings,\n\\(p(x_i)\\) is probability score \\(x_i\\),\n\\(\\log_b\\) is the logarithm with base \\(b\\) (here \\(\\log_2\\) as we have two predicted values),\nThe sum is taken over all possible outcomes of \\(X\\).\n\nCalculating cumulative error rate and cumulative accuracy helps in understanding how the model’s performance evolves over the dataset.\nVisualizing the relationship between ambiguousness and error provides insights into how uncertainty in the model’s predictions correlates with its accuracy. This can help in identifying patterns and understanding the conditions under which the model performs well or poorly.\n\nfrom functions import calculate_ambiguousness\n\nerrors = np.abs(y_pred - np.array(test_rankings))\n\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9)))\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()\n\n                                                \n\n\n\n\n3.4.8 Hyperparameter Optimization\nIn the initial model the choice of hyperparameters was based on default values, examples from demo’s or trial and error. To improve the model’s performance, we applied a hyperparameter optimization technique to find the best set of hyperparameters. We used a grid search with cross-validation to find the optimal hyperparameters for the XGBoost model. The grid search was performed over a predefined set of hyperparameters, and the best hyperparameters were selected based on the model’s performance on the validation set. The best hyperparameters were then used to train the final model.\n\nfrom functions import compare_json\n\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n    \ndifferences = compare_json(model_params, best_trial_params)\n\nparams_tbl = pd.DataFrame(differences)\nparams_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)\nprint(params_tbl)\n\n                      max_depth     gamma  subsample  colsample_bytree  \\\nbase parameters               6  0.100000   0.800000          0.800000   \noptimized parameters          5  0.304548   0.781029          0.922528   \n\n                      learning_rate  n_estimators  \nbase parameters            0.100000           100  \noptimized parameters       0.239488           490  \n\n\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\n# Load the best trial parameters from a JSON file.\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\")\n\n\nTraining time: 1.9750583171844482 seconds\n\nTraining accuracy: 95.18%\n\n\n\n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.00556492805480957 seconds\n\ntest_rankings = [1 1 0 0 1 1], \ny_pred = [1 1 0 0 0 1], \ny_pred_proba = \n[[5.82575798e-04 9.99417424e-01]\n [1.33752823e-04 9.99866247e-01]\n [9.72199440e-01 2.78005321e-02]\n [9.87804174e-01 1.21957995e-02]\n [5.05212545e-01 4.94787425e-01]\n [1.01699233e-02 9.89830077e-01]]\n\n\n\nerrors = np.abs(y_pred - np.array(test_rankings))\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors, \"Schedules\": test_neighbors, \"Objectives\": test_objectives}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n\n\n\n\n\n\n\nAmbiguousness\nError\nSchedules\nObjectives\nCumulative error rate\nCumulative accuracy\n\n\n\n\n460\n0.000075\n0\n([1, 2, 4, 1, 1, 0, 1, 1, 3, 1, 1, 0, 1, 1, 2,...\n[111.9192594976547, 117.92971932670405]\n0.0\n1.0\n\n\n977\n0.000193\n0\n([1, 3, 1, 1, 1, 0, 0, 2, 2, 1, 3, 0, 2, 2, 0,...\n[106.06261375989104, 113.30079370453848]\n0.0\n1.0\n\n\n140\n0.000241\n0\n([1, 5, 0, 2, 3, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1,...\n[108.78749046739821, 114.52227389419927]\n0.0\n1.0\n\n\n379\n0.000384\n0\n([1, 0, 0, 1, 1, 1, 1, 5, 0, 1, 0, 2, 1, 1, 2,...\n[110.47000733469568, 123.31454961809814]\n0.0\n1.0\n\n\n337\n0.000418\n0\n([1, 1, 1, 0, 2, 1, 2, 1, 1, 0, 2, 0, 1, 1, 0,...\n[90.22676574302173, 100.06884212878104]\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9),\n                    customdata=df[[\"Schedules\", \"Objectives\"]],\n                    hovertemplate=\n                        \"Ambiguousness: %{x} &lt;br&gt;\" +\n                        \"Error: %{y} &lt;br&gt;\" +\n                        \"Schedules: %{customdata[0][0]} / %{customdata[0][1]} &lt;br&gt;\" +\n                        \"Objectives: %{customdata[1]} &lt;br&gt;\"\n                    ))\n                  \nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#results",
    "href": "xgboost-pairwise-ranking-large.html#results",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.5 Results",
    "text": "3.5 Results\nWe wanted to test whether an XGBoost classification model could be used to assess and rank the quality of pairs of schedules. For performance benchmarking we use the conventional calculation method utilizing Lindley recursions.\nWe trained the XGBoost ranking model with a limited set of features (schedules) and labels (objectives). The total number of possible schedules is approximately 244663.0 million. For training and evaluation, we sampled 200000 schedules and corresponding neighbors. Generating the feature and label set took a total of 57.9057 seconds, with the calculation of objective values accounting for 48.1225 seconds.\nThe model demonstrates strong and consistent performance with high accuracies both for training, testing and validation (92.38%) with good generalization and stability. Total training time for the final model was 1.9751 seconds. The evaluation of 1000 test schedules took 0.0056 seconds for the the XGBoost model and 0.5212 for the conventional method, which is an improvement of 93X.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#discussion",
    "href": "xgboost-pairwise-ranking-large.html#discussion",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.6 Discussion",
    "text": "3.6 Discussion\n\ntraining_time = round(modeling_time, 4)\nconventional_time = round(evaluation_time, 4)\nxgboost_time = round(prediction_time, 4)\n\n# Define time values for plotting\ntime_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds\n\n# Calculate evaluations for method 1\nmethod1_evaluations = np.where(time_values &gt;= training_time, (time_values - training_time) / xgboost_time * 1000, 0)\n\n# Calculate evaluations for method 2\nmethod2_evaluations = time_values / conventional_time * 1000\n\n# Create line chart\nfig = go.Figure()\n\n# Add method 1 trace\nfig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))\n\n# Add method 2 trace\nfig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))\n\n# Update layout\nfig.update_layout(\n    title=\"Speed comparison between XGBoost ranking model and conventional method\",\n    xaxis_title=\"Time (seconds)\",\n    yaxis_title=\"Number of Evaluations\",\n    legend_title=\"Methods\",\n    template=\"plotly_white\"\n)\n\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#timeline",
    "href": "xgboost-pairwise-ranking-large.html#timeline",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.7 Timeline",
    "text": "3.7 Timeline\nThis experiment was started on 26-04-2025. The expected completion date is 26-04-2025.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#references",
    "href": "xgboost-pairwise-ranking-large.html#references",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.8 References",
    "text": "3.8 References",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "",
    "text": "4.1 Objective\nObjective: Testing the performance of an XGBoost model trained for ranking pairwise schedules taken from a neighborhood around quasi optimal initial schedule (Bailey-Welch).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#background",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#background",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.2 Background",
    "text": "4.2 Background\nIn a previous experiment we developed a Machine Learning model using XGBoost that can evaluate two neighboring schedules and rank them according to preference. For evaluation random schedules were sampled from the full solution set.\nThe full solution set however contains many schedules that are obviously not optimal. Adding them to the training set would provide the model with rather useless knowledge. Therefore in this experiment we only sample pairs of schedules taken from within the vicinity of a ‘good’ starting point.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#hypothesis",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#hypothesis",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.3 Hypothesis",
    "text": "4.3 Hypothesis\nAn XGBoost ranking model achieves superior computational efficiency compared to evaluating each element of a pair individually, leading to faster overall performance in ranking tasks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#methodology",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#methodology",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.4 Methodology",
    "text": "4.4 Methodology\n\n4.4.1 Tools and Materials\nWe use packages from Scikit-learn to prepare training data and evaluate the model and the XGBClassifier interface from the XGBoost library.\n\nimport time\nimport math\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.base import clone\nimport xgboost as xgb\nfrom xgboost.callback import TrainingCallback\nimport plotly.graph_objects as go\nimport pickle\nimport random\nfrom scipy.optimize import minimize\nfrom itertools import combinations\n\n\n\n4.4.2 Experimental Design\nTo compare an XGBoost Machine Learning model with a simple evaluation of each individual element of the pair, we will use a pairwise ranking approach. The objective is to rank two neighboring schedules according to preference.\n\nfrom functions import compute_convolutions, bailey_welch_schedule\n\nN = 22 # Number of patients\nT = 20 # Number of intervals\nd = 5 # Length of each interval\nmax_s = 20 # Maximum service time\nq = 0.20 # Probability of a scheduled patient not showing up\nw = 0.1 # Weight for the waiting time in objective function\nl = 10\nnum_schedules = 300000 # Number of schedules to sample\n\n# Create service time distribution\ndef generate_weighted_list(max_s, l, i):\n    \"\"\"\n    Generates a service time probability distribution using optimization.\n\n    This function creates a discrete probability distribution over T possible\n    service times (from 1 to T). It uses optimization (SLSQP) to find a\n    distribution whose weighted average service time is as close as possible\n    to a target value 'l', subject to the constraint that the probabilities\n    sum to 1 and each probability is between 0 and 1.\n\n    After finding the distribution, it sorts the probabilities: the first 'i'\n    probabilities (corresponding to service times 1 to i) are sorted in\n    ascending order, and the remaining probabilities (service times i+1 to T)\n    are sorted in descending order.\n\n    Note:\n        - This function relies on a globally defined integer 'T', representing\n          the maximum service time considered (or number of probability bins).\n        - The parameter 'max_s' is accepted but not used directly within this\n          function's optimization or sorting logic as shown. It might be\n          related to how 'T' is determined externally.\n        - Requires NumPy and SciPy libraries (specifically scipy.optimize.minimize).\n\n    Args:\n        max_s (any): Maximum service time parameter (currently unused in the\n                     provided function body's core logic).\n        l (float): The target weighted average service time for the distribution.\n        i (int): The index determining the sorting split point. Probabilities\n                 for service times 1 to 'i' are sorted ascendingly, and\n                 probabilities for service times 'i+1' to 'T' are sorted\n                 descendingly. Must be between 1 and T-1 for meaningful sorting.\n\n    Returns:\n        numpy.ndarray: An array of size T+1. The first element (index 0) is 0.\n                       Elements from index 1 to T represent the calculated\n                       and sorted probability distribution, summing to 1.\n                       Returns None if optimization fails.\n    \"\"\"\n    # Initialize an array of T+1 values, starting with zero\n    # Index 0 is unused for probability, indices 1 to T hold the distribution\n    values = np.zeros(l + 1)\n\n    # --- Inner helper function for optimization ---\n    def objective(x):\n        \"\"\"Objective function: Squared difference between weighted average and target l.\"\"\"\n        # Calculate weighted average: sum(index * probability) / sum(probability)\n        # Since sum(probability) is constrained to 1, it simplifies.\n        weighted_avg = np.dot(np.arange(1, l + 1), x) # Corresponds to sum(k * P(ServiceTime=k))\n        return (weighted_avg - l) ** 2\n\n    # --- Constraints for optimization ---\n    # Constraint 1: The sum of the probabilities (x[0] to x[T-1]) must be 1\n    constraints = ({\n        'type': 'eq',\n        'fun': lambda x: np.sum(x) - 1\n    })\n\n    # Bounds: Each probability value x[k] must be between 0 and 1\n    # Creates a list of T tuples, e.g., [(0, 1), (0, 1), ..., (0, 1)]\n    bounds = [(0, 1)] * l\n\n    # Initial guess: Use Dirichlet distribution to get a random distribution that sums to 1\n    # Provides a starting point for the optimizer. np.ones(T) gives equal weights initially.\n    initial_guess = np.random.dirichlet(np.ones(l))\n\n    # --- Perform Optimization ---\n    # Minimize the objective function subject to the sum and bounds constraints\n    # using the Sequential Least Squares Programming (SLSQP) method.\n    result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n\n    # Check if optimization was successful\n    if not result.success:\n        print(f\"Warning: Optimization failed! Message: {result.message}\")\n        # Handle failure case, e.g., return None or raise an error\n        return None # Or potentially return a default distribution\n\n    # Assign the optimized probabilities (result.x) to the correct slice of the values array\n    # result.x contains the T probabilities for service times 1 to T.\n    values[1:] = result.x\n\n    # --- Reorder the values based on the index 'i' ---\n    # Ensure 'i' is within a valid range for slicing and sorting\n    if not (0 &lt; i &lt; l):\n       print(f\"Warning: Index 'i' ({i}) is outside the valid range (1 to {T-1}). Sorting might be trivial.\")\n       # Adjust i or handle as an error depending on requirements\n       i = max(1, min(i, l - 1)) # Clamp i to a safe range for demonstration\n\n    # Sort the first 'i' probabilities (indices 1 to i) in ascending order\n    first_part = np.sort(values[1:i+1])\n    # Sort the remaining 'T-i' probabilities (indices i+1 to T) in descending order\n    second_part = np.sort(values[i+1:])[::-1] # [::-1] reverses the sorted array\n\n    # Combine the sorted parts back into the 'values' array\n    values[1:i+1] = first_part\n    values[i+1:] = second_part\n\n    # Return the final array with the sorted probability distribution\n    return values\n\ni = 5  # First 5 highest values in ascending order, rest in descending order\ns = generate_weighted_list(max_s, l, i)\nprint(s)\nprint(\"Sum:\", np.sum(s[1:]))  # This should be 1\nprint(\"Weighted service time:\", np.dot(np.arange(len(s)), s))  # This should be close to l\ninitial_x = bailey_welch_schedule(T, d, N, s)\nprint(f\"Initial schedule: {initial_x}\")\nconvolutions = compute_convolutions(s, N, q)\nfile_path_parameters = f\"datasets/parameters_{N}_{T}_{l}.pkl\"\nwith open(file_path_parameters, 'wb') as f:\n    pickle.dump({\n      'N': N,\n      'T': T,\n      'd': d,\n      'max_s': max_s,\n      'q': q,\n      'w': w,\n      'l': l,\n      'num_schedules': num_schedules,\n      'convolutions': convolutions\n      }, f)\n    print(f\"Data saved successfully to '{file_path_parameters}'\")\n\n[0.00000000e+00 8.55350513e-12 2.62106933e-11 4.38869080e-11\n 6.27233022e-11 7.84102228e-11 1.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00]\nSum: 1.0000000001420395\nWeighted service time: 6.000000000369108\nInitial schedule: [2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 5]\nData saved successfully to 'datasets/parameters_22_20_10.pkl'\n\n\nWe will create a random set of pairs of neighboring schedules from within the neighborhood around schedule [2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 5].\nA neighbor of a schedule x is considered a schedule x’ where single patients have been shifted one interval to the left. Eg: ([2,1,1,2], [1,2,0,3]) are neighbors and ([2,1,1,2], [2,1,3,0]) are not, because [1,2,0,3] - [2,1,1,2] = [-1, 1, -1, 1] and [2,1,3,0] - [2,1,1,2] = [0, 0, 2, -2].\nService times will have a discrete distribution. The probability of a scheduled patient not showing up will be \\(q = 0.2\\).\nThe objective function will be the weighted average of the total waiting time of all patients and overtime. The model will be trained to predict which of the two neighboring schedules has the lowest objective value. The prediction time will be recorded. Then the same schedules will be evaluated by computing the objective value and then ranked.\n\n\n4.4.3 Variables\n\nIndependent Variables: A list of tuples with pairs of neighboring schedules.\nDependent Variables: A list with rankings for each tuple of pairwise schedules. Eg: If the rank for ([2,1,1], [1,1,2]) equals 0 this means that the schedule with index 0 ([2,1,1]) has the lowest objective value.\n\n\n\n4.4.4 Data Collection\nThe data set will be generated using simulation in which random samples will be drawn from the population of all possible schedules. For each sample a random neighboring schedule will be created.\n\n\n4.4.5 Sample Size and Selection\nSample Size: The total population size equals \\({{N + T -1}\\choose{N}} \\approx\\) 244663.0 mln. For this experiment we will be using a relatively small sample of 300000 pairs of schedules.\nSample Selection: The samples will be drawn from a lexicographic order of possible schedules in order to accurately reflect the combinatorial nature of the problem and to ensure unbiased sampling from the entire combinatorial space.\n\n\n4.4.6 Experimental Procedure\nThe experiment involves multiple steps, beginning with data preparation and concluding with model evaluation.The diagram below illustrates the sequence of steps.\n\n\n\n\n\ngraph TD\n    A[\"From population\"] --&gt;|\"Sample\"| B[\"Random subset\"]\n    B --&gt; |Create neighbors| C[\"Features: Schedule pairs\"]\n    C --&gt; |Calculate objectives| D[\"Objective values\"]\n    D --&gt; |Rank objectives| E[\"Labels: Rankings\"]\n    E --&gt; |\"Split dataset\"| F[\"Training set\"]\n    E --&gt; |\"Split dataset\"| G[\"Test set\"]\n    F --&gt; |\"Train\"| H[\"Model\"]\n    H[\"Model\"] --&gt; |\"Apply\"| G[\"Test set\"]\n    G[\"Test set\"] --&gt; |\"Evaluate\"| I[\"Performance\"]\n\n\n\n\n\n\nStep 1: Create pairs of neighboring schedules. A set of 300000 schedules will be sampled from the neighborhood of the initial schedule. For each schedule a pair of neighbors will be created. The order of the neighbors will be randomly switched to create a more diverse training set. The time taken to sample the schedules and create the neighbors will be recorded.\n\nfrom functions import get_v_star, get_neighborhood\n\ndef sample_neighbors_list(x: list[int], v_star: np.ndarray, all = True) -&gt; (list[int], list[int]):\n    \"\"\"\n    Create a set of pairs of schedules that are from the same neighborhood.\n    \n    Parameters:\n      x (list[int]): A list of integers with |s| = T and sum N.\n      v_star (np.ndarray): Precomputed vectors V* of length T.\n      \n    Returns:\n      tuple(list[int], list[int]): A pair of schedules.\n    \"\"\"\n    T = len(x)\n\n    # Precompute binomial coefficients (weights for random.choices)\n    binom_coeff = [math.comb(T, i) for i in range(1, T)]\n\n    # Choose a random value of i with the corresponding probability\n    i = random.choices(range(1, T), weights=binom_coeff)[0]\n\n    # Instead of generating the full list of combinations, sample one directly\n    j = random.sample(range(T), i)\n    \n    x_p = x.copy()\n    for k in j:\n        x_temp = np.array(x_p) + v_star[k]\n        x_temp = x_temp.astype(int)\n        if np.all(x_temp &gt;= 0):\n            x_p = x_temp.astype(int).tolist()\n    if all:\n        return x, x_p\n    else:    \n        return x_p\n\nstart = time.time()\nv_star = get_v_star(T)\n# Sample a set of schedules from the neighborhood of the initial schedule\nneighbors_selection = [sample_neighbors_list(initial_x, v_star, all = False) for i in range(num_schedules)] # This can be done in parallel to improve speed\nprint(len(neighbors_selection))\nend = time.time()\n# For the sampled schedules, create the neighbors\nneighbors_list = [sample_neighbors_list(schedule, v_star) for schedule in neighbors_selection]\n# Randomly switch the order of the neighbors\nneighbors_list = [neighbor if random.random() &lt; 0.5 else neighbor[::-1] for neighbor in neighbors_list]\nend = time.time()\nh = random.choices(range(num_schedules), k=7)\nprint(f\"Sampled schedules: {h}\")\nfor i in h:\n    original_schedule = neighbors_list[i][0]\n    neighbor_schedule = neighbors_list[i][1]\n    difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]\n    print(f\"Neighbors\\n{original_schedule}\\n{neighbor_schedule}\\n{difference}\")\ntraining_set_feat_time = end - start\nprint(f\"\\nProcessing time: {training_set_feat_time} seconds\\n\")\n\n300000\nSampled schedules: [297709, 111924, 88811, 264090, 4945, 156347, 211174]\nNeighbors\n[2, 0, 2, 1, 0, 1, 1, 1, 1, 0, 2, 0, 1, 2, 0, 1, 0, 0, 3, 4]\n[2, 0, 2, 0, 1, 1, 0, 1, 1, 1, 2, 0, 0, 2, 0, 1, 1, 0, 2, 5]\n[0, 0, 0, 1, -1, 0, 1, 0, 0, -1, 0, 0, 1, 0, 0, 0, -1, 0, 1, -1]\nNeighbors\n[1, 2, 0, 1, 0, 0, 2, 1, 0, 2, 1, 0, 1, 0, 2, 0, 1, 1, 0, 7]\n[2, 1, 0, 2, 0, 0, 2, 1, 0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 0, 6]\n[-1, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 1]\nNeighbors\n[4, 1, 0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 0, 2, 2, 0, 0, 1, 1, 4]\n[3, 1, 1, 0, 1, 0, 1, 1, 2, 0, 1, 1, 0, 2, 1, 1, 0, 0, 2, 4]\n[1, 0, -1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 1, -1, 0]\nNeighbors\n[1, 2, 1, 0, 1, 0, 2, 0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0, 1, 6]\n[1, 1, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 2, 0, 0, 7]\n[0, 1, -1, 0, 0, 0, 1, -1, 1, -1, 1, 0, 0, 0, 0, 0, -1, 0, 1, -1]\nNeighbors\n[2, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 2, 0, 1, 2, 4]\n[3, 0, 1, 2, 0, 0, 1, 2, 1, 0, 1, 1, 1, 0, 1, 2, 0, 0, 2, 4]\n[-1, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, 0]\nNeighbors\n[2, 1, 0, 2, 0, 1, 0, 2, 1, 1, 0, 0, 1, 2, 2, 0, 1, 0, 0, 6]\n[2, 1, 0, 2, 0, 1, 0, 1, 2, 0, 1, 0, 1, 1, 2, 1, 0, 1, 0, 6]\n[0, 0, 0, 0, 0, 0, 0, 1, -1, 1, -1, 0, 0, 1, 0, -1, 1, -1, 0, 0]\nNeighbors\n[2, 2, 0, 2, 0, 0, 2, 1, 0, 1, 2, 0, 0, 1, 1, 2, 0, 0, 1, 5]\n[3, 1, 0, 2, 0, 0, 2, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 5]\n[-1, 1, 0, 0, 0, 0, 0, -1, 0, 0, 1, -1, 0, 0, 0, 1, -1, 0, 1, 0]\n\nProcessing time: 56.18493580818176 seconds\n\n\n\nStep 2: For each schedule in each pair calculate the objective. For each pair save the index of the schedule that has the lowest objective value.\n\nfrom functions import calculate_objective_serv_time_lookup\n\nobjectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[0], d, convolutions)]\n]\nstart = time.time()\nobjectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[1], d, convolutions)]\n]\nend = time.time()\ntraining_set_lab_time = end - start\nobjectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]\nrankings = np.argmin(objectives, axis=1).tolist()\nfor i in range(5):\n    print(f\"Objectives: {objectives[i]}, Ranking: {rankings[i]}\")\n\nprint(f\"\\nProcessing time: {training_set_lab_time} seconds\\n\")\n\n# Step 1: Flatten the objectives into a 1D array\nflattened_data = [value for sublist in objectives for value in sublist]\n\n# Step 2: Find the index of the minimum value\nmin_index = np.argmin(flattened_data)\n\n# Step 3: Convert that index back to the original 2D structure\nrow_index = min_index // 2  # Assuming each inner list has 2 values\ncol_index = min_index % 2\n\nprint(f\"The minimum objective value is at index [{row_index}][{col_index}].\\nThis is schedule: {neighbors_list[row_index][col_index]} with objective value {objectives[row_index][col_index]}.\")\n\nfile_path_best_schedule = f\"datasets/best_schedule_{N}_{T}_{l}.pkl\"\nwith open(file_path_best_schedule, 'wb') as f:\n    pickle.dump({'best_schedule':neighbors_list[row_index][col_index], 'objective': objectives[row_index][col_index]}, f)\n    print(f\"Data saved successfully to '{file_path_best_schedule}'\")\n\nprint(f\"\\nAverage ranking: {np.mean(rankings)}\\n\")\n\n# Saving neighbors_list and objectives to a pickle file\nfile_path_neighbors = f\"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl\"\nwith open(file_path_neighbors, 'wb') as f:\n    pickle.dump({'neighbors_list': neighbors_list, 'objectives': objectives, 'rankings': rankings}, f)\n    print(f\"Data saved successfully to '{file_path_neighbors}'\")\n\nObjectives: [41.192163397073834, 33.91792953004357], Ranking: 1\nObjectives: [30.015257826244635, 34.82531832638152], Ranking: 0\nObjectives: [28.970935329939742, 31.69520476993224], Ranking: 0\nObjectives: [29.423810859939547, 32.22552602133364], Ranking: 0\nObjectives: [28.886313566007626, 31.062427422746687], Ranking: 0\n\nProcessing time: 94.84818005561829 seconds\n\nThe minimum objective value is at index [234129][1].\nThis is schedule: [2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3] with objective value 23.723087354309918.\nData saved successfully to 'datasets/best_schedule_22_20_10.pkl'\n\nAverage ranking: 0.49960666666666664\n\nData saved successfully to 'datasets/neighbors_and_objectives_22_20_10.pkl'\n\n\nStep 3: Create training and test sets.\n\n# Prepare the dataset\nX = []\nfor neighbors in neighbors_list:\n    X.append(neighbors[0] + neighbors[1])\n\nX = np.array(X)\ny = np.array(rankings)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 4: Train the XGBoost model.\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B[Initialize StratifiedKFold]\n    B --&gt; C[Initialize XGBClassifier]\n    C --&gt; D[Set results as empty list]\n    D --&gt; E[Loop through each split of cv split]\n    E --&gt; F[Get train and test indices]\n    F --&gt; G[Split X and y into X_train, X_test, y_train, y_test]\n    G --&gt; H[Clone the classifier]\n    H --&gt; I[Call fit_and_score function]\n    I --&gt; J[Fit the estimator]\n    J --&gt; K[Score on training set]\n    J --&gt; L[Score on test set]\n    K --&gt; M[Return estimator, train_score, test_score]\n    L --&gt; M\n    M --&gt; N[Append the results]\n    N --&gt; E\n    E --&gt; O[Loop ends]\n    O --&gt; P[Print results]\n    P --&gt; Q[End]\n\n\n\n\n\n\n\nclass CustomCallback(TrainingCallback):\n    def __init__(self, period=10):\n        self.period = period\n\n    def after_iteration(self, model, epoch, evals_log):\n        if (epoch + 1) % self.period == 0:\n            print(f\"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}\")\n        return False\n    \ndef fit_and_score(estimator, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0\n    )\n\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return estimator, train_score, test_score\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n\n# Initialize the XGBClassifier without early stopping here\n# Load the best trial parameters from a JSON file.\nwith open(\"model_params.json\", \"r\") as f:\n    model_params = json.load(f)\n    \n# Initialize the EarlyStopping callback with validation dataset\nearly_stop = xgb.callback.EarlyStopping(\n    rounds=10, metric_name='logloss', data_name='validation_0', save_best=True\n)\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n    early_stopping_rounds=9,\n    #callbacks=[CustomCallback(period=50), early_stop],\n    callbacks=[CustomCallback(period=50)],\n)\nprint(\"Params: \")\nfor key, value in model_params.items():\n    print(f\" {key}: {value}\")\n\nstart = time.time()\nresults = []\n\nfor train_idx, test_idx in cv.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    est, train_score, test_score = fit_and_score(\n        clone(clf), X_train, X_test, y_train, y_test\n    )\n    results.append((est, train_score, test_score))\nend = time.time()\ntraining_time = end - start\nprint(f\"\\nTraining time: {training_time} seconds\\n\")\n\nParams: \n max_depth: 6\n min_child_weight: 1\n gamma: 0.1\n subsample: 0.8\n colsample_bytree: 0.8\n learning_rate: 0.1\n n_estimators: 100\nEpoch 49, Evaluation log: 0.330827276971781\nEpoch 99, Evaluation log: 0.241263629116538\nEpoch 49, Evaluation log: 0.3329712547330962\nEpoch 99, Evaluation log: 0.24284057029957865\nEpoch 49, Evaluation log: 0.3354860251456111\nEpoch 99, Evaluation log: 0.24594283219885352\nEpoch 49, Evaluation log: 0.33087436630042893\nEpoch 99, Evaluation log: 0.24281658210926543\nEpoch 49, Evaluation log: 0.3315848726167266\nEpoch 99, Evaluation log: 0.23840238815734482\n\nTraining time: 8.271502256393433 seconds\n\n\n\nStep 5: To evaluate the performance of the XGBoost ranking model, we will use Stratified K-Fold Cross-Validation with 5 splits, ensuring each fold maintains the same class distribution as the original dataset. Using StratifiedKFold(n_splits=5, shuffle=True, random_state=94), the dataset will be divided into five folds. In each iteration, the model will be trained on four folds and evaluated on the remaining fold. A custom callback, CustomCallback(period=10), will print the evaluation log every 10 epochs.\nThe fit_and_score function will fit the model and score it on both the training and test sets, storing the results for each fold. This provides insight into the model’s performance across different subsets of the data, helps in understanding how well the model generalizes to unseen data and identifies potential overfitting or underfitting issues. The overall processing time for the cross-validation will also be recorded.\n\n# Print results\nfor i, (est, train_score, test_score) in enumerate(results):\n    print(f\"Fold {i+1} - Train Score (Accuracy): {train_score:.4f}, Test Score (Accuracy): {test_score:.4f}\")\n\nFold 1 - Train Score (Accuracy): 0.9298, Test Score (Accuracy): 0.9292\nFold 2 - Train Score (Accuracy): 0.9287, Test Score (Accuracy): 0.9283\nFold 3 - Train Score (Accuracy): 0.9278, Test Score (Accuracy): 0.9253\nFold 4 - Train Score (Accuracy): 0.9296, Test Score (Accuracy): 0.9288\nFold 5 - Train Score (Accuracy): 0.9316, Test Score (Accuracy): 0.9308\n\n\nTraining the model on the entire dataset provides a final model that has learned from all available data. Recording the training time helps in understanding the computational efficiency and scalability of the model with the given hyperparameters.\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nclf.save_model('models/classifier_large_instance.json')\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\\n\")\n\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\nTraining accuracy: 93.04%\n\n\nTraining time: 1.2039070129394531 seconds\n\n\n\n\n\n4.4.7 Validation\nGenerating test schedules and calculating their objectives and rankings allows us to create a new dataset for evaluating the model’s performance on unseen data.\n\nnum_test_schedules = 1000\n\n#test_schedules = random_combination_with_replacement(T, N, num_test_schedules)\ntest_schedules = [sample_neighbors_list(initial_x, v_star, all = False) for i in range(num_test_schedules)]\n\ntest_neighbors = [sample_neighbors_list(test_schedule, v_star) for test_schedule in test_schedules] # This can be done in parellel to improve speed\n\nprint(f\"Sampled: {len(test_schedules)} schedules\\n\")\n\ntest_objectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[0], d, convolutions)]\n]\n# Start time measurement for the evaluation\nstart = time.time()\ntest_objectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[1], d, convolutions)]\n]\ntest_rankings = [0 if test_obj &lt; test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]\nend = time.time()\nevaluation_time = end - start\n\n# Combine the objectives for each pair for later processing\ntest_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]\n\nprint(f\"\\nEvaluation time: {evaluation_time} seconds\\n\")\n\nfor i in range(6):\n    print(f\"Neighbors: {test_neighbors[i]},\\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\\n\")\n\nSampled: 1000 schedules\n\n\nEvaluation time: 0.3122389316558838 seconds\n\nNeighbors: ([2, 2, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 2, 0, 1, 0, 2, 4], [1, 2, 2, 0, 0, 1, 0, 2, 0, 2, 1, 1, 0, 0, 2, 1, 0, 1, 2, 4]),\nObjectives: [29.701457292464866, 31.715569768348942], Ranking: 0\n\nNeighbors: ([1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 2, 0, 2, 0, 1, 0, 6], [1, 0, 2, 1, 0, 1, 2, 1, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0, 0, 7]),\nObjectives: [32.852065799807626, 41.51282904718643], Ranking: 0\n\nNeighbors: ([2, 2, 1, 0, 2, 0, 1, 1, 0, 2, 1, 0, 1, 0, 1, 1, 1, 0, 1, 5], [3, 1, 1, 1, 1, 0, 1, 1, 0, 3, 0, 0, 1, 0, 1, 2, 0, 0, 2, 4]),\nObjectives: [30.925256669858793, 31.973426712512044], Ranking: 0\n\nNeighbors: ([2, 1, 1, 2, 1, 0, 0, 2, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 0, 5], [1, 2, 1, 2, 0, 0, 0, 2, 0, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 5]),\nObjectives: [29.217772508491286, 31.270721985609374], Ranking: 0\n\nNeighbors: ([2, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 2, 1, 1, 0, 1, 1, 0, 5], [1, 2, 2, 0, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 2, 0, 0, 6]),\nObjectives: [30.389467121853016, 34.18981552935611], Ranking: 0\n\nNeighbors: ([2, 2, 0, 2, 0, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 5], [2, 2, 0, 2, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1, 1, 2, 0, 0, 0, 5]),\nObjectives: [30.236820391496245, 31.378353603885415], Ranking: 0\n\n\n\nMaking predictions on new data and comparing them to the actual rankings provides an evaluation of the model’s performance in practical applications. Recording the prediction time helps in understanding the model’s efficiency during inference.\n\ninput_X = test_neighbors\nX_new = []\nfor test_neighbor in input_X:\n    X_new.append(test_neighbor[0] + test_neighbor[1])\n    \n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.004350185394287109 seconds\n\ntest_rankings = [0 0 0 0 0 0], \ny_pred = [0 0 0 0 0 0], \ny_pred_proba = \n[[0.8908624  0.10913759]\n [0.987966   0.012034  ]\n [0.7802712  0.2197288 ]\n [0.63890153 0.36109847]\n [0.9876534  0.0123466 ]\n [0.5742072  0.42579284]]\n\n\nCalculating the ambiguousness of the predicted probabilities helps in understanding the model’s confidence in its predictions. High ambiguousness indicates uncertain predictions, while low ambiguousness indicates confident predictions.\nAmbiguousness is calculated using the formula for entropy:\n\\[\nH(X) = - \\sum_{i} p(x_i) \\log_b p(x_i)\n\\]\nWhere in our case:\n\n\\(H(X)\\) is the ambiguousness of the random variable \\(X\\) - the set of probability scores for the predicted rankings,\n\\(p(x_i)\\) is probability score \\(x_i\\),\n\\(\\log_b\\) is the logarithm with base \\(b\\) (here \\(\\log_2\\) as we have two predicted values),\nThe sum is taken over all possible outcomes of \\(X\\).\n\nCalculating cumulative error rate and cumulative accuracy helps in understanding how the model’s performance evolves over the dataset.\nVisualizing the relationship between ambiguousness and error provides insights into how uncertainty in the model’s predictions correlates with its accuracy. This can help in identifying patterns and understanding the conditions under which the model performs well or poorly.\n\nfrom functions import calculate_ambiguousness\n\nerrors = np.abs(y_pred - np.array(test_rankings))\n\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9)))\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()\n\n                                                \n\n\n\n\n4.4.8 Hyperparameter Optimization\nIn the initial model the choice of hyperparameters was based on default values, examples from demo’s or trial and error. To improve the model’s performance, we applied a hyperparameter optimization technique to find the best set of hyperparameters. We used a grid search with cross-validation to find the optimal hyperparameters for the XGBoost model. The grid search was performed over a predefined set of hyperparameters, and the best hyperparameters were selected based on the model’s performance on the validation set. The best hyperparameters were then used to train the final model.\n\nfrom functions import compare_json\n\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n    \ndifferences = compare_json(model_params, best_trial_params)\n\nparams_tbl = pd.DataFrame(differences)\nparams_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)\nprint(params_tbl)\n\n                      max_depth     gamma  subsample  colsample_bytree  \\\nbase parameters               6  0.100000   0.800000          0.800000   \noptimized parameters          5  0.304548   0.781029          0.922528   \n\n                      learning_rate  n_estimators  \nbase parameters            0.100000           100  \noptimized parameters       0.239488           490  \n\n\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\n# Load the best trial parameters from a JSON file.\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\")\n\n\nTraining time: 4.910453796386719 seconds\n\nTraining accuracy: 97.38%\n\n\n\n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.0065288543701171875 seconds\n\ntest_rankings = [0 0 0 0 0 0], \ny_pred = [0 0 0 0 0 0], \ny_pred_proba = \n[[9.9449468e-01 5.5053122e-03]\n [9.9999994e-01 5.4009387e-08]\n [9.4748026e-01 5.2519754e-02]\n [9.7302395e-01 2.6976075e-02]\n [9.9998993e-01 1.0066873e-05]\n [8.9615613e-01 1.0384388e-01]]\n\n\n\nerrors = np.abs(y_pred - np.array(test_rankings))\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors, \"Schedules\": test_neighbors, \"Objectives\": test_objectives}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n\n\n\n\n\n\n\nAmbiguousness\nError\nSchedules\nObjectives\nCumulative error rate\nCumulative accuracy\n\n\n\n\n296\n3.640491e-08\n0\n([1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 0, 1, 0, 2, 1,...\n[30.661641910651696, 39.30636469198758]\n0.0\n1.0\n\n\n885\n5.872603e-08\n0\n([1, 1, 1, 1, 1, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1,...\n[30.088477020892046, 38.69555704455301]\n0.0\n1.0\n\n\n127\n8.667784e-08\n0\n([3, 1, 1, 0, 1, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1,...\n[29.628436760540822, 35.263428221789745]\n0.0\n1.0\n\n\n612\n1.884952e-07\n0\n([1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 0, 2, 0, 2,...\n[33.32991990572627, 42.335310966341225]\n0.0\n1.0\n\n\n430\n2.506813e-07\n0\n([1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1,...\n[29.760357965079237, 37.65524385585658]\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9),\n                    customdata=df[[\"Schedules\", \"Objectives\"]],\n                    hovertemplate=\n                        \"Ambiguousness: %{x} &lt;br&gt;\" +\n                        \"Error: %{y} &lt;br&gt;\" +\n                        \"Schedules: %{customdata[0][0]} / %{customdata[0][1]} &lt;br&gt;\" +\n                        \"Objectives: %{customdata[1]} &lt;br&gt;\"\n                    ))\n                  \nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#results",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#results",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.5 Results",
    "text": "4.5 Results\nWe wanted to test whether an XGBoost classification model could be used to assess and rank the quality of pairs of schedules. For performance benchmarking we use the conventional calculation method utilizing Lindley recursions.\nWe trained the XGBoost ranking model with a limited set of features (schedules) and labels (objectives). The total number of possible schedules is approximately 244663.0 million. For training and evaluation, we sampled 600000 schedules and corresponding neighbors. Generating the feature and label set took a total of 151.0331 seconds, with the calculation of objective values accounting for 94.8482 seconds.\nThe model demonstrates strong and consistent performance with high accuracies both for training, testing and validation (96.1%) with good generalization and stability. Total training time for the final model was 4.9105 seconds. The evaluation of 1000 test schedules took 0.0065 seconds for the the XGBoost model and 0.3122 for the conventional method, which is an improvement of 47X.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#discussion",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#discussion",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.6 Discussion",
    "text": "4.6 Discussion\n\ntraining_time = round(modeling_time, 4)\nconventional_time = round(evaluation_time, 4)\nxgboost_time = round(prediction_time, 4)\n\n# Define time values for plotting\ntime_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds\n\n# Calculate evaluations for method 1\nmethod1_evaluations = np.where(time_values &gt;= training_time, (time_values - training_time) / xgboost_time * 1000, 0)\n\n# Calculate evaluations for method 2\nmethod2_evaluations = time_values / conventional_time * 1000\n\n# Create line chart\nfig = go.Figure()\n\n# Add method 1 trace\nfig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))\n\n# Add method 2 trace\nfig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))\n\n# Update layout\nfig.update_layout(\n    title=\"Speed comparison between XGBoost ranking model and conventional method\",\n    xaxis_title=\"Time (seconds)\",\n    yaxis_title=\"Number of Evaluations\",\n    legend_title=\"Methods\",\n    template=\"plotly_white\"\n)\n\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#timeline",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#timeline",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.7 Timeline",
    "text": "4.7 Timeline\nThis experiment was started on 26-04-2025. The expected completion date is 26-04-2025.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#references",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#references",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.8 References",
    "text": "4.8 References",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html",
    "href": "local-search-ranking-large.html",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "",
    "text": "5.1 Objective\nTest the working and performance of a previously trained XGBoost Ranking model in a local search application.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#background",
    "href": "local-search-ranking-large.html#background",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.2 Background",
    "text": "5.2 Background\nIn previous experiments, we trained an XGBoost Classifier model to predict the objective values of neighboring schedules. In this experiment, we will use the trained models to perform a local search to find the best schedule.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#hypothesis",
    "href": "local-search-ranking-large.html#hypothesis",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.3 Hypothesis",
    "text": "5.3 Hypothesis\nThe XGBoost Classifier model will be able to efficiently guide the local search algorithm to find a schedule with a lower objective value than the initial schedule.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#methodology",
    "href": "local-search-ranking-large.html#methodology",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.4 Methodology",
    "text": "5.4 Methodology\n\n5.4.1 Tools and Materials\n\nimport numpy as np\nimport json\nimport time\nfrom itertools import chain, combinations\nimport sys\nfrom math import comb  # Python 3.8 and later\nimport xgboost as xgb\nimport pickle\nfrom typing import List, Tuple, Dict, Iterable, TypeVar, Union, Any, Optional, Literal\n\nimport logging\nimport sys # Needed for StreamHandler in order to enable explicit console output\n\n# Logging configuration\nlog_level = logging.DEBUG # DEBUG or INFO\nlog_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n\n# Log to a file instead of to the console:\nlogging.basicConfig(level=log_level, format=log_format, filename='search.log', filemode='w')\n\n# Get a logger instance\nlogger = logging.getLogger(__name__)\n\n\n\n5.4.2 Load Parameters\n\nN = 22 # Number of patients\nT = 20 # Number of intervals\nl = 10 # Target service time length\n\nfile_path_parameters = f\"datasets/parameters_{N}_{T}_{l}.pkl\" # For retrieving saved scheduling parameters\n# Load the data from the pickle file\nwith open(file_path_parameters, 'rb') as f:\n    data_params = pickle.load(f)\n\nN = data_params['N'] # Number of patients\nT = data_params['T'] # Number of intervals\nd = data_params['d'] # Length of each interval\nmax_s = data_params['max_s'] # Maximum service time\nq = data_params['q'] # Probability of a scheduled patient not showing up\nw = data_params['w'] # Weight for the waiting time in objective function\nl = data_params['l']\n  \nnum_schedules = data_params['num_schedules'] # Size of training set\nconvolutions = data_params['convolutions'] # Service time distributions used in training phase adjusted for no-shows\nprint(f\"Parameters loaded: N={N}, T={T}, l={l}, d={d}, max_s={max_s}, q={q}, w={w}, num_schedules={num_schedules}\")\n\nParameters loaded: N=22, T=20, l=10, d=5, max_s=20, q=0.2, w=0.1, num_schedules=300000\n\n\n\n\n5.4.3 Experimental Design\nWe will use the trained XGBoost Classifier model to guide a local search algorithm to find the best schedule. The local search algorithm will start with an initial schedule and iteratively explore the neighborhood of the current schedule to find a better one. As an initial schedule, we will use the schedule with the lowest objective value from the training dataset that was used to train the XGBoost Classifier model.\n\n\n5.4.4 Variables\n\nIndependent Variables:\n\nInitial schedule, trained XGBoost Classifier\n\nDependent Variables:\n\nSpeed, accuracy, and convergence of the local search algorithm.\n\n\n\n\n5.4.5 Data Collection\nWe will use the training dataset to initialize the local search algorithm.\n\n\n5.4.6 Sample Size and Selection\n\n\n5.4.7 Experimental Procedure\n\n\n\n\n\ngraph TD\n                A[Start] --&gt; B(\"Initialize schedule x\");\n                B --&gt; C{\"Iterate through all subsets U of V*\"};\n                C -- \"For each U\" --&gt; D{\"Compute y = x + sum(v in U)\"};\n                D -- \"Check y &gt;= 0\" --&gt; E{\"Compute cost C(y)\"};\n                E --&gt; F{\"Is C(y) &lt; C(x)?\"};\n                F -- \"Yes\" --&gt; G[\"Update x := y\"];\n                G --&gt; C;\n                F -- \"No\" --&gt; H{\"Finished iterating all U?\"};\n                H -- \"Yes\" --&gt; I[\"End: x is optimal schedule\"];\n                H -- \"No\" --&gt; C;\n                D -- \"If y &lt; 0\" --&gt; C;",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#results",
    "href": "local-search-ranking-large.html#results",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.5 Results",
    "text": "5.5 Results\n\n5.5.1 Load the initial best schedule.\nStart with the best solution found so far \\(\\{x^*, C(x^*)\\}\\) from the training set.\n\n# Load the best solution from the training dataset\nfile_path_schedules = f\"datasets/best_schedule_{N}_{T}_{l}.pkl\"\n# Load the data from the pickle file\nwith open(file_path_schedules, 'rb') as f:\n    best_schedule_data = pickle.load(f)\n    \nprint(f\"The data has following keys: {[key for key in best_schedule_data.keys()]}\")\n\nprint(f\"The current best schedule is: {best_schedule_data['best_schedule']} with objective value {best_schedule_data['objective']}.\")\n\n# Set the initial schedule to the best solution from the training dataset\ninitial_schedule = best_schedule_data['best_schedule']\n\nThe data has following keys: ['best_schedule', 'objective']\nThe current best schedule is: [2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3] with objective value 23.723087354309918.\n\n\n\n\n5.5.2 Generate the neighborhood of \\(x^*\\).\n\n5.5.2.1 Define \\(V^*\\) and \\(U_t\\).\nDefine the vectors \\(V^*\\) as follows:\n\\[\n\\left\\{\n\\begin{array}{c}\n\\vec{v_1}, \\\\\n\\vec{v_2}, \\\\\n\\vec{v_3}, \\\\\n\\vdots \\\\\n\\vec{v_{T-1}}, \\\\\n\\vec{v_T} \\\\\n\\end{array}\n\\right\\} =\n\\left\\{\n\\begin{array}{c}\n(-1, 0,...., 0, 1), \\\\\n(1, -1, 0,...., 0), \\\\\n(0, 1, -1,...., 0), \\\\\n\\vdots \\\\\n(0,...., 1, -1, 0), \\\\\n(0,...., 0, 1, -1) \\\\\n\\end{array}\n\\right\\}\n\\]\nDefine \\(U_t\\) as the set of all possible subsets of \\(V^*\\) such that each subset contains exactly \\(t\\) elements, i.e.,\n\\[\nU_t = \\{ S \\subsetneq V^* \\mid |S| = t \\}, \\quad t \\in \\{1, 2, \\dots, T\\}.\n\\]\n\nfrom functions import get_v_star\n\ndef powerset(iterable, size=1):\n    \"powerset([1,2,3], 2) --&gt; (1,2) (1,3) (2,3)\"\n    return [[i for i in item] for item in combinations(iterable, size)]\n  \nx = initial_schedule\n\n# Generate a matrix 'v_star' using the 'get_v_star' function\nv_star = get_v_star(T)\n\n# Generate all possible non-empty subsets (powerset) of the set {0, 1, 2, ..., t-1}\n# 'ids' will be a list of tuples, where each tuple is a subset of indices\nsize = 2\nids = powerset(range(T), size)\nlen(ids)\nids[:T]\n\n[[0, 1],\n [0, 2],\n [0, 3],\n [0, 4],\n [0, 5],\n [0, 6],\n [0, 7],\n [0, 8],\n [0, 9],\n [0, 10],\n [0, 11],\n [0, 12],\n [0, 13],\n [0, 14],\n [0, 15],\n [0, 16],\n [0, 17],\n [0, 18],\n [0, 19],\n [1, 2]]\n\n\n\n\n5.5.2.2 Define the neighborhood of \\(x\\)\nDefine the neighborhood of \\(x\\) as all vectors of the form \\(x + u_{tk}, \\forall \\, u_{tk} \\in U_t\\).\n\nfrom functions import get_neighborhood\ntest_nh = get_neighborhood(x, v_star, ids)\nprint(f\"All neighborhoods with {size} patients switched:\\n x = {np.array(x)}: \\n {test_nh}\")\n\nAll neighborhoods with 2 patients switched:\n x = [2 1 1 2 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 3]: \n [[2 0 1 ... 1 1 4]\n [1 2 0 ... 1 1 4]\n [1 1 2 ... 1 1 4]\n ...\n [2 1 1 ... 1 0 3]\n [2 1 1 ... 0 2 2]\n [2 1 1 ... 2 1 2]]\n\n\n\n\n\n5.5.3 Local search algorithm with prediction\nLoad the pre-trained model and use it for evaluating schedules within a local search algorithm. The search algorithm checks for false positives (prediction improvement = “True”, actual is improvement = “False”) and false negatives (prediction improvement = “False”, actual is improvement = “True”). In both cases the model is updated using the schedules and associated objective values (rankings).\n\n\n\n\n\ngraph TD\n    %% --- Part 1: Initialization & Outer Loop ---\n\n    A[Start: local_search_predict_update] --&gt; B{Inputs: x, w, v_star, clf, params, size, restarts, threshold};\n    B --&gt; C{\"Validate Inputs (clf, x length)\"};\n    C -- Valid --&gt; D[\"Initialize: x_star, T, restart_count=0, t=1\"];\n    C -- Invalid --&gt; Z_Err1[\"Raise ValueError\"];\n    D --&gt; E{\"Calculate Initial cost_star\"};\n    E -- Success --&gt; F{\"Outer Loop: t &lt;= size AND restart_count &lt; restarts?\"};\n    E -- Error --&gt; Z_Err2[\"Return x_star, clf\"];\n\n    %% Connections FROM other parts back to the Outer Loop check (F)\n    Connector_O([From Part 2: Break Inner Loop]) --&gt; F;\n    Connector_CC_Yes([From Part 3: Found Better at Level t]) --&gt; F;\n    Connector_DD([From Part 3: Incremented t]) --&gt; F;\n\n    %% Connections TO other parts\n    F -- No --&gt; Y[\"End: Return x_star, clf\"];\n    F -- Yes --&gt; G[\"Generate Neighborhood (level t)\"];\n    G --&gt; Connector_H([To Part 2: Start Inner Loop]);\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n    %% --- Part 2: Inner Loop - Neighbor Evaluation ---\n\n    Connector_G([From Part 1: Generate Neighborhood]) --&gt; H{\"Inner Loop: For each neighbor\"};\n\n    H -- Next Neighbor --&gt; I{\"Predict with clf: prediction, P(0)\"};\n    I -- Error Predicting --&gt; I_Err[\"Log Error, Assume P=0\"];\n    I_Err --&gt; J;\n    I -- Success --&gt; J{\"Perform Expensive Check? (Pred=1 OR P(0) &lt; threshold)\"};\n    J -- No --&gt; H_Next[Next Neighbor]; %% Skip expensive check\n    J -- Yes --&gt; K{\"Calculate True Cost (Expensive Objective Func)\"};\n    K -- Error --&gt; K_Err[\"Log Error\"];\n    K_Err --&gt; H_Next;\n    K -- Success --&gt; L{\"Is neighbor truly better? (cost_neighbor &lt; cost_star)\"};\n\n    %% Path 1: Improvement Found\n    L -- Yes --&gt; M[\"Update x_star, cost_star, T\"];\n    M --&gt; N[\"Set found_better=True, t=1, restart_count++\"];\n    N --&gt; O[\"Break Inner Loop\"];\n    O --&gt; Connector_F1([To Part 1: Outer Loop Check]); %% Connects back to F\n\n    %% Path 2: No Improvement\n    L -- No --&gt; P{\"Misprediction? (Pred=1 AND Actual=0)\"};\n    P -- No --&gt; Q[\"Log Borderline/Correct Pred=0\"];\n    Q --&gt; H_Next;\n    P -- Yes --&gt; R[\"Log Misprediction\"];\n    R --&gt; Connector_S([To Part 3: Start Retraining]); %% Trigger Retraining\n\n    %% Loop Control\n    H_Next --&gt; H; %% Process next neighbor\n    H -- End of Neighbors --&gt; BB{\"End Inner Loop\"};\n    BB --&gt; Connector_BB([To Part 3: Check Level Result]);\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n    %% --- Part 3: Retraining Sub-routine & Loop Control ---\n\n    %% Retraining Sub-routine Start\n    Connector_R([From Part 2: Misprediction Detected]) --&gt; S[\"Start Retraining Sub-routine\"];\n    subgraph Retraining Sub-routine\n        direction TB\n        S --&gt; T{\"Calculate True Costs for ALL neighbors at level t\"};\n        T --&gt; U{\"Opportunistic Better Found during Cost Calc?\"};\n        U -- Yes --&gt; V[\"Update x_star, cost_star, T\"];\n        V --&gt; W[\"Set found_better_retrain=True\"];\n        W --&gt; X[\"Collect Data: Append features/labels for update\"];\n        U -- No --&gt; X;\n        X --&gt; X_Loop{\"More neighbors to process for retraining?\"};\n        X_Loop -- Yes --&gt; T;\n        X_Loop -- No --&gt; Y_Fit{\"Fit clf incrementally\"};\n        Y_Fit -- Error --&gt; Y_FitErr[\"Log Fit Error\"];\n        Y_FitErr --&gt; Z_CheckOpp{\"Check if found_better_retrain?\"};\n        Y_Fit -- Success --&gt; Z_CheckOpp;\n    end\n\n    %% Retraining Outcome\n    Z_CheckOpp -- Yes --&gt; AA[\"Set found_better=True, t=1, restart_count++\"];\n    AA --&gt; Connector_O([To Part 1: Outer Loop Check via Break]); %% Connects back to F via O\n    Z_CheckOpp -- No --&gt; Connector_H_Next([To Part 2: Next Neighbor]); %% Retraining finished, continue inner loop\n\n    %% Inner Loop Finished - Level Control Logic\n    Connector_BB([From Part 2: End Inner Loop]) --&gt; CC{\"Found better solution at level t?\"};\n    CC -- Yes --&gt; Connector_F2([To Part 1: Outer Loop Check]); %% Restart checks from t=1\n    CC -- No --&gt; DD[\"Increment t\"];\n    DD --&gt; Connector_F3([To Part 1: Outer Loop Check]); %% Continue outer loop with next t\n\n\n\n\n\n\n\ndef local_search_predict(\n    x: List[int],\n    w: float,\n    v_star: np.ndarray,\n    clf: xgb.XGBClassifier,\n    obj_func_params: Dict[str, Any],\n    size: int = 2,\n    restarts: int = 3,\n    check_proba_threshold: float = 0.7,\n    retrain_on: Literal['both', 'fp', 'fn', 'none'] = 'fp'\n) -&gt; Tuple[List[int], xgb.XGBClassifier]:\n    \"\"\"\n    Performs local search guided by an XGBClassifier, minimizing expensive\n    objective calls. Verifies prediction=0 if P(class=0) is below threshold.\n    Updates the classifier incrementally when specified mispredictions occur.\n    Uses logging instead of print statements. T is inferred from len(x).\n\n    Args:\n        x (List[int]): Starting point.\n        w (float): Weight for combining objectives.\n        v_star (np.ndarray): Current best overall solution (used for guidance).\n        clf (xgb.XGBClassifier): Pre-trained XGBoost Classifier.\n        obj_func_params (Dict[str, Any]): Parameters for objective function.\n        size (int, optional): Max neighborhood size. Defaults to 2.\n        restarts (int, optional): Max restarts. Defaults to 3.\n        check_proba_threshold (float, optional): Threshold for P(class=0) verification. Defaults to 0.7. # Corrected default in comment\n        retrain_on (Literal['both', 'fp', 'fn', 'none'], optional):\n            Specifies when to trigger retraining based on misprediction type:\n            - 'both': Retrain on False Positives (P=1, A=0) and False Negatives (P=0, A=1).\n            - 'fp': Retrain only on False Positives. (Default) # Corrected default in comment\n            - 'fn': Retrain only on False Negatives.\n            - 'none': Never retrain based on mispredictions.\n            Defaults to 'fp'.\n\n    Returns:\n        Tuple[List[int], xgb.XGBClassifier]: Best solution found and potentially updated classifier.\n    \"\"\"\n    # --- Input Validation ---\n    # Check if clf appears fitted (basic check)\n    if not hasattr(clf, 'classes_') or not hasattr(clf, 'n_features_in_'):\n         logger.warning(\"Classifier 'clf' may not be fitted. Proceeding with caution.\")\n         # Depending on strictness, you might raise an error here instead.\n         # raise ValueError(\"Classifier 'clf' must be fitted before use.\")\n\n    if not x:\n        logger.error(\"Input schedule x cannot be empty (length must be positive).\")\n        raise ValueError(\"Input schedule x cannot be empty (length must be positive).\")\n\n    allowed_retrain_values = {'both', 'fp', 'fn', 'none'}\n    if retrain_on not in allowed_retrain_values:\n        logger.error(\"Invalid value for 'retrain_on': %s. Must be one of %s\", retrain_on, allowed_retrain_values)\n        raise ValueError(f\"Invalid value for 'retrain_on'. Must be one of {allowed_retrain_values}\")\n\n    # --- Initialization ---\n    x_star = list(x) # Work with a copy\n    T = len(x_star) # Infer T from the length - calculated once initially\n    restart_count = 0\n    t = 1 # Start with neighborhood size 1\n\n    # Calculate initial cost\n    try:\n        logger.info(\"Calculating initial cost...\")\n        objectives_star = calculate_objective_serv_time_lookup(x_star, **obj_func_params)\n        cost_star = w * objectives_star[0] + (1 - w) * objectives_star[1]\n        logger.info(\"Initial solution cost: %.4f\", cost_star)\n    except Exception as e:\n        logger.exception(\"Error calculating initial cost: %s\", e)\n        return x_star, clf # Return current best and original classifier on error\n\n    # --- Main Search Loop ---\n    while t &lt;= size and restart_count &lt; restarts:\n        logger.info(\"--- Running local search level t=%d (Restart %d/%d) ---\", t, restart_count + 1, restarts)\n\n        ids_gen_iterable = powerset(range(T), t) # Use current T\n        # Pass x_star (current best) to neighborhood generation\n        neighborhood_iter = get_neighborhood(x_star, v_star, ids_gen_iterable)\n\n        found_better_solution_at_level_t = False\n        neighbors_data_at_level_t: List[Dict[str, Any]] = [] # Store data for potential retraining\n        neighbors_processed_count = 0\n\n        for neighbor_np in neighborhood_iter:\n            neighbors_processed_count += 1\n            neighbor = neighbor_np.tolist() # Convert numpy array to list\n            neighbor_info = {\"schedule\": neighbor, \"cost\": None, \"true_label\": None, \"prediction\": None}\n            neighbors_data_at_level_t.append(neighbor_info) # Add neighbor info early\n\n            # --- Feature Creation ---\n            # Feature is concatenation - ensure this matches how clf was trained\n            feature_pair = x_star + neighbor\n\n            # --- 1. Predict using the CHEAP classifier ---\n            prediction = 0 # Default prediction\n            proba_class_0 = 1.0 # Default probability\n            try:\n                # Reshape feature_pair for XGBoost if needed (expects 2D array)\n                feature_pair_np = np.array(feature_pair).reshape(1, -1)\n                prediction = clf.predict(feature_pair_np)[0]\n                proba = clf.predict_proba(feature_pair_np)[0]\n                # Ensure proba has expected structure (e.g., 2 elements for binary class)\n                if len(proba) &gt; 0:\n                   proba_class_0 = proba[0] # Probability of class 0\n                else:\n                   logger.warning(\"Predict_proba returned unexpected structure: %s. Using default P(0)=1.0\", proba)\n            except Exception as e:\n                logger.warning(\"Error predicting for neighbor %d: %s. Assuming prediction=0.\", neighbors_processed_count, e)\n                # Keep default prediction=0, proba_class_0=1.0\n\n            neighbor_info[\"prediction\"] = prediction # Store prediction\n            logger.debug(\"  Neighbor %d: Predicted=%d (P(0)=%.3f)\", neighbors_processed_count, prediction, proba_class_0)\n\n            # --- 2. Decide whether to perform expensive check ---\n            perform_expensive_check = False\n            check_reason = \"\"\n\n            if prediction == 1:\n                perform_expensive_check = True\n                check_reason = \"Predicted 1\"\n            elif proba_class_0 &lt; check_proba_threshold:\n                perform_expensive_check = True\n                check_reason = f\"Borderline P(0) &lt; {check_proba_threshold:.3f}\"\n            else: # prediction == 0 and proba_class_0 &gt;= threshold\n                logger.debug(\"  -&gt; Skipping objective function call (Confident P=0).\")\n\n            # --- 3. Perform EXPENSIVE check if needed ---\n            if perform_expensive_check:\n                logger.debug(\"  -&gt; Verifying (%s)...\", check_reason)\n                try:\n                    objectives_neighbor = calculate_objective_serv_time_lookup(neighbor, **obj_func_params)\n                    cost_neighbor = w * objectives_neighbor[0] + (1 - w) * objectives_neighbor[1]\n                    is_truly_better = cost_neighbor &lt; cost_star\n                    true_label = 1 if is_truly_better else 0\n\n                    # Store results in neighbor_info\n                    neighbor_info[\"cost\"] = cost_neighbor\n                    neighbor_info[\"true_label\"] = true_label\n\n                    logger.debug(\"      True Cost=%.4f (Current Best=%.4f) -&gt; Actual Better=%s\",\n                                 cost_neighbor, cost_star, is_truly_better)\n\n                    # --- 4. Check for Misprediction and Trigger Retraining (Conditional) ---\n                    misprediction = (prediction != true_label)\n                    trigger_retraining = False\n                    opportunistic_update_occurred = False # Reset for this neighbor check\n\n                    if misprediction and retrain_on != 'none':\n                        misprediction_type = \"\"\n                        should_retrain_this_type = False\n\n                        if prediction == 1 and not is_truly_better: # False Positive (P=1, A=0)\n                            misprediction_type = \"False Positive (P=1, A=0)\"\n                            should_retrain_this_type = retrain_on in ['both', 'fp']\n                        elif prediction == 0 and is_truly_better: # False Negative (P=0, A=1)\n                            misprediction_type = \"False Negative (P=0, A=1)\"\n                            should_retrain_this_type = retrain_on in ['both', 'fn']\n\n                        if should_retrain_this_type:\n                            logger.warning(\"      Misprediction! (%s). Triggering retraining process based on 'retrain_on=%s'.\",\n                                           misprediction_type, retrain_on)\n                            trigger_retraining = True\n                        elif misprediction_type: # Misprediction occurred but not the type we retrain on\n                             logger.info(\"      Misprediction occurred (%s), but retraining is disabled for this type ('retrain_on=%s').\",\n                                         misprediction_type, retrain_on)\n\n                    # --- Retraining Sub-routine (if triggered) ---\n                    if trigger_retraining:\n                        features_for_update: List[List[int]] = []\n                        labels_for_update: List[int] = []\n                        best_opportunistic_neighbor = None\n                        best_opportunistic_cost = cost_star # Initialize with current best cost\n\n                        logger.info(\"      Calculating true costs for %d neighbors at level %d for retraining...\",\n                                    len(neighbors_data_at_level_t), t)\n\n                        for n_idx, n_info in enumerate(neighbors_data_at_level_t):\n                            n_schedule = n_info[\"schedule\"]\n                            n_cost = n_info[\"cost\"]\n                            n_true_label = n_info[\"true_label\"]\n\n                            # Calculate cost if not already done (e.g., for neighbors skipped earlier)\n                            if n_cost is None or n_true_label is None:\n                                try:\n                                    logger.debug(\"          Calculating missing cost for neighbor %d...\", n_idx+1)\n                                    n_objectives = calculate_objective_serv_time_lookup(n_schedule, **obj_func_params)\n                                    n_cost = w * n_objectives[0] + (1 - w) * n_objectives[1]\n                                    n_is_better = n_cost &lt; cost_star\n                                    n_true_label = 1 if n_is_better else 0\n                                    n_info[\"cost\"] = n_cost # Update info cache\n                                    n_info[\"true_label\"] = n_true_label\n                                except Exception as e:\n                                    logger.warning(\"          Error calculating cost for neighbor %d (%s) during retraining: %s. Skipping.\",\n                                                   n_idx+1, n_schedule, e)\n                                    continue # Skip this neighbor for training data\n\n                            # Prepare data for fitting\n                            n_feature_pair = x_star + n_schedule # Create feature pair for this neighbor\n                            features_for_update.append(n_feature_pair)\n                            labels_for_update.append(n_true_label)\n                            logger.debug(\"          Neighbor %d: Cost=%.4f, True Label=%d (Used for training)\",\n                                         n_idx+1, n_cost, n_true_label)\n\n                            # Check for opportunistic update (find the best neighbor *among those evaluated*)\n                            if n_true_label == 1 and n_cost &lt; best_opportunistic_cost:\n                                logger.info(\"          Opportunistic Update Candidate! Found/Confirmed better neighbor (%d) during cost calculation.\", n_idx+1)\n                                best_opportunistic_neighbor = list(n_schedule) # Store a copy of the schedule\n                                best_opportunistic_cost = n_cost # Update best cost found *during retraining*\n                                opportunistic_update_occurred = True\n\n\n                        # Perform incremental fit if data was gathered\n                        if features_for_update:\n                            logger.info(\"      Fitting model incrementally with %d data points...\", len(labels_for_update))\n                            try:\n                                X_update = np.array(features_for_update) # Convert list of lists to 2D numpy array\n                                y_update = np.array(labels_for_update)\n\n                                # Ensure clf is fitted before incremental update if it's the first time\n                                # XGBoost's fit with xgb_model handles this correctly.\n                                clf.fit(X_update, y_update, xgb_model=clf.get_booster()) # Pass the existing booster\n                                logger.info(\"      Model update complete.\")\n\n                            except Exception as e:\n                                logger.exception(\"      Error during incremental model update: %s\", e)\n                        else:\n                            logger.warning(\"      No valid data gathered for retraining.\")\n\n                        # If an opportunistic update was found, apply it now\n                        if opportunistic_update_occurred:\n                             logger.info(f\"      Applying opportunistic update. New best: {best_opportunistic_neighbor} with cost = {best_opportunistic_cost:.4f}.\")\n                             x_star = best_opportunistic_neighbor # Use the best one found (already a list)\n                             cost_star = best_opportunistic_cost\n                             T = len(x_star) # Update T as length might have changed\n                        # --- End of Retraining Sub-routine ---\n\n                    # --- 5. Handle Updates & Loop Control ---\n                    # Check if we should update x_star and restart the search level\n                    if opportunistic_update_occurred:\n                        found_better_solution_at_level_t = True # Mark improvement found\n                        t = 1 # Reset level\n                        restart_count += 1\n                        logger.info(\"      Restarting search from t=1 due to opportunistic update during retraining. Restart count: %d\", restart_count)\n                        break # Exit inner loop (for neighbor_np in neighborhood_iter)\n\n                    elif is_truly_better: # True Positive or handled False Negative (update to the current neighbor)\n                        logger.info(f\"      Confirmed better solution (or handled FN). Updating x_star to {neighbor} with cost = {cost_neighbor:.4f}.\")\n                        # CORRECTED: Assign neighbor directly as it's already a list\n                        x_star = neighbor\n                        cost_star = cost_neighbor\n                        T = len(x_star) # Update T as length might have changed\n                        found_better_solution_at_level_t = True\n                        t = 1 # Reset level\n                        restart_count += 1\n                        logger.info(\"      Restarting search from t=1. Restart count: %d\", restart_count)\n                        break # Exit inner loop (for neighbor_np in neighborhood_iter)\n\n                    # else: (Not truly better and no opportunistic update) -&gt; continue to next neighbor implicitly\n\n                except Exception as e:\n                    logger.warning(\"  Error calculating objective or handling result for neighbor %d (%s): %s.\",\n                                   neighbors_processed_count, neighbor, e)\n            # --- End of 'if perform_expensive_check:' ---\n\n        # --- End of neighbor loop (for neighbor_np in neighborhood_iter) ---\n\n        # If we finished the loop for level t without finding a better solution (or breaking early)\n        if not found_better_solution_at_level_t:\n            if neighbors_processed_count &gt; 0:\n                logger.info(\"No improving solution found or confirmed at level t=%d.\", t)\n            else:\n                logger.info(\"No neighbors generated or processed at level t=%d.\", t)\n            t += 1 # Move to the next neighborhood size level\n\n    # --- End of outer while loop ---\n    logger.info(\"Local search finished after %d restarts or reaching max size %d.\", restart_count, size)\n    logger.info(\"Final solution: %s\", x_star)\n    logger.info(\"Final cost: %.4f\", cost_star)\n\n    return x_star, clf\n\n\nfrom functions import calculate_objective_serv_time_lookup\nstart = time.time()\n# Define the path to the saved model\nmodel_path = \"models/classifier_large_instance.json\" # Make sure this path is correct\n\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n)\n\n# Load the model directly from the file path\nclf.load_model(model_path)\n\nintial_objectives = calculate_objective_serv_time_lookup(x, d, convolutions)\ninitial_c_star = w * intial_objectives[0] + (1 - w) * intial_objectives[1]\nx_star = local_search_predict(x, w, v_star, clf, {'d': d, 'convolutions': convolutions}, size=T, restarts=T)[0]\nfinal_objectives = calculate_objective_serv_time_lookup(x_star, d, convolutions)\nfinal_c_star = w * final_objectives[0] + (1 - w) * final_objectives[1]\nend = time.time()\nprint(f\"\\nInitial schedule: {x}, with objective value: {initial_c_star}.\\nFinal schedule: {x_star}, with objective value: {final_c_star}. Search time {end - start:.2f} seconds.\")\n\n\nInitial schedule: [2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3], with objective value: 23.723087354309918.\nFinal schedule: [2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 3], with objective value: 23.700684253723423. Search time 939.88 seconds.\n\n\n\n\n5.5.4 Run the conventional local search algorithm for validation\nWe will run a conventional local search algorithm to evaluate the new method, assessing both the quality of the results and its computational efficiency.\n\nfrom functions import local_search\n# Computing optimal solution with real cost\nprint(f\"Initial schedule: {x}\")\nstart = time.time()\ntest_x = local_search(x, d, convolutions, w, v_star, T, echo=True)\nend = time.time()\n\nInitial schedule: [2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3]\nInitial solution: [2 1 1 2 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 3], cost: 23.723087354309918\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 18\nFound better solution: [2 1 1 2 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 3], cost: 23.700818524462598\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 18\nFound better solution: [2 1 1 2 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 3], cost: 23.700684253723423\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 18\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 155\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 850\nRunning local search with switching 4 patient(s)\nSize of neighborhood: 3333\nRunning local search with switching 5 patient(s)\nSize of neighborhood: 9944\nRunning local search with switching 6 patient(s)\nSize of neighborhood: 23444\nRunning local search with switching 7 patient(s)\nSize of neighborhood: 44760\nRunning local search with switching 8 patient(s)\nSize of neighborhood: 70330\nFound better solution: [2 1 1 2 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2], cost: 23.020346596110915\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 19\nFound better solution: [1 1 1 2 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3], cost: 23.01025194826672\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 19\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 172\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 987\nRunning local search with switching 4 patient(s)\nSize of neighborhood: 4029\nFound better solution: [2 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 3], cost: 22.966437347631064\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 19\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 172\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 987\nRunning local search with switching 4 patient(s)\nSize of neighborhood: 4029\nRunning local search with switching 5 patient(s)\nSize of neighborhood: 12444\nRunning local search with switching 6 patient(s)\nSize of neighborhood: 30192\nRunning local search with switching 7 patient(s)\nSize of neighborhood: 58956\nRunning local search with switching 8 patient(s)\nSize of neighborhood: 94146\nRunning local search with switching 9 patient(s)\nSize of neighborhood: 124202\nRunning local search with switching 10 patient(s)\nSize of neighborhood: 136136\nRunning local search with switching 11 patient(s)\nSize of neighborhood: 124202\nRunning local search with switching 12 patient(s)\nSize of neighborhood: 94146\nRunning local search with switching 13 patient(s)\nSize of neighborhood: 58956\nRunning local search with switching 14 patient(s)\nSize of neighborhood: 30192\nFound better solution: [2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2], cost: 22.62783494379382\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 20\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 190\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 1140\nRunning local search with switching 4 patient(s)\nSize of neighborhood: 4845\nRunning local search with switching 5 patient(s)\nSize of neighborhood: 15504\nRunning local search with switching 6 patient(s)\nSize of neighborhood: 38760\nRunning local search with switching 7 patient(s)\nSize of neighborhood: 77520\nRunning local search with switching 8 patient(s)\nSize of neighborhood: 125970\nRunning local search with switching 9 patient(s)\nSize of neighborhood: 167960\nRunning local search with switching 10 patient(s)\nSize of neighborhood: 184756\nRunning local search with switching 11 patient(s)\nSize of neighborhood: 167960\nRunning local search with switching 12 patient(s)\nSize of neighborhood: 125970\nRunning local search with switching 13 patient(s)\nSize of neighborhood: 77520\nRunning local search with switching 14 patient(s)\nSize of neighborhood: 38760\nRunning local search with switching 15 patient(s)\nSize of neighborhood: 15504\nRunning local search with switching 16 patient(s)\nSize of neighborhood: 4845\nRunning local search with switching 17 patient(s)\nSize of neighborhood: 1140\nRunning local search with switching 18 patient(s)\nSize of neighborhood: 190\nRunning local search with switching 19 patient(s)\nSize of neighborhood: 20\n\n\n\nprint(f\"Initial schedule: {x}\\nFinal schedule: {test_x[0]}\\nDifference: {test_x[0] - x}\\nObjective value: {test_x[1]}. Search time: {end - start:.2f} seconds.\")\ntest_res = calculate_objective_serv_time_lookup(test_x[0], d, convolutions)\n\nInitial schedule: [2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3]\nFinal schedule: [2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2]\nDifference: [ 0  0  0 -1  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0 -1]\nObjective value: 22.62783494379382. Search time: 844.30 seconds.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#discussion",
    "href": "local-search-ranking-large.html#discussion",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.6 Discussion",
    "text": "5.6 Discussion",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#timeline",
    "href": "local-search-ranking-large.html#timeline",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.7 Timeline",
    "text": "5.7 Timeline\nThis experiment was started on 01-04-2025 and concluded on 17-04-2025",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#references",
    "href": "local-search-ranking-large.html#references",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.8 References",
    "text": "5.8 References",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html",
    "href": "combinatorial-bayes-optimization.html",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "",
    "text": "6.1 Objective\nThe objective of this experiment is to evaluate and compare the performance of three distinct Combinatorial Bayesian Optimization (CBO) strategies for a scheduling problem, specifically outpatient appointment scheduling as described by Kaandorp and Koole (2007). We investigate:\nWe aim to determine which strategy is most effective in identifying an optimal or near-optimal schedule, as measured by the objective function value, leveraging dictionary-based embeddings for the high-dimensional combinatorial space (Deshwal et al., 2023).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#objective",
    "href": "combinatorial-bayes-optimization.html#objective",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "",
    "text": "CBO utilizing Expected Improvement (EI) as the acquisition function.\nCBO utilizing Lower Confidence Bound (LCB) as the acquisition function with a fixed kappa (\\(\\kappa\\)) value.\nCBO utilizing Lower Confidence Bound (LCB) as the acquisition function with a dynamically increasing kappa (\\(\\kappa\\)) value.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#background",
    "href": "combinatorial-bayes-optimization.html#background",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.2 Background",
    "text": "6.2 Background\nWe consider an outpatient appointment scheduling problem (Kaandorp and Koole, 2007) where the schedule is represented by a vector \\(\\mathbf{x} = (x_0, x_1, \\ldots, x_{T-1})^T\\). This vector comprises \\(T\\) components, where \\(x_j\\) denotes the non-negative allocation (number of patients) to time slot \\(j\\), for \\(j = 0, \\ldots, T-1\\). A fundamental constraint is that the total allocation across all time slots must equal a fixed constant \\(N\\): \\[\\sum_{j=0}^{T-1} x_j = N\\] We require \\(x_j \\ge 0\\) for all \\(j = 0, \\ldots, T-1\\). Consequently, a valid schedule \\(\\mathbf{x}\\) belongs to the feasible set \\(\\mathcal{F} = \\{ \\mathbf{z} \\in \\mathbb{D}^{T} \\mid \\sum_{j=0}^{T-1} z_j = N, z_j \\ge 0 \\text{ for all } j\\}\\), where \\(\\mathbb{D}\\) is the set of non-negative integers (\\(\\mathbb{Z}_{\\ge 0}\\)).\nKaandorp and Koole (2007) define a neighborhood structure for local search based on perturbation vectors derived from a set of \\(T\\) basis change vectors, \\(v_i \\in \\mathbb{D}^{T}\\), for \\(i = 0, \\ldots, T-1\\). These basis vectors represent elementary shifts of allocation between time slots:\n\n\\(v_0 = (-1, 0, \\ldots, 0, 1)\\) (Shift unit from slot 0 to slot \\(T-1\\))\n\\(v_1 = (1, -1, 0, \\ldots, 0)\\) (Shift unit from slot 1 to slot 0)\n\\(v_i = (0, \\ldots, 0, \\underbrace{1}_{\\text{pos } i-1}, \\underbrace{-1}_{\\text{pos } i}, 0, \\ldots, 0)\\) for \\(i = 2, \\ldots, T-1\\) (Shift unit from slot \\(i\\) to slot \\(i-1\\))\n\nA key property of these basis vectors is that the sum of components for each vector is zero: \\(\\sum_{j=0}^{T-1} v_{ij} = 0\\) for all \\(i=0, \\ldots, T-1\\).\nPerturbations are constructed using a binary selection vector \\(\\mathbf{U} = (u_0, u_1, \\ldots, u_{T-1})\\), where \\(u_i \\in \\{0, 1\\}\\). Each \\(u_i\\) indicates whether the basis change \\(v_i\\) is included in the perturbation. The resulting perturbation vector \\(\\mathbf{r}(\\mathbf{U}) \\in \\mathbb{D}^{T}\\) is the linear combination: \\[\\mathbf{r}(\\mathbf{U}) := \\sum_{i=0}^{T-1} u_i v_i\\]\nSince each \\(v_i\\) sums to zero, any perturbation \\(\\mathbf{r}(\\mathbf{U})\\) also sums to zero: \\(\\sum_{j=0}^{T-1} r_j(\\mathbf{U}) = 0\\). This ensures that applying such a perturbation to a valid schedule \\(\\mathbf{x}\\) preserves the total allocation \\(N\\).\nThe neighborhood of a schedule \\(\\mathbf{x} \\in \\mathcal{F}\\), denoted by \\(\\mathcal{N}(\\mathbf{x})\\), comprises all distinct, feasible schedules \\(\\mathbf{x}'\\) reachable by applying a non-zero perturbation \\(\\mathbf{r}(\\mathbf{U})\\) (Kaandorp and Koole, 2007, use a slightly different but related neighborhood definition based on combinations of these basis vectors).\nThe objective function to be minimized is a weighted sum of Expected Waiting Time (EWT) and Expected Staff Penalty (ESP), as defined by Kaandorp and Koole (2007): \\[C(\\mathbf{x}) = w \\cdot EWT(\\mathbf{x}) + (1-w) \\cdot ESP(\\mathbf{x})\\] Kaandorp and Koole (2007) prove that this objective function is multimodular, which guarantees that a local search algorithm using their defined neighborhood converges to the global optimum.\nHowever, evaluating \\(C(\\mathbf{x})\\) can be computationally expensive, especially for large \\(N\\) and \\(T\\). Furthermore, the search space defined by the binary vectors \\(\\mathbf{U}\\) is high-dimensional (\\(2^T - 2\\) possibilities, excluding \\(\\mathbf{0}\\) and \\(\\mathbf{1}\\)). Bayesian Optimization (BO) is a suitable framework for optimizing such expensive black-box functions. Standard BO methods often struggle with high-dimensional combinatorial spaces. Deshwal et al. (2023) propose a method using dictionary-based embeddings (Hamming Embedding via Dictionaries - HED) to map the high-dimensional binary space of \\(\\mathbf{U}\\) vectors into a lower-dimensional continuous space, where standard Gaussian Process (GP) models can be effectively applied. This experiment applies the HED approach within a BO framework to solve the scheduling problem formulated by Kaandorp and Koole (2007).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#hypothesis",
    "href": "combinatorial-bayes-optimization.html#hypothesis",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.3 Hypothesis",
    "text": "6.3 Hypothesis\nWe hypothesize that:\n\nAll three CBO strategies, leveraging the HED embedding (Deshwal et al., 2023), will be capable of finding schedules superior to the initial schedule derived from the Bailey-Welch method (a common heuristic mentioned by Kaandorp and Koole (2007)).\nCBO strategies employing Lower Confidence Bound (LCB) may exhibit superior performance or faster convergence compared to Expected Improvement (EI), due to the explicit exploration-exploitation trade-off inherent in LCB.\nThe LCB strategy with a dynamically increasing kappa (\\(\\kappa\\)) value will demonstrate a more robust search, potentially leading to improved final solutions compared to a fixed \\(\\kappa\\). However, preliminary results suggest that a fixed \\(\\kappa\\) might perform better for this specific problem instance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#methodology",
    "href": "combinatorial-bayes-optimization.html#methodology",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.4 Methodology",
    "text": "6.4 Methodology\n\n6.4.1 Tools and Materials\n\nProgramming Language: Python 3\nCore Libraries: NumPy, SciPy\nMachine Learning: Scikit-learn (for GaussianProcessRegressor, MinMaxScaler)\nData Structures: Standard Python lists and dictionaries, NumPy arrays.\nImported functions: bailey_welch_schedule, get_v_star, compute_convolutions, calculate_objective_serv_time_lookup (implementing the logic from Kaandorp and Koole (2007), assumed to be in an external functions.py file).\n\n\n\n6.4.2 Experimental Design\nThree distinct Bayesian optimization experiments are conducted, applying the HED embedding approach (Deshwal et al., 2023) to the scheduling problem:\n\nExperiment 1: Expected Improvement (EI)\n\nAcquisition Function: Expected Improvement.\nObjective: Minimize \\(C(\\mathbf{x})\\) by iteratively selecting candidate vectors \\(\\mathbf{U}\\) (via their embeddings) that maximize the EI.\n\nExperiment 2: Lower Confidence Bound (LCB) - Fixed Kappa\n\nAcquisition Function: Lower Confidence Bound.\nObjective: Minimize \\(C(\\mathbf{x})\\) using a fixed kappa (\\(\\kappa\\)) value in the LCB acquisition function applied to the GP model over the embedded space.\n\nExperiment 3: Lower Confidence Bound (LCB) - Increasing Kappa\n\nAcquisition Function: Lower Confidence Bound.\nObjective: Minimize \\(C(\\mathbf{x})\\) using a kappa (\\(\\kappa\\)) value that dynamically increases whenever an improved solution is discovered during the BO process.\n\n\nFor all experiments, Hamming Distance Embedding (HED) with a “diverse random” dictionary construction strategy (Deshwal et al., 2023) is employed to map the binary perturbation vectors \\(\\mathbf{U}\\) to a continuous embedding space. A Gaussian Process (GP) model with Automatic Relevance Determination (ARD) kernels models the (negative) objective function in this embedded space.\n\n\n6.4.3 Variables\n\nIndependent Variables:\n\nType of acquisition function (EI, LCB).\nStrategy for \\(\\kappa\\) in LCB (fixed, increasing).\nThe specific binary perturbation vector \\(\\mathbf{U}\\) selected in each iteration (chosen via optimizing the acquisition function over the embedded space).\n\nDependent Variables:\n\nThe objective function value \\(C(\\mathbf{x}')\\) for the resulting schedule \\(\\mathbf{x}' = \\mathbf{x} + \\mathbf{r}(\\mathbf{U})\\) (calculated using the method from Kaandorp and Koole (2007)).\nThe best objective function value found throughout the optimization process.\n\n\n\n\n6.4.4 Data Collection\nData, comprising evaluated pairs \\((\\mathbf{U}, C(\\mathbf{x}'))\\), is collected iteratively:\n\nAn initial set of N_INITIAL randomly generated \\(\\mathbf{U}\\) vectors is evaluated.\nIn each of the subsequent N_ITERATIONS, BATCH_SIZE_q new \\(\\mathbf{U}\\) vectors are selected by optimizing the respective acquisition function over NUM_CANDIDATES_Acqf randomly generated candidate vectors in the original binary space (evaluated via their embeddings). These newly selected vectors are then evaluated, and the results are added to the dataset.\n\n\n\n6.4.5 Sample Size and Selection\n\nN_INITIAL: 20 (number of initial random evaluations)\nN_ITERATIONS: 25 (number of Bayesian optimization iterations)\nBATCH_SIZE_q: 5 (number of candidates selected and evaluated per iteration)\nNUM_CANDIDATES_Acqf: \\(T \\times 1024 = 20 \\times 1024 = 20480\\) (number of random candidates generated for optimizing the acquisition function in each iteration)\nm: 128 (dimensionality of the HED embedding space, following Deshwal et al. (2023))\n\nThe selection of new points for evaluation is guided by the respective acquisition function (EI or LCB) optimized over the embedded space representation of candidate \\(\\mathbf{U}\\) vectors.\n\n\n6.4.6 Experimental Procedure\n\n6.4.6.1 1. Setup\nImport necessary libraries and configure warning filters.\n# Core Libraries\nimport numpy as np\nimport time\nimport warnings\nfrom scipy.optimize import minimize\nfrom typing import List, Dict, Tuple, Callable, Optional, Union, Any, Iterable\n\n# Scikit-learn for GP, Scaling, and potentially acquisition functions\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, ConstantKernel, WhiteKernel\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.exceptions import ConvergenceWarning\n\n# SciPy for statistics (needed for Expected Improvement calculation)\nfrom scipy.stats import norm\n\n# Assume functions.py contains implementations based on Kaandorp and Koole (2007)\n# from functions import bailey_welch_schedule, get_v_star, compute_convolutions, calculate_objective_serv_time_lookup\n# Mocking them for now as the file is not provided:\ndef bailey_welch_schedule(T, d, N, s): \n    # Placeholder: Returns a simple initial schedule (e.g., even distribution or zeros)\n    # Kaandorp and Koole (2007) mention Bailey-Welch schedules two patients at t=0, then one per block.\n    print(f\"Mock bailey_welch_schedule called with T={T}, d={d}, N={N}\")\n    mock_schedule = np.zeros(T)\n    if T &gt; 0 and N &gt; 0:\n        mock_schedule[0] = 2 # Bailey-Welch starts with two\n        remaining_N = N - 2\n        intervals_per_patient = T / N # Approximate spacing\n        current_interval = intervals_per_patient \n        for _ in range(remaining_N):\n            idx = min(int(round(current_interval)), T-1)\n            # Ensure we don't overwrite the first slot if T is small\n            if idx == 0 and mock_schedule[0] == 2: \n                 idx = 1 % T # Put in next slot if available\n            mock_schedule[idx] += 1\n            current_interval += intervals_per_patient\n        # Adjust if sum is not N due to rounding/integer constraints\n        current_sum = np.sum(mock_schedule)\n        diff = N - current_sum\n        if diff &gt; 0: # Add remaining patients somewhere\n            for i in range(int(diff)): mock_schedule[(i+1)%T] += 1\n        elif diff &lt; 0: # Remove excess patients (less likely with this mock)\n             for i in range(int(abs(diff))): \n                 non_zero_indices = np.where(mock_schedule &gt; 0)[0]\n                 if len(non_zero_indices) &gt; 0:\n                     mock_schedule[non_zero_indices[-1]] -= 1 # Remove from later slots first\n                 else: break # Should not happen if N &gt; 0\n    return mock_schedule\n\n\ndef get_v_star(T): \n    # Basis change vectors as defined in Kaandorp and Koole (2007), Appendix A\n    # Note: Their v_i corresponds to u_i in the original code's notation\n    print(f\"Mock get_v_star called with T={T}\")\n    v_star_matrix = np.zeros((T, T))\n    if T &gt; 0:\n        # v_0 = (-1, 0, ..., 0, 1) -&gt; Corresponds to u_1 in Kaandorp&Koole notation (shift from t=1 to t=T)\n        # The code's v_0 shifts from slot 0 to T-1. Let's match the code's definition.\n        v_star_matrix[0, 0] = -1\n        v_star_matrix[0, T-1] = 1 \n        # v_1 = (1, -1, 0, ..., 0) -&gt; Corresponds to u_2 in Kaandorp&Koole (shift from t=2 to t=1)\n        # Matches code's v_1 (shift from slot 1 to 0)\n        if T &gt; 1:\n            v_star_matrix[1, 1] = -1\n            v_star_matrix[1, 0] = 1\n        # v_i = (0,..,1_{i-1},-1_i,..,0) for i=2..T-1 -&gt; Corresponds to u_{i+1} in Kaandorp&Koole (shift from t=i+1 to t=i)\n        # Matches code's v_i (shift from slot i to i-1)\n        for i in range(2, T):\n            v_star_matrix[i, i] = -1\n            v_star_matrix[i, i-1] = 1\n    # The provided code uses T basis vectors. Kaandorp&Koole define T basis vectors u_1..u_T\n    # Let's ensure the dimensions match the code (TxT matrix for T basis vectors)\n    return v_star_matrix # Returns a TxT matrix where row i is basis vector v_i\n\ndef compute_convolutions(s, N, q_prob): \n    # Placeholder: Computes convolutions for queueing calculations (Kaandorp and Koole, 2007)\n    print(f\"Mock compute_convolutions called with N={N}, q_prob={q_prob}\")\n    return {\"mock_convolution_data\": True} \n\ndef calculate_objective_serv_time_lookup(Y, d, convolutions): \n    # Placeholder: Calculates EWT and ESP based on schedule Y (Kaandorp and Koole, 2007)\n    # Simple mock: return sum of Y as ewt and variance as esp, scaled\n    mock_ewt = np.sum(Y) * 0.1 * d # Factor in interval duration\n    mock_esp = np.var(Y) * 0.5 * d\n    return mock_ewt, mock_esp\n\n# Filter warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning) \n\n\n6.4.6.2 2. Constants\nDefinition of problem parameters and initial configuration.\n# --- Problem Definition ---\n\n# Fixed Data (Parameters for the scheduling problem based on Kaandorp and Koole (2007))\nN = 21          # Total number of patients to be scheduled\nT = 20          # Number of time slots (also dimension of perturbation vector U)\nd = 5           # Duration of each time slot (e.g., in minutes)\nmax_s = 20      # Maximum possible service time (used for generating distribution s_dist)\nq_prob = 0.20   # No-show probability (denoted 'rho' in Kaandorp and Koole (2007))\nw = 0.1         # Weight for EWT in objective C(x)\nl_target = 10   # Target average service time (parameter 'l' in original code)\nv_star = get_v_star(T) # Basis change vectors matrix (T x T)\n\n# Function to generate a weighted service time probability distribution\ndef generate_weighted_list(max_s_val: int, target_l: float, split_i: int) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Generates a service time probability distribution s.\n    (Implementation as provided previously)\n    \"\"\"\n    # Input Validation\n    if not isinstance(max_s_val, int) or max_s_val &lt;= 0: return None\n    if not isinstance(target_l, (int, float)) or not (1 &lt;= target_l &lt;= max_s_val): return None\n    if not isinstance(split_i, int) or not (0 &lt; split_i &lt; max_s_val): return None\n\n    def _objective(p_dist: np.ndarray) -&gt; float:\n        service_times = np.arange(1, max_s_val + 1)\n        weighted_avg = np.dot(service_times, p_dist)\n        return (weighted_avg - target_l) ** 2\n\n    constraints = ({'type': 'eq', 'fun': lambda p_dist: np.sum(p_dist) - 1.0})\n    bounds = [(0, 1)] * max_s_val\n    initial_guess = np.random.dirichlet(np.ones(max_s_val))\n    \n    try:\n        result = minimize(_objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n        if not result.success: return None\n        optimized_probs = result.x\n        optimized_probs[optimized_probs &lt; 0] = 0\n        current_sum = np.sum(optimized_probs)\n        if not np.isclose(current_sum, 1.0):\n            if current_sum &gt; 0: optimized_probs /= current_sum\n            else: return None\n    except Exception as e: return None\n\n    first_part_probs = optimized_probs[:split_i]\n    second_part_probs = optimized_probs[split_i:]\n    sorted_first_part = np.sort(first_part_probs)      \n    sorted_second_part = np.sort(second_part_probs)[::-1] \n    \n    final_dist = np.zeros(max_s_val + 1)\n    final_dist[1 : split_i + 1] = sorted_first_part\n    final_dist[split_i + 1 : max_s_val + 1] = sorted_second_part\n    \n    if not np.isclose(np.sum(final_dist[1:]), 1.0): print(f\"Warning: Service dist sum {np.sum(final_dist[1:])}\")\n    return final_dist\n\n# Generate service time distribution 's_dist'\ni_val = 5 \ns_dist = generate_weighted_list(max_s, l_target, i_val) \nif s_dist is not None:\n    convolutions = compute_convolutions(s_dist, N, q_prob)\nelse:\n    print(\"Failed to generate service time distribution. Using fallback.\")\n    s_dist = np.zeros(max_s + 1); \n    if max_s &gt; 0: s_dist[1:] = 1/max_s \n    convolutions = compute_convolutions(s_dist, N, q_prob) \n\n# Initial schedule 'X_initial_schedule' from Bailey-Welch heuristic\nX_initial_schedule = np.array(bailey_welch_schedule(T, d, N, s_dist))\nprint(f\"Initial schedule X: {X_initial_schedule}\")\n\n# Penalty for infeasible solutions\nLARGE_PENALTY = 1e10\n\n\n6.4.6.3 3. Common Functions (Objective Evaluation and HED)\nObjective evaluation implements \\(C(\\mathbf{x})\\) from Kaandorp and Koole (2007). HED implementation follows Deshwal et al. (2023).\ndef evaluate_objective(U_np, X_vec, v_star_matrix, convolutions_data, d_interval, w_weight):\n    \"\"\"\n    Evaluates the objective function C(x') where x' = X_vec + r(U_np).\n    Uses logic from Kaandorp and Koole (2007) via calculate_objective_serv_time_lookup.\n    Applies HED approach from Deshwal et al. (2023) by operating on U_np.\n    (Implementation as provided previously)\n    \"\"\"\n    # Input validation...\n    if not isinstance(U_np, np.ndarray): raise TypeError(\"Input U_np must be a numpy array\")\n    if U_np.ndim != 1: raise ValueError(\"Input U_np must be 1-dimensional\")\n    if U_np.shape[0] != v_star_matrix.shape[0]: raise ValueError(f\"Dimension mismatch: U_np length {U_np.shape[0]} != v_star_matrix rows {v_star_matrix.shape[0]}.\")\n    if X_vec.shape[0] != v_star_matrix.shape[1]: raise ValueError(\"Dimension mismatch: X_vec length must match v_star_matrix columns.\")\n    if not np.all((U_np == 0) | (U_np == 1)): raise ValueError(\"Input U_np must be binary (0s and 1s).\")\n\n    perturbation_rU = np.sum(v_star_matrix[U_np == 1, :], axis=0)\n    Y_schedule = X_vec + perturbation_rU\n\n    if np.all(Y_schedule &gt;= 0):\n        ewt, esp = calculate_objective_serv_time_lookup(Y_schedule, d_interval, convolutions_data)\n        objective_value = w_weight * ewt + (1 - w_weight) * esp\n        return objective_value\n    else:\n        return LARGE_PENALTY\n\n# --- Hamming Distance Embedding (HED) Implementation (Deshwal et al., 2023) ---\ndef hamming_distance(u1_vec: np.ndarray, u2_vec: np.ndarray) -&gt; int:\n    \"\"\"Calculates the Hamming distance.\"\"\"\n    return np.sum(u1_vec != u2_vec)\n\ndef generate_diverse_random_dictionary(T_dim: int, m_embed_dim: int) -&gt; np.ndarray:\n    \"\"\"Generates the random dictionary A using the 'diverse random' strategy.\"\"\"\n    dictionary_A = np.zeros((m_embed_dim, T_dim), dtype=int)\n    for i in range(m_embed_dim):\n        theta = np.random.uniform(0, 1) \n        row = (np.random.rand(T_dim) &lt; theta).astype(int)\n        dictionary_A[i, :] = row\n    return dictionary_A\n\ndef embed_vector(U_binary_vec: np.ndarray, dictionary_A: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Embeds a single binary vector U_binary_vec using HED.\"\"\"\n    m_embed_dim = dictionary_A.shape[0]\n    embedding_phi = np.zeros(m_embed_dim, dtype=float) \n    for i in range(m_embed_dim):\n        embedding_phi[i] = hamming_distance(U_binary_vec, dictionary_A[i, :])\n    return embedding_phi\n\ndef embed_batch(U_batch_binary_vecs: np.ndarray, dictionary_A: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Embeds a batch of binary vectors U_batch_binary_vecs using HED.\"\"\"\n    m_embed_dim = dictionary_A.shape[0]\n    if U_batch_binary_vecs.ndim == 1: U_batch_binary_vecs = U_batch_binary_vecs.reshape(1, -1)\n    batch_size = U_batch_binary_vecs.shape[0]\n    embeddings_np = np.zeros((batch_size, m_embed_dim), dtype=float) \n    for j in range(batch_size):\n        embeddings_np[j, :] = embed_vector(U_batch_binary_vecs[j, :], dictionary_A)\n    return embeddings_np\n\n# --- Common Gaussian Process (GP) Fitting Function (Deshwal et al., 2023) ---\ndef get_fitted_model(train_X_embedded_scaled: np.ndarray, train_Y_neg_obj: np.ndarray, m_embed_dim: int) -&gt; GaussianProcessRegressor:\n    \"\"\"Fits a GP model to the scaled embedded training data.\"\"\"\n    if train_Y_neg_obj.ndim &gt; 1 and train_Y_neg_obj.shape[1] == 1: train_Y_neg_obj = train_Y_neg_obj.ravel()\n\n    kernel = ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \\\n             Matern(length_scale=np.ones(m_embed_dim), length_scale_bounds=(1e-2, 1e2), nu=2.5) + \\\n             WhiteKernel(noise_level=1e-10, noise_level_bounds=\"fixed\") \n    \n    gp_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-10, n_restarts_optimizer=10, random_state=42)\n    gp_model.fit(train_X_embedded_scaled, train_Y_neg_obj)\n    return gp_model\n\n\n6.4.6.4 4. Experiment 1: CBO with Expected Improvement (EI)\nApplies the methodology from Deshwal et al. (2023) using EI.\n# --- EI Specific Acquisition Function and Optimization ---\ndef expected_improvement(mu_pred: np.ndarray, sigma_pred: np.ndarray, f_best_neg: float, xi: float = 0.01) -&gt; np.ndarray:\n    \"\"\"Computes the Expected Improvement (EI) acquisition function.\"\"\"\n    sigma_pred = np.maximum(sigma_pred, 1e-9)\n    Z = (mu_pred - f_best_neg - xi) / sigma_pred\n    ei_values = (mu_pred - f_best_neg - xi) * norm.cdf(Z) + sigma_pred * norm.pdf(Z)\n    ei_values[sigma_pred &lt;= 1e-9] = 0.0\n    return ei_values\n\ndef optimize_acqf_ei(gp_model: GaussianProcessRegressor, scaler: MinMaxScaler, \n                     dictionary_A: np.ndarray, T_dim: int, q_batch_size: int, \n                     num_acqf_candidates: int, current_best_neg_f_val: float) -&gt; np.ndarray:\n    \"\"\"Optimizes the EI acquisition function using HED.\"\"\"\n    m_embed_dim = dictionary_A.shape[0]\n    candidate_u_vectors_np = np.random.randint(0, 2, size=(num_acqf_candidates, T_dim))\n    embedded_candidates_np = embed_batch(candidate_u_vectors_np, dictionary_A)\n    embedded_candidates_scaled_np = scaler.transform(embedded_candidates_np)\n    mu_pred, std_pred = gp_model.predict(embedded_candidates_scaled_np, return_std=True)\n    acq_values = expected_improvement(mu_pred, std_pred, current_best_neg_f_val, xi=0.01)\n    top_indices = np.argsort(acq_values)[-q_batch_size:][::-1]\n    return candidate_u_vectors_np[top_indices, :]\n\n# --- BO Loop Parameters ---\nN_INITIAL_POINTS = 20\nN_BO_ITERATIONS = 25\nBATCH_SIZE_Q = 5 \nNUM_ACQF_CANDIDATES = T * 1024 \nM_EMBED_DIM = 128 # Dictionary size as used in Deshwal et al. (2023) experiments\n\n# --- EI BO Loop ---\nprint(\"\\n--- Starting Experiment 1: CBO with Expected Improvement (EI) ---\")\nevaluated_U_np_list_ei = []  \nevaluated_f_vals_ei = []     \ntrain_Y_list_ei = []         \n\n# Initialization \nprint(f\"Generating {N_INITIAL_POINTS} initial random points for EI...\")\ninitial_candidates_ei = []\nwhile len(initial_candidates_ei) &lt; N_INITIAL_POINTS:\n    U_init_candidate = np.random.randint(0, 2, size=T)\n    if not any(np.array_equal(U_init_candidate, u) for u in initial_candidates_ei):\n        initial_candidates_ei.append(U_init_candidate)\n\nfor U_init in initial_candidates_ei:\n    f_val = evaluate_objective(U_init, X_initial_schedule, v_star, convolutions, d, w)\n    neg_f_val = -f_val \n    evaluated_U_np_list_ei.append(U_init)\n    evaluated_f_vals_ei.append(f_val)\n    train_Y_list_ei.append(neg_f_val)\n\nbest_obj_so_far_ei = min(evaluated_f_vals_ei) if evaluated_f_vals_ei else float('inf')\nprint(f\"Initial best objective value (EI): {best_obj_so_far_ei:.4f}\")\nif not np.isfinite(best_obj_so_far_ei): print(\"Warning: Initial best objective (EI) is infinite.\")\n\n# Bayesian Optimization Iterations\nfor iteration in range(N_BO_ITERATIONS):\n    iter_start_time = time.time()\n    print(f\"\\n--- EI Iteration {iteration + 1}/{N_BO_ITERATIONS} ---\")\n    current_dictionary_A_ei = generate_diverse_random_dictionary(T, M_EMBED_DIM)\n    if not evaluated_U_np_list_ei: continue\n    evaluated_U_np_array_ei = np.array(evaluated_U_np_list_ei)\n    embedded_train_X_ei = embed_batch(evaluated_U_np_array_ei, current_dictionary_A_ei)\n    scaler_ei = MinMaxScaler() \n    if embedded_train_X_ei.shape[0] &gt; 0:\n        embedded_train_X_scaled_ei = scaler_ei.fit_transform(embedded_train_X_ei)\n    else: embedded_train_X_scaled_ei = embedded_train_X_ei\n    train_Y_for_fit_ei = np.array(train_Y_list_ei) \n    if embedded_train_X_scaled_ei.shape[0] &gt; 0 and train_Y_for_fit_ei.shape[0] == embedded_train_X_scaled_ei.shape[0]:\n        gp_model_ei = get_fitted_model(embedded_train_X_scaled_ei, train_Y_for_fit_ei, M_EMBED_DIM)\n    else: continue\n    current_best_neg_f_val_ei = np.max(train_Y_for_fit_ei) if train_Y_for_fit_ei.size &gt; 0 else -float('inf')\n    if current_best_neg_f_val_ei &lt;= -LARGE_PENALTY / 2 and np.isfinite(current_best_neg_f_val_ei): pass\n    next_U_candidates_np_ei = optimize_acqf_ei(\n        gp_model_ei, scaler_ei, current_dictionary_A_ei, T, BATCH_SIZE_Q, NUM_ACQF_CANDIDATES, current_best_neg_f_val_ei\n    )\n    for i_cand in range(next_U_candidates_np_ei.shape[0]):\n        next_U = next_U_candidates_np_ei[i_cand, :]\n        if any(np.array_equal(next_U, u) for u in evaluated_U_np_list_ei): continue\n        next_f = evaluate_objective(next_U, X_initial_schedule, v_star, convolutions, d, w)\n        evaluated_U_np_list_ei.append(next_U)\n        evaluated_f_vals_ei.append(next_f)\n        train_Y_list_ei.append(-next_f) \n        if next_f &lt; best_obj_so_far_ei: best_obj_so_far_ei = next_f\n    print(f\"Best objective (EI) so far: {best_obj_so_far_ei:.4f}\")\n\n# --- Results for EI ---\nprint(\"\\n--- EI Optimization Finished ---\")\nif not evaluated_f_vals_ei:\n    final_best_f_ei = float('inf'); final_best_U_ei = None\nelse:\n    final_best_idx_ei = np.argmin(evaluated_f_vals_ei)\n    final_best_U_ei = evaluated_U_np_list_ei[final_best_idx_ei]\n    final_best_f_ei = evaluated_f_vals_ei[final_best_idx_ei]\n    print(f\"Total evaluations (EI): {len(evaluated_f_vals_ei)}\")\n    print(f\"Best Objective Value Found (EI): {final_best_f_ei:.4f}\")\n\n\n6.4.6.5 5. Experiment 2: CBO with Lower Confidence Bound (LCB) - Fixed Kappa\nApplies the methodology from Deshwal et al. (2023) using LCB with fixed \\(\\kappa\\).\n# --- LCB Specific Acquisition Function and Optimization ---\ndef lower_confidence_bound(mu_pred: np.ndarray, sigma_pred: np.ndarray, kappa: float) -&gt; np.ndarray:\n    \"\"\"Computes the Lower Confidence Bound (LCB) acquisition function.\"\"\"\n    sigma_pred = np.maximum(sigma_pred, 1e-9) \n    return mu_pred - kappa * sigma_pred \n\ndef optimize_acqf_lcb(gp_model: GaussianProcessRegressor, scaler: MinMaxScaler, \n                      dictionary_A: np.ndarray, T_dim: int, q_batch_size: int, \n                      num_acqf_candidates: int, kappa_val: float) -&gt; np.ndarray:\n    \"\"\"Optimizes the LCB acquisition function using HED.\"\"\"\n    m_embed_dim = dictionary_A.shape[0]\n    candidate_u_vectors_np = np.random.randint(0, 2, size=(num_acqf_candidates, T_dim))\n    embedded_candidates_np = embed_batch(candidate_u_vectors_np, dictionary_A)\n    embedded_candidates_scaled_np = scaler.transform(embedded_candidates_np)\n    mu_pred, std_pred = gp_model.predict(embedded_candidates_scaled_np, return_std=True)\n    acq_values = lower_confidence_bound(mu_pred, std_pred, kappa=kappa_val)\n    top_indices = np.argsort(acq_values)[-q_batch_size:][::-1]\n    return candidate_u_vectors_np[top_indices, :]\n\n# --- LCB (Fixed Kappa) BO Loop ---\nprint(\"\\n--- Starting Experiment 2: CBO with LCB (Fixed Kappa) ---\")\nKAPPA_FIXED = 2.576 # Standard value for ~99% confidence\n\nevaluated_U_np_list_lcb_fixed = []\nevaluated_f_vals_lcb_fixed = []\ntrain_Y_list_lcb_fixed = []\n\n# Initialization\nprint(f\"Generating {N_INITIAL_POINTS} initial random points for LCB (Fixed Kappa)...\")\ninitial_candidates_lcb_fixed = []\nwhile len(initial_candidates_lcb_fixed) &lt; N_INITIAL_POINTS:\n    U_init_candidate = np.random.randint(0, 2, size=T)\n    if not any(np.array_equal(U_init_candidate, u) for u in initial_candidates_lcb_fixed):\n        initial_candidates_lcb_fixed.append(U_init_candidate)\n\nfor U_init in initial_candidates_lcb_fixed:\n    f_val = evaluate_objective(U_init, X_initial_schedule, v_star, convolutions, d, w)\n    neg_f_val = -f_val\n    evaluated_U_np_list_lcb_fixed.append(U_init)\n    evaluated_f_vals_lcb_fixed.append(f_val)\n    train_Y_list_lcb_fixed.append(neg_f_val)\n\nbest_obj_so_far_lcb_fixed = min(evaluated_f_vals_lcb_fixed) if evaluated_f_vals_lcb_fixed else float('inf')\nprint(f\"Initial best objective value (LCB Fixed Kappa): {best_obj_so_far_lcb_fixed:.4f}\")\n\n# BO Iterations\nfor iteration in range(N_BO_ITERATIONS):\n    iter_start_time = time.time()\n    print(f\"\\n--- LCB (Fixed Kappa) Iteration {iteration + 1}/{N_BO_ITERATIONS} ---\")\n    current_dictionary_A_lcb_fixed = generate_diverse_random_dictionary(T, M_EMBED_DIM)\n    if not evaluated_U_np_list_lcb_fixed: continue\n    evaluated_U_np_array_lcb_fixed = np.array(evaluated_U_np_list_lcb_fixed)\n    embedded_train_X_lcb_fixed = embed_batch(evaluated_U_np_array_lcb_fixed, current_dictionary_A_lcb_fixed)\n    scaler_lcb_fixed = MinMaxScaler()\n    if embedded_train_X_lcb_fixed.shape[0] &gt; 0:\n        embedded_train_X_scaled_lcb_fixed = scaler_lcb_fixed.fit_transform(embedded_train_X_lcb_fixed)\n    else: embedded_train_X_scaled_lcb_fixed = embedded_train_X_lcb_fixed\n    train_Y_for_fit_lcb_fixed = np.array(train_Y_list_lcb_fixed)\n    if embedded_train_X_scaled_lcb_fixed.shape[0] &gt; 0 and train_Y_for_fit_lcb_fixed.shape[0] == embedded_train_X_scaled_lcb_fixed.shape[0]:\n        gp_model_lcb_fixed = get_fitted_model(embedded_train_X_scaled_lcb_fixed, train_Y_for_fit_lcb_fixed, M_EMBED_DIM)\n    else: continue\n    next_U_candidates_np_lcb_fixed = optimize_acqf_lcb(\n        gp_model_lcb_fixed, scaler_lcb_fixed, current_dictionary_A_lcb_fixed, T, BATCH_SIZE_Q, NUM_ACQF_CANDIDATES, KAPPA_FIXED\n    )\n    for i_cand in range(next_U_candidates_np_lcb_fixed.shape[0]):\n        next_U = next_U_candidates_np_lcb_fixed[i_cand, :]\n        if any(np.array_equal(next_U, u) for u in evaluated_U_np_list_lcb_fixed): continue\n        next_f = evaluate_objective(next_U, X_initial_schedule, v_star, convolutions, d, w)\n        evaluated_U_np_list_lcb_fixed.append(next_U)\n        evaluated_f_vals_lcb_fixed.append(next_f)\n        train_Y_list_lcb_fixed.append(-next_f)\n        if next_f &lt; best_obj_so_far_lcb_fixed: best_obj_so_far_lcb_fixed = next_f\n    print(f\"Best objective (LCB Fixed Kappa) so far: {best_obj_so_far_lcb_fixed:.4f}\")\n\n# --- Results for LCB (Fixed Kappa) ---\nprint(\"\\n--- LCB (Fixed Kappa) Optimization Finished ---\")\nif not evaluated_f_vals_lcb_fixed:\n    final_best_f_lcb_fixed = float('inf'); final_best_U_lcb_fixed = None\nelse:\n    final_best_idx_lcb_fixed = np.argmin(evaluated_f_vals_lcb_fixed)\n    final_best_U_lcb_fixed = evaluated_U_np_list_lcb_fixed[final_best_idx_lcb_fixed]\n    final_best_f_lcb_fixed = evaluated_f_vals_lcb_fixed[final_best_idx_lcb_fixed]\n    print(f\"Total evaluations (LCB Fixed Kappa): {len(evaluated_f_vals_lcb_fixed)}\")\n    print(f\"Best Objective Value Found (LCB Fixed Kappa): {final_best_f_lcb_fixed:.4f}\")\n\n\n6.4.6.6 6. Experiment 3: CBO with Lower Confidence Bound (LCB) - Increasing Kappa\nApplies the methodology from Deshwal et al. (2023) using LCB with dynamic \\(\\kappa\\).\n# --- LCB (Increasing Kappa) BO Loop ---\nprint(\"\\n--- Starting Experiment 3: CBO with LCB (Increasing Kappa) ---\")\nINITIAL_KAPPA_DYNAMIC = 3.75\nKAPPA_INCREASE_FACTOR = 1.3\n# MAX_KAPPA_DYNAMIC = 15.0 # Optional cap\n\nevaluated_U_np_list_lcb_dyn = []\nevaluated_f_vals_lcb_dyn = []\ntrain_Y_list_lcb_dyn = []\ncurrent_kappa_dynamic = INITIAL_KAPPA_DYNAMIC\n\n# Initialization\nprint(f\"Generating {N_INITIAL_POINTS} initial random points for LCB (Increasing Kappa)...\")\ninitial_candidates_lcb_dyn = []\nwhile len(initial_candidates_lcb_dyn) &lt; N_INITIAL_POINTS:\n    U_init_candidate = np.random.randint(0, 2, size=T)\n    if not any(np.array_equal(U_init_candidate, u) for u in initial_candidates_lcb_dyn):\n        initial_candidates_lcb_dyn.append(U_init_candidate)\n\nfor U_init in initial_candidates_lcb_dyn:\n    f_val = evaluate_objective(U_init, X_initial_schedule, v_star, convolutions, d, w)\n    neg_f_val = -f_val\n    evaluated_U_np_list_lcb_dyn.append(U_init)\n    evaluated_f_vals_lcb_dyn.append(f_val)\n    train_Y_list_lcb_dyn.append(neg_f_val)\n\nbest_obj_so_far_lcb_dyn = min(evaluated_f_vals_lcb_dyn) if evaluated_f_vals_lcb_dyn else float('inf')\nprint(f\"Initial best objective value (LCB Increasing Kappa): {best_obj_so_far_lcb_dyn:.4f}\")\nprint(f\"Initial Kappa: {current_kappa_dynamic:.3f}\")\n\n# BO Iterations\nfor iteration in range(N_BO_ITERATIONS):\n    iter_start_time = time.time()\n    print(f\"\\n--- LCB (Increasing Kappa) Iteration {iteration + 1}/{N_BO_ITERATIONS} ---\")\n    current_dictionary_A_lcb_dyn = generate_diverse_random_dictionary(T, M_EMBED_DIM)\n    if not evaluated_U_np_list_lcb_dyn: continue\n    evaluated_U_np_array_lcb_dyn = np.array(evaluated_U_np_list_lcb_dyn)\n    embedded_train_X_lcb_dyn = embed_batch(evaluated_U_np_array_lcb_dyn, current_dictionary_A_lcb_dyn)\n    scaler_lcb_dyn = MinMaxScaler()\n    if embedded_train_X_lcb_dyn.shape[0] &gt; 0:\n        embedded_train_X_scaled_lcb_dyn = scaler_lcb_dyn.fit_transform(embedded_train_X_lcb_dyn)\n    else: embedded_train_X_scaled_lcb_dyn = embedded_train_X_lcb_dyn\n    train_Y_for_fit_lcb_dyn = np.array(train_Y_list_lcb_dyn)\n    if embedded_train_X_scaled_lcb_dyn.shape[0] &gt; 0 and train_Y_for_fit_lcb_dyn.shape[0] == embedded_train_X_scaled_lcb_dyn.shape[0]:\n        gp_model_lcb_dyn = get_fitted_model(embedded_train_X_scaled_lcb_dyn, train_Y_for_fit_lcb_dyn, M_EMBED_DIM)\n    else: continue\n    next_U_candidates_np_lcb_dyn = optimize_acqf_lcb(\n        gp_model_lcb_dyn, scaler_lcb_dyn, current_dictionary_A_lcb_dyn, T, BATCH_SIZE_Q, NUM_ACQF_CANDIDATES, current_kappa_dynamic\n    )\n    improvement_found_in_batch = False\n    for i_cand in range(next_U_candidates_np_lcb_dyn.shape[0]):\n        next_U = next_U_candidates_np_lcb_dyn[i_cand, :]\n        if any(np.array_equal(next_U, u) for u in evaluated_U_np_list_lcb_dyn): continue\n        next_f = evaluate_objective(next_U, X_initial_schedule, v_star, convolutions, d, w)\n        evaluated_U_np_list_lcb_dyn.append(next_U)\n        evaluated_f_vals_lcb_dyn.append(next_f)\n        train_Y_list_lcb_dyn.append(-next_f)\n        if next_f &lt; best_obj_so_far_lcb_dyn:\n            best_obj_so_far_lcb_dyn = next_f\n            old_kappa = current_kappa_dynamic\n            current_kappa_dynamic *= KAPPA_INCREASE_FACTOR\n            # current_kappa_dynamic = min(current_kappa_dynamic, MAX_KAPPA_DYNAMIC) # Optional cap\n            improvement_found_in_batch = True\n    print(f\"Best objective (LCB Increasing Kappa) so far: {best_obj_so_far_lcb_dyn:.4f}\")\n    # print(f\"Kappa for next iteration: {current_kappa_dynamic:.3f}\")\n\n# --- Results for LCB (Increasing Kappa) ---\nprint(\"\\n--- LCB (Increasing Kappa) Optimization Finished ---\")\nif not evaluated_f_vals_lcb_dyn:\n    final_best_f_lcb_dyn = float('inf'); final_best_U_lcb_dyn = None\nelse:\n    final_best_idx_lcb_dyn = np.argmin(evaluated_f_vals_lcb_dyn)\n    final_best_U_lcb_dyn = evaluated_U_np_list_lcb_dyn[final_best_idx_lcb_dyn]\n    final_best_f_lcb_dyn = evaluated_f_vals_lcb_dyn[final_best_idx_lcb_dyn]\n    print(f\"Total evaluations (LCB Increasing Kappa): {len(evaluated_f_vals_lcb_dyn)}\")\n    print(f\"Best Objective Value Found (LCB Increasing Kappa): {final_best_f_lcb_dyn:.4f}\")\n    print(f\"Final Kappa value reached: {current_kappa_dynamic:.3f}\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#results",
    "href": "combinatorial-bayes-optimization.html#results",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.5 Results",
    "text": "6.5 Results\nThe initial schedule, derived using the Bailey-Welch method (Kaandorp and Koole, 2007), serves as a baseline. The objective function \\(C(\\mathbf{x})\\) combines Expected Waiting Time (\\(EWT\\)) and Expected Staff Penalty (\\(ESP\\)). Lower values of \\(C(\\mathbf{x})\\) are preferable. Each experiment consisted of \\(N_{INITIAL} = 20\\) initial random evaluations followed by \\(N_{ITERATIONS} = 25\\) Bayesian optimization iterations, with \\(BATCH\\_SIZE_q = 5\\) evaluations per iteration, totaling approximately \\(20 + 25 \\times 5 = 145\\) evaluations per experiment. The optimization operates on the binary perturbation vector \\(\\mathbf{U}\\), using the HED embedding (Deshwal et al., 2023).\nThe key performance metric is the best (minimum) objective function value found.\n\n6.5.1 Experiment 1: CBO with Expected Improvement (EI)\n\nInitial Best Objective (after random search): \\(73.6389\\)\nFinal Best Objective Found: \\(68.2348\\)\nTotal Evaluations: \\(145\\)\nBest Perturbation Vector \\(\\mathbf{U}_{EI}^*\\): [0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1]\nResulting Optimal Schedule \\(\\mathbf{x}_{EI}^*\\): [2 1 1 0 1 0 2 0 0 1 1 1 0 1 0 1 1 0 1 7]\n\n\n\n6.5.2 Experiment 2: CBO with Lower Confidence Bound (LCB) - Fixed Kappa (\\(\\kappa = 2.576\\))\n\nInitial Best Objective (after random search): \\(68.0201\\)\nFinal Best Objective Found: \\(66.9201\\)\nTotal Evaluations: \\(143\\)\nBest Perturbation Vector \\(\\mathbf{U}_{LCB\\_fixed}^*\\): [0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1]\nResulting Optimal Schedule \\(\\mathbf{x}_{LCB\\_fixed}^*\\): [2 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 7]\n\n\n\n6.5.3 Experiment 3: CBO with Lower Confidence Bound (LCB) - Increasing Kappa\n\nInitial Best Objective (after random search): \\(74.0010\\)\nFinal Best Objective Found: \\(68.0497\\)\nTotal Evaluations: \\(145\\)\nBest Perturbation Vector \\(\\mathbf{U}_{LCB\\_dyn}^*\\): [0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 1]\nResulting Optimal Schedule \\(\\mathbf{x}_{LCB\\_dyn}^*\\): [2 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 7]\nInitial \\(\\kappa\\): \\(3.750\\)\nFinal \\(\\kappa\\): \\(39.767\\)\n\n\n\n6.5.4 Summary of Best Objectives\n\n\n\n\n\n\n\nExperiment\nBest Objective \\(C(\\mathbf{x}^*)\\)\n\n\n\n\nCBO with EI\n\\(68.2348\\)\n\n\nCBO with LCB (Fixed \\(\\kappa=2.576\\))\n\\(66.9201\\)\n\n\nCBO with LCB (Increasing \\(\\kappa\\))\n\\(68.0497\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#discussion",
    "href": "combinatorial-bayes-optimization.html#discussion",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.6 Discussion",
    "text": "6.6 Discussion\nThe experiments aimed to compare three CBO strategies, leveraging the HED embedding technique (Deshwal et al., 2023), for optimizing the outpatient appointment scheduling problem formulated by Kaandorp and Koole (2007). All three methods successfully improved upon their respective initial random search results, demonstrating the applicability of BO with HED to this combinatorial problem.\n\nPerformance Comparison: The CBO strategy employing LCB with a fixed \\(\\kappa = 2.576\\) yielded the lowest objective function value (\\(66.9201\\)), indicating the best performance among the tested methods for this specific problem instance and parameter settings. EI achieved a final objective of \\(68.2348\\), while LCB with increasing \\(\\kappa\\) reached \\(68.0497\\).\nHypothesis Evaluation:\n\nHypothesis 1 was supported: All CBO strategies found significantly better schedules than the initial random points.\nHypothesis 2 received partial support: LCB with fixed \\(\\kappa\\) outperformed EI, but LCB with increasing \\(\\kappa\\) did not significantly outperform EI.\nHypothesis 3 was not supported: The dynamic \\(\\kappa\\) strategy, despite adapting its exploration parameter substantially (from \\(3.750\\) to \\(39.767\\)), did not achieve the best result, being outperformed by the fixed \\(\\kappa\\) LCB.\n\nExploration vs. Exploitation: The fixed \\(\\kappa\\) LCB strategy appears to strike an effective balance for this problem. The dynamic \\(\\kappa\\) strategy, becoming increasingly explorative upon finding improvements, might have explored too broadly after initial gains, potentially moving away from the most promising regions identified earlier. The EI strategy’s focus on immediate expected improvement was less effective than the fixed LCB’s balanced approach. The effectiveness of the HED embedding, combined with ARD in the GP kernel (as discussed by Deshwal et al., 2023), likely played a crucial role in enabling the GP to model the complex objective landscape effectively.\nComputational Effort: All methods required a similar number of expensive objective function evaluations (~145). The main computational overhead per iteration, beyond function evaluation, involves HED embedding, GP fitting, and acquisition function optimization.\nLimitations and Future Work:\n\nThe optimality guarantee mentioned by Kaandorp and Koole (2007) applies to their specific local search algorithm operating directly on the schedule space \\(\\mathcal{F}\\), leveraging multimodularity. Our BO approach operates on the perturbation vector space \\(\\mathbf{U}\\) via HED embeddings. While BO aims for global optimization, it doesn’t inherit the same theoretical guarantee of finding the global optimum as the original local search, especially given the stochastic nature of GP modeling and acquisition function optimization.\nThe performance is likely sensitive to BO hyperparameters (dictionary size \\(m\\), \\(\\kappa\\) values, number of candidates for acquisition optimization).\nThe mock implementation of functions.py needs replacement with actual scheduling logic for accurate results.\nFurther investigation into different dictionary construction methods (e.g., binary wavelets as mentioned in Deshwal et al., 2023) or adaptive \\(\\kappa\\) schedules could be beneficial.\n\n\nIn conclusion, applying CBO with HED embeddings appears promising for this scheduling problem. The LCB acquisition function with a fixed, well-chosen \\(\\kappa\\) demonstrated the best performance in this study.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#timeline",
    "href": "combinatorial-bayes-optimization.html#timeline",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.7 Timeline",
    "text": "6.7 Timeline\n\nExperiment Setup and Code Implementation: (Specify Date Range)\nInitial Parameter Tuning (if any): (Specify Date Range)\nExecution of Experiment 1 (EI): (Specify Date)\nExecution of Experiment 2 (LCB Fixed Kappa): (Specify Date)\nExecution of Experiment 3 (LCB Increasing Kappa): (Specify Date)\nResults Analysis and Report Compilation: (Specify Date Range)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "combinatorial-bayes-optimization.html#references",
    "href": "combinatorial-bayes-optimization.html#references",
    "title": "6  Combinatorial Bayesian Optimization Experiments",
    "section": "6.8 References",
    "text": "6.8 References\n\nDeshwal, A., Ament, S., Balandat, M., Bakshy, E., Doppa, J. R., & Eriksson, D. (2023). Bayesian Optimization over High-Dimensional Combinatorial Spaces via Dictionary-based Embeddings. Proceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS), PMLR 206:5141-5168.\nKaandorp, G. C., & Koole, G. (2007). Optimal outpatient appointment scheduling. Health Care Management Science, 10(3), 217–229. https://doi.org/10.1007/s10729-007-9015-x\n(Placeholder for Bailey-Welch algorithm reference, e.g., Bailey, N. T. J. (1952). A study of queues and appointment systems in hospital out-patient departments, with special reference to waiting-times. Journal of the Royal Statistical Society: Series B (Methodological), 14(2), 185-199.)\n(Placeholder for Gaussian Process literature, e.g., Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press.)\n\n```\nIk heb de verwijzingen naar (Kaandorp and Koole, 2007) en (Deshwal et al., 2023) toegevoegd in de secties Objective, Background, Methodology, Discussion en References. De tekst is aangepast om de bijdragen van elk paper aan het huidige experiment te verduidelijk",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combinatorial Bayesian Optimization Experiments</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "7  References",
    "section": "",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "service-time-with-no-shows.html",
    "href": "service-time-with-no-shows.html",
    "title": "service_time_with_no_shows",
    "section": "",
    "text": "Function Documentation\nservice_time_with_no_shows(s: List[float], q: float) -&gt; List[float]",
    "crumbs": [
      "Function documentation",
      "`service_time_with_no_shows`"
    ]
  },
  {
    "objectID": "service-time-with-no-shows.html#function-documentation",
    "href": "service-time-with-no-shows.html#function-documentation",
    "title": "service_time_with_no_shows",
    "section": "",
    "text": "Description\nAdjusts a distribution of service times to account for no-shows. The function scales the original service time distribution by the probability of a patient showing up (i.e., 1 - q) and then adds the no-show probability q to the service time for zero time slots.\n\n\nParameters\n\ns (List[float]): The original service time probability distribution. This list represents the probabilities associated with different service times.\nq (float): The probability of no-shows. This value should be between 0 and 1.\n\n\n\nReturns\n\nList[float]: The adjusted service time probability distribution where the no-show probability has been incorporated into the probability of zero service time.\n\n\n\nExample\n\nfrom functions import service_time_with_no_shows\n\n# Example usage\noriginal_distribution = [0.0, 0.5, 0.3, 0.2]\nno_show_probability = 0.1\nadjusted_distribution = service_time_with_no_shows(original_distribution, no_show_probability)\nprint(\"Adjusted distribution:\", adjusted_distribution)\n\nAdjusted distribution: [0.1, 0.45, 0.27, 0.18000000000000002]\n\n\n\nimport unittest\n\nclass TestServiceTimeWithNoShows(unittest.TestCase):\n    def test_adjust_distribution(self):\n        # Test with a known distribution and no-show probability\n        original_distribution = [0.0, 0.5, 0.3, 0.2]\n        no_show_probability = 0.1\n        \n        # Expected adjustment: second element 0.1, \n        # other elements: multiplied by 0.9\n        expected_distribution = [0.1, 0.45, 0.27, 0.18]\n        \n        result = service_time_with_no_shows(original_distribution, no_show_probability)\n        \n        # Using almost equal check due to floating point arithmetic\n        for r, e in zip(result, expected_distribution):\n            self.assertAlmostEqual(r, e, places=5)\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`service_time_with_no_shows`"
    ]
  },
  {
    "objectID": "compute-convolutions.html",
    "href": "compute-convolutions.html",
    "title": "compute_convolutions",
    "section": "",
    "text": "Function Documentation\ncompute_convolutions(probabilities: List[float], N: int, q: float = 0.0) -&gt; Dict[int, np.ndarray]",
    "crumbs": [
      "Function documentation",
      "`compute_convolutions`"
    ]
  },
  {
    "objectID": "compute-convolutions.html#function-documentation",
    "href": "compute-convolutions.html#function-documentation",
    "title": "compute_convolutions",
    "section": "",
    "text": "Description\nComputes the k-fold convolution of a given probability mass function (PMF) for k from 1 up to N. Before computing the convolutions, the PMF is adjusted for no-shows using the provided no-show probability q via the service_time_with_no_shows function. Convolution is performed using NumPy’s np.convolve.\n\n\nParameters\n\nprobabilities (List[float]): The original PMF represented as a list where the index corresponds to a value (for instance, a service time) and the value at that index is its probability. This function is generic and does not have to be used solely for service times.\nN (int): The maximum number of convolutions to compute.\nq (float, optional): The probability of a no-show. Defaults to 0.0.\n\n\n\nReturns\n\nDict[int, np.ndarray]: A dictionary where each key k (with 1 ≤ k ≤ N) corresponds to the PMF resulting from the k-fold convolution of the adjusted PMF.\n\n\n\nExample\n\nimport numpy as np\nfrom functions import compute_convolutions, service_time_with_no_shows\n\n# Example usage\noriginal_pmf = [0.0, 0.5, 0.3, 0.2]\nN = 3\nno_show_probability = 0.1\n\nconvs = compute_convolutions(original_pmf, N, no_show_probability)\nfor k, pmf in convs.items():\n    print(f\"{k}-fold convolution: {pmf}\")\n\n1-fold convolution: [0.1  0.45 0.27 0.18]\n2-fold convolution: [0.01   0.09   0.2565 0.279  0.2349 0.0972 0.0324]\n3-fold convolution: [0.001    0.0135   0.06885  0.169425 0.234495 0.236925 0.160623 0.083106\n 0.026244 0.005832]\n\n\n\nimport unittest\n\nclass TestComputeConvolutions(unittest.TestCase):\n    def test_single_convolution(self):\n        # When N = 1, the result should be the adjusted PMF\n        original_pmf = [0.0, 0.5, 0.3, 0.2]\n        no_show_probability = 0.1\n        N = 1\n        expected = np.array(service_time_with_no_shows(original_pmf, no_show_probability))\n        result = compute_convolutions(original_pmf, N, no_show_probability)\n        self.assertTrue(np.allclose(result[1], expected), \"Single convolution test failed\")\n\n    def test_multiple_convolutions(self):\n        # Test for N = 3 using a simple PMF\n        original_pmf = [0.0, 0.5, 0.3, 0.2]\n        no_show_probability = 0.0  # No adjustment for simplicity\n        N = 3\n        result = compute_convolutions(original_pmf, N, no_show_probability)\n\n        # For N=1, result is the original pmf\n        self.assertTrue(np.allclose(result[1], np.array(original_pmf)))\n\n        # For higher convolutions, ensure the sum of probabilities remains 1 (within numerical precision)\n        for k in range(1, N + 1):\n            self.assertAlmostEqual(np.sum(result[k]), 1.0, places=5, msg=f\"Sum of probabilities for {k}-fold convolution is not 1\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.002s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`compute_convolutions`"
    ]
  },
  {
    "objectID": "calculate-objective-serv-time-lookup.html",
    "href": "calculate-objective-serv-time-lookup.html",
    "title": "calculate_objective_serv_time_lookup",
    "section": "",
    "text": "Function Documentation\ncalculate_objective_serv_time_lookup(schedule: List[int], d: int, convolutions: dict) -&gt; Tuple[float, float]",
    "crumbs": [
      "Function documentation",
      "`calculate_objective_serv_time_lookup`"
    ]
  },
  {
    "objectID": "calculate-objective-serv-time-lookup.html#function-documentation",
    "href": "calculate-objective-serv-time-lookup.html#function-documentation",
    "title": "calculate_objective_serv_time_lookup",
    "section": "",
    "text": "Description\nThis notebook provides documentation for the function calculate_objective_serv_time_lookup, which calculates an objective value (in terms of expected waiting time and expected spillover) based on a given schedule and pre-computed convolutions of a probability mass function (PMF).\nThe function uses the following inputs:\n\nschedule: A list of integers representing the number of patients scheduled in each time slot.\nd: An integer indicating the duration threshold for a time slot.\nconvolutions: A dictionary of precomputed convolutions of the service time PMF. The key 1 should correspond to the adjusted service time distribution (for example, one adjusted for no-shows), while keys greater than 1 are used for multiple patients in a time slot.\n\nThe function returns a tuple:\n\newt: The sum of expected waiting times over the schedule.\nesp: The expected spillover time (or overtime) after the final time slot.",
    "crumbs": [
      "Function documentation",
      "`calculate_objective_serv_time_lookup`"
    ]
  },
  {
    "objectID": "calculate-objective-serv-time-lookup.html#example-usage",
    "href": "calculate-objective-serv-time-lookup.html#example-usage",
    "title": "calculate_objective_serv_time_lookup",
    "section": "Example Usage",
    "text": "Example Usage\nA trivial example using a precomputed convolution dictionary with a degenerate PMF (i.e. always zero service time) is provided in the unit tests below.\n\nimport numpy as np\nfrom typing import List, Dict, Tuple\nfrom functions import service_time_with_no_shows, compute_convolutions, calculate_objective_serv_time_lookup\n\n# For demonstration purposes, we use a trivial convolution dictionary.\noriginal_distribution = [0.0, 0.5, 0.3, 0.2]\nno_show_probability = 0.1\nadjusted_distribution = service_time_with_no_shows(original_distribution, no_show_probability)\nschedule_example = [2, 0, 0, 0, 0, 0, 1]\nN = sum(schedule_example)\nconvolutions_example = compute_convolutions(original_distribution, N, no_show_probability)\nd_example = 1\newt, esp = calculate_objective_serv_time_lookup(schedule_example, d_example, convolutions_example)\nprint(\"Adjusted Service Time Distribution: \", adjusted_distribution)\nprint(\"Expected Adjusted Service Time: \", np.dot(range(len(adjusted_distribution)), adjusted_distribution))\nprint(\"Expected Waiting Time:\", ewt)\nprint(\"Expected Spillover:\", esp)\n\nAdjusted Service Time Distribution:  [0.1, 0.45, 0.27, 0.18000000000000002]\nExpected Adjusted Service Time:  1.53\nExpected Waiting Time: 1.53\nExpected Spillover: 0.6300000000000001\n\n\n\nimport unittest\n\nclass TestCalculateObjectiveServTimeLookup(unittest.TestCase):\n    def setUp(self):\n        # Create a convolution dictionary\n        self.convolutions = convolutions_example\n        self.d = d_example\n\n    def test_single_time_slot(self):\n        # With one patient there will be no waiting and spillover (overtime) can be calculated by hand.\n        schedule = [1]\n        ewt, esp = calculate_objective_serv_time_lookup(schedule, self.d, self.convolutions)\n        self.assertAlmostEqual(ewt, 0.0, places=5, msg=\"Expected waiting time should be 0\")\n        self.assertAlmostEqual(esp, 0.6300000000000001, places=5, msg=\"Expected spillover should be 0\")\n\n    def test_zero_patients(self):\n        # If no patients are scheduled in a time slot, the process simply advances in time.\n        schedule = [0]\n        ewt, esp = calculate_objective_serv_time_lookup(schedule, self.d, self.convolutions)\n        self.assertAlmostEqual(ewt, 0.0, places=5, msg=\"Expected waiting time should be 0 when no patients\")\n        self.assertAlmostEqual(esp, 0.0, places=5, msg=\"Expected spillover should be 0 when no patients\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.001s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`calculate_objective_serv_time_lookup`"
    ]
  },
  {
    "objectID": "get-neighborhood.html",
    "href": "get-neighborhood.html",
    "title": "get_neighborhood",
    "section": "",
    "text": "Function Documentation\nget_neighborhood(x: Union[List[int], np.ndarray], v_star: np.ndarray, ids: List[List[int]], verbose: bool = False) -&gt; np.ndarray",
    "crumbs": [
      "Function documentation",
      "`get_neighborhood`"
    ]
  },
  {
    "objectID": "get-neighborhood.html#function-documentation",
    "href": "get-neighborhood.html#function-documentation",
    "title": "get_neighborhood",
    "section": "",
    "text": "Description\nThe get_neighborhood function computes a set of neighbor solutions by adding together selected rows from the array v_star to an initial solution vector x. The selection of rows is determined by the list of index lists ids, where each inner list represents a combination of indices. After generating the candidate neighbors, the function filters out any that contain negative values. An optional verbose flag provides debugging output during execution.\n\n\nParameters\n\nx (Union[List[int], np.ndarray]):\nThe current solution vector. Can be provided as a list of integers or as a NumPy array.\nv_star (np.ndarray):\nA 2D NumPy array where each row is an adjustment vector. These vectors are used to modify the current solution to explore its neighborhood.\nids (List[List[int]]):\nA list of index lists, where each inner list specifies which rows from v_star to sum together. Each combination represents a potential adjustment to the current solution.\nverbose (bool, optional):\nA flag indicating whether to print debugging information (e.g., intermediate computations, progress messages). Defaults to False.\n\n\n\nReturns\n\nnp.ndarray:\nA 2D NumPy array where each row is a neighbor solution (i.e., the result of adding a valid combination of adjustment vectors from v_star to x). Only neighbors with all non-negative entries are included in the output.\n\n\n\nExample\n\nimport numpy as np\nfrom functions import get_neighborhood, get_v_star, powerset\n\n# Define an initial solution vector\nx = [3, 2, 1]\n\n# Generate adjustment vectors using get_v_star\n# For instance, create a set of cyclic adjustment vectors of length 3\nv_star = get_v_star(3)\n\n# Generate combinations of indices (e.g., using a powerset for switching 1 patient)\nids = powerset(range(3), size=1)\n\n# Generate the neighborhood (neighbors with non-negative entries only)\nneighbors = get_neighborhood(x, v_star, ids, echo=True)\nprint(\"Neighbor solutions:\")\nprint(neighbors)\n\nPrinting every 50th result\nv_star[0]: [-1  0  1]\nx, x', delta:\n[3 2 1],\n[2 2 2],\n[-1  0  1]\n-----------------\nv_star[1]: [ 1 -1  0]\nv_star[2]: [ 0  1 -1]\nSize of raw neighborhood: 3\nFiltered out: 0 schedules with negative values.\nSize of filtered neighborhood: 3\nNeighbor solutions:\n[[2 2 2]\n [4 1 1]\n [3 3 0]]\n\n\n\nimport unittest\nimport numpy as np\nfrom functions import get_neighborhood, get_v_star, powerset\n\nclass TestGetNeighborhood(unittest.TestCase):\n    def test_non_negative_neighbors(self):\n        # Test with a simple solution vector and adjustment vectors\n        x = [3, 2, 1]\n        v_star = get_v_star(3)\n        ids = powerset(range(3), size=1)\n        \n        neighbors = get_neighborhood(x, v_star, ids, echo=False)\n        \n        # Ensure that no neighbor has negative entries\n        self.assertTrue(np.all(neighbors &gt;= 0), \"Some neighbor solutions contain negative values\")\n    \n    def test_neighborhood_shape(self):\n        # Test that the neighborhood returns a NumPy array with the proper dimensions\n        x = [3, 2, 1]\n        v_star = get_v_star(3)\n        ids = powerset(range(3), size=1)\n        neighbors = get_neighborhood(x, v_star, ids, echo=False)\n        self.assertIsInstance(neighbors, np.ndarray, \"Neighborhood is not a NumPy array\")\n        # The number of rows should equal the number of valid combinations in ids (after filtering negatives)\n        self.assertLessEqual(neighbors.shape[0], len(ids), \"Neighborhood size is larger than expected\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.002s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`get_neighborhood`"
    ]
  },
  {
    "objectID": "local-search.html",
    "href": "local-search.html",
    "title": "local_search",
    "section": "",
    "text": "Function Documentation\nlocal_search(x: Union[List[int], np.ndarray], d: int, convolutions: Dict[int, np.ndarray], w: float, v_star: np.ndarray, size: int = 2, echo: bool = False) -&gt; Tuple[np.ndarray, float]",
    "crumbs": [
      "Function documentation",
      "`local_search`"
    ]
  },
  {
    "objectID": "local-search.html#function-documentation",
    "href": "local-search.html#function-documentation",
    "title": "local_search",
    "section": "",
    "text": "Description\nThe local_search function optimizes a schedule by iteratively exploring its neighborhood. Starting with an initial solution x, the function computes its objective value using the precomputed convolutions of the service time probability mass function. The neighborhood is generated by combining adjustment vectors from v_star (using a powerset-based approach) and filtering out candidates that contain negative values. The search continues until no further improvement is found for neighborhoods up to the specified size. The objective function combines expected average waiting time per patient and spillover time weighted by w.\n\n\nParameters\n\nx (Union[List[int], np.ndarray]):\nThe initial solution vector representing the schedule. It can be provided as a list of integers or as a NumPy array.\nd (int):\nThe duration threshold for a time slot. It is used to adjust the service process and waiting time distribution.\nconvolutions (Dict[int, np.ndarray]):\nA dictionary containing precomputed convolutions of the service time PMF. The key 1 represents the adjusted service time distribution, and other keys represent the convolution for the corresponding number of scheduled patients.\nw (float):\nThe weighting factor for combining the two performance objectives: expected waiting time and expected spillover time.\nv_star (np.ndarray):\nA 2D NumPy array of adjustment vectors. Each row in v_star is used to modify the current solution vector in order to generate its neighborhood.\nsize (int, optional):\nThe maximum number of patients to switch (i.e., the size of the neighborhood to explore) during the local search. Defaults to 2.\necho (bool, optional):\nA flag that, when set to True, prints progress and debugging messages during the search process. Defaults to False.\n\n\n\nReturns\n\nTuple[np.ndarray, float]:\nA tuple containing:\n\nThe best solution found as a 1D NumPy array.\nThe corresponding cost (objective value) as a float.\n\n\n\n\nExample\n\nimport numpy as np\nfrom functions import local_search, calculate_objective_serv_time_lookup, compute_convolutions, get_v_star, powerset\n\nfrom typing import List, Dict, Tuple, Union\n\ndef ways_to_distribute(N: int, T: int) -&gt; List[List[int]]:\n    \"\"\"\n    Compute all possible ways to distribute N identical items into T bins.\n    \n    Each distribution is represented as a list of T nonnegative integers whose sum is N.\n    \n    Parameters:\n        N (int): Total number of identical items.\n        T (int): Number of bins.\n        \n    Returns:\n        List[List[int]]: A list of distributions. Each distribution is a list of T integers that sum to N.\n        \n    Example:\n        &gt;&gt;&gt; ways_to_distribute(3, 2)\n        [[0, 3], [1, 2], [2, 1], [3, 0]]\n    \"\"\"\n    # Base case: only one bin left, all items must go into it.\n    if T == 1:\n        return [[N]]\n    \n    distributions = []\n    # Iterate over possible numbers of items in the first bin\n    for i in range(N + 1):\n        # Recursively distribute the remaining items among the remaining bins.\n        for distribution in ways_to_distribute(N - i, T - 1):\n            distributions.append([i] + distribution)\n            \n    return distributions\n  \ndef choose_best_solution(solutions: List[np.ndarray], d: int, convs: Dict[int, np.ndarray], w: float, v_star: np.ndarray) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Choose the best solution from a list of solutions based on the objective function.\n    \n    Parameters:\n        solutions (List[np.ndarray]): A list of solution vectors.\n        d (int): Duration threshold for a time slot.\n        convs (Dict[int, np.ndarray]): Precomputed convolutions of the service time PMF.\n        w (float): Weighting factor for the objective function.\n        \n    Returns:\n        Tuple[np.ndarray, float]: The best solution and its corresponding cost.\n    \"\"\"\n    best_solution = None\n    best_cost = float('inf')\n    \n    for solution in solutions:\n        waiting_time, spillover = calculate_objective_serv_time_lookup(solution, d, convs)\n        cost = w * waiting_time /N + (1 - w) * spillover\n        if cost &lt; best_cost:\n            best_solution = solution\n            best_cost = cost\n            \n    return np.array(best_solution), best_cost\n\n# Example schedule: initial solution vector\nx_initial = [3, 2, 1, 0]\nT = len(x_initial)\nN = sum(x_initial)\n\n# Duration threshold for a time slot\nd = 5\n\n# Example probability mass function and no-show probability\nservice_time = np.zeros(11)\nservice_time[3] = 0.2\nservice_time[5] = 0.3\nservice_time[8] = 0.5\nq = 0.1\n\n# Compute convolutions (precomputed service time distributions)\nconvs = compute_convolutions(service_time, N=N, q=q)\n\n# Weighting factor for the objective function\nw = 0.5\n\n# Generate adjustment vectors for the schedule (v_star)\nv_star = get_v_star(len(x_initial))\n\n# Perform local search to optimize the schedule\nbest_solution, best_cost = local_search(x_initial, d, convs, w, v_star, size=T, echo=True)\n\nprint(\"Best Solution:\", best_solution)\nprint(\"Best Cost:\", best_cost)\n\nInitial solution: [3 2 1 0], cost: 37.71594467672401\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 3\nFound better solution: [2 2 1 1], cost: 30.52386358592401\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 4\nFound better solution: [1 2 1 2], cost: 25.53066071370001\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 4\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 6\nFound better solution: [1 1 1 3], cost: 22.474543162500005\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 4\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 6\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 4\nBest Solution: [1 1 1 3]\nBest Cost: 22.474543162500005\n\n\n\nimport unittest\nimport numpy as np\nfrom functions import local_search, compute_convolutions, get_v_star\n\nclass TestLocalSearch(unittest.TestCase):\n    def test_local_search_improvement(self):\n        # Set up a simple test with a known schedule and parameters\n        x_initial = [3, 2, 1, 0]\n        T = len(x_initial)\n        N = sum(x_initial)\n        d = 5\n        service_time = np.zeros(11)\n        service_time[3] = 0.2\n        service_time[5] = 0.3\n        service_time[8] = 0.5\n        q = 0.1\n        convs = compute_convolutions(service_time, N=N, q=q)\n        w = 0.5\n        v_star = get_v_star(len(x_initial))\n        \n        # Perform local search\n        best_solution, best_cost = local_search(x_initial, d, convs, w, v_star, size=T, echo=False)\n        print(\"Best Solution:\", best_solution, \"Best Cost:\", best_cost)\n        \n        # Iterate over all solutions and choose best solution\n        solutions = ways_to_distribute(N, T)\n        best_solution_brute, best_cost_brute = choose_best_solution(solutions, d, convs, w, v_star)\n        print(\"Best Brute-force Solution:\", best_solution_brute, \"Best Brute-force Cost:\", best_cost_brute)\n        \n        # Verify that the local search solution is equal to the brute-force solution\n        self.assertTrue(np.array_equal(best_solution, best_solution_brute), \"The local search solution should match the brute-force solution.\")\n        \n        # Verify that the returned solution has the same length as the initial schedule\n        self.assertEqual(len(best_solution), len(x_initial), \"The optimized solution should have the same length as the initial solution.\")\n        \n        # Check that the cost is a float and that a solution is returned\n        self.assertIsInstance(best_cost, float, \"Cost should be a float value.\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\nF\n======================================================================\nFAIL: test_local_search_improvement (__main__.TestLocalSearch.test_local_search_improvement)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/var/folders/gf/gtt1mww524x0q33rqlwsmjw80000gn/T/ipykernel_67823/3482521855.py\", line 31, in test_local_search_improvement\n    self.assertTrue(np.array_equal(best_solution, best_solution_brute), \"The local search solution should match the brute-force solution.\")\nAssertionError: False is not true : The local search solution should match the brute-force solution.\n\n----------------------------------------------------------------------\nRan 1 test in 0.014s\n\nFAILED (failures=1)\n\n\nInitial solution: [3 2 1 0], cost: 37.71594467672401\nBest Solution: [1 1 1 3] Best Cost: 22.474543162500005\nBest Brute-force Solution: [2 1 1 2] Best Brute-force Cost: 9.894705450000002",
    "crumbs": [
      "Function documentation",
      "`local_search`"
    ]
  }
]