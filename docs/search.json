[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Appointment Scheduling Experiments",
    "section": "",
    "text": "To Do",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#to-do",
    "href": "index.html#to-do",
    "title": "Appointment Scheduling Experiments",
    "section": "",
    "text": "Add a brief introduction to the project\nEvaluator Functions\n\nDocument and test the evaluator functions\nRun experiments using the evaluator functions\n\nSearcher Functions\n\nDocument and test the search functions\nRun experiments using the search functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "function-testing.html",
    "href": "function-testing.html",
    "title": "2  Evaluator functions testing",
    "section": "",
    "text": "2.1 Objective\nIn this experiment we will test whether the functions for calculating the objective values work properly and efficiently.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#background",
    "href": "function-testing.html#background",
    "title": "2  Evaluator functions testing",
    "section": "2.2 Background",
    "text": "2.2 Background\nFor developing new methods for optimizing appointment schedules it is necessary that the function for calculating objective values works properly. It is also important that the function is efficient, as it will be used in optimization algorithms that will be run many times.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#hypothesis",
    "href": "function-testing.html#hypothesis",
    "title": "2  Evaluator functions testing",
    "section": "2.3 Hypothesis",
    "text": "2.3 Hypothesis\nThe functions for calculating that have been developed are working fast and generate correct results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#methodology",
    "href": "function-testing.html#methodology",
    "title": "2  Evaluator functions testing",
    "section": "2.4 Methodology",
    "text": "2.4 Methodology\n\n2.4.1 Tools and Materials\nFor testing the correct working of the functions used to calculate objective values we will compare the exact calculation to results from Monte Carlo (MC) simulations. The MC simulations allow modeling the system and replicating closely the actual process of patients arriving and being served. The exact calculation is based on the convolution of the service time distribution and the number of patients arriving at each time slot.\n\n\n2.4.2 Experimental Design\nWe will define some typical instances of schedules and calculate the objective values for them both using the exact method as well as through MC simulations. We will then compare the results.\n\n\n2.4.3 Variables\n\nIndependent Variables:\n\nDifferent instances of appointment schedules.\n\nDependent Variables:\n\nObjective value results from exact calculations and simulations.\nSpeed indicators\n\n\n\n\n2.4.4 Setup\nWe have defined the following test cases:\n\nimport numpy as np\nimport pandas as pd\nimport time\nimport plotly.graph_objects as go\nfrom functions import service_time_with_no_shows, compute_convolutions, compute_convolutions_fft, calculate_objective_serv_time_lookup\n\n# Parameters\nd = 5\nq = 0.1\n    \n# Create service time distribution\nservice_time = np.zeros(11)\nservice_time[3] = 0.2\nservice_time[5] = 0.3\nservice_time[8] = 0.5\n\naverage_service_time = np.dot(range(len(service_time)), service_time)\nprint(f\"Average service time: {average_service_time}\")\n    \n# Different schedule patterns with the same total number of patients (except for test schedule)\nschedules = [\n    (\"Calibration\", [2, 0, 0, 0, 0, 1]),\n    (\"Uniform\", [2, 2, 2, 2]),\n    (\"Decreasing\", [5, 2, 1, 0]),\n    (\"Increasing\", [0, 1, 2, 5]),\n    (\"Front-heavy\", [4, 4, 0, 0]),\n    (\"Back-heavy\", [0, 0, 4, 4]),\n    (\"Alternating\", [4, 0, 4, 0]),\n    (\"Bailey-rule\", [2, 1, 1, 1, 1, 1, 1])  # Schedule 2 initially, then 1 each slot\n]\n\n# Set number of simulations for Monte Carlo simulation\nnr_simulations = 1000\n\n# Create dictionary for storing results\nresults_dict = {'schedule_name': [], 'average_waiting_time': [], 'average_overtime': [], 'expected_waiting_time': [], 'expected_overtime': [], 'average_computation_time': []}\nresults_dict['schedule_name'] = [s[0] for s in schedules]\n\nAverage service time: 6.1\n\n\nThe “Calibration” test set is used to calibrate the simulation results with the exact results. For this schedule, the exact results can be easily calculated by hand. The average service time after correcting for no-shows for one patient is 5.49 (see below) and only the second patient in this example will experience waiting times. So the average expected waiting time is 5.49 / 3 = 1.83.\nThe expected overtime is is the expected spillover time caused by the last patient in the schedule. As there are no patients before the last patient in the last interval, the spillover time distribution is simply distribution of the event that the (adjusted) service time will exceed the interval time. For the case that overtime is 3 (8 - 5), the probability is 0.45 and for other values it is 0.55. So the expected overtime is 0.45 * 3 + 0.55 * 0 = 1.35.\nThe other test sets are used to examine the performance of the functions for different schedule patterns.\n\n\n2.4.5 Sample Size and Selection\nSample Size: - For each schedule instance we will run 1000 simulations.\nSample Selection: - During each simulation for each patient a random service time will be sampled from the distribution (adjusted for no-shows).\n\n\n2.4.6 Experimental Procedure\n\n2.4.6.1 Step 1: Adjust the service time distribution for no-shows.\n\n# Adjust service time distribution for no-shows and compare to original\nservice_time_no_shows = service_time_with_no_shows(service_time, q)\nprint(f\"Service time distribution with no-shows: {service_time_no_shows}\")\n\naverage_service_time_no_shows = np.dot(range(len(service_time_no_shows)), service_time_no_shows)\nprint(f\"Average service time with no-shows: {average_service_time_no_shows}\")\n\nService time distribution with no-shows: [0.1, 0.0, 0.0, 0.18000000000000002, 0.0, 0.27, 0.0, 0.0, 0.45, 0.0, 0.0]\nAverage service time with no-shows: 5.49\n\n\n\n\n2.4.6.2 Step 2: Monte carlo simulation\nFor each schedule instance:\n\nCalculate \\(N\\) and \\(T\\), the total number of patients and the total time of the schedule.\nFor each simulation:\n\nSample random service times for each of the \\(N\\) patient from service times distribution with no-shows.\nCalculate the the average waiting time and the overtime for the schedule using a Lindley recursion, starting at \\(t = 0\\) and ending at \\(t = T - 1\\).\n\n\n\nimport numpy as np\nfrom typing import List, Tuple, Union\n\ndef simulate_schedule(\n    schedule: List[int],\n    service_time_no_shows: Union[List[float], np.ndarray],\n    d: int,\n    nr_simulations: int\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Runs a Monte Carlo simulation for a single schedule.\n\n    This function simulates patient scheduling over multiple time slots and computes the average waiting \n    time per patient and the average overtime across all simulation iterations. Each time slot has a duration \n    threshold `d`. Service times for patients are sampled based on the provided probability mass function.\n\n    Parameters:\n        schedule (List[int]): A list where each element represents the number of patients scheduled in each time slot.\n        service_time_no_shows (Union[List[float], np.ndarray]): A probability mass function (PMF) for service times.\n        d (int): The duration threshold for a time slot.\n        nr_simulations (int): The number of simulation iterations to run.\n\n    Returns:\n        Tuple[float, float]: A tuple containing:\n            - The average waiting time per patient across simulations.\n            - The average overtime across simulations.\n    \"\"\"\n    N: int = sum(schedule)  # Total number of patients\n    T: int = len(schedule)  # Total number of time slots\n\n    total_waiting_time: float = 0.0\n    total_overtime: float = 0.0\n\n    for _ in range(nr_simulations):\n        cum_waiting_time: float = 0.0\n\n        # --- Process the first time slot ---\n        num_patients: int = schedule[0]\n        # Generate random service times for the first slot\n        sampled: np.ndarray = np.random.choice(\n            range(len(service_time_no_shows)),\n            size=num_patients,\n            p=service_time_no_shows\n        )\n\n        if num_patients == 0:\n            waiting_time: float = 0.0\n            spillover_time: float = 0.0\n        elif num_patients == 1:\n            waiting_time = 0.0\n            spillover_time = max(0, sampled[0])\n        else:\n            # For more than one patient, the waiting time is the cumulative sum\n            # of the service times for all but the last patient.\n            waiting_time = float(sum(np.cumsum(sampled[:-1])))\n            spillover_time = max(0, sum(sampled) - d)\n        cum_waiting_time += waiting_time\n\n        # --- Process the remaining time slots ---\n        for t in range(1, T):\n            num_patients = schedule[t]\n            # Generate random service times for time slot t\n            sampled = np.random.choice(\n                range(len(service_time_no_shows)),\n                size=num_patients,\n                p=service_time_no_shows\n            )\n            if num_patients == 0:\n                waiting_time = 0.0\n                spillover_time = max(0, spillover_time - d)\n            elif num_patients == 1:\n                waiting_time = spillover_time\n                spillover_time = max(0, spillover_time + sampled[0] - d)\n            else:\n                # Each patient waits the current spillover,\n                # plus additional waiting due to the service times of those ahead.\n                waiting_time = spillover_time * num_patients + sum(np.cumsum(sampled[:-1]))\n                spillover_time = max(0, spillover_time + sum(sampled) - d)\n            cum_waiting_time += waiting_time\n\n        # Accumulate normalized waiting time (per patient) and overtime\n        total_waiting_time += cum_waiting_time / N\n        total_overtime += spillover_time\n\n    avg_waiting_time: float = total_waiting_time / nr_simulations\n    avg_overtime: float = total_overtime / nr_simulations\n\n    return avg_waiting_time, avg_overtime\n\n\n# Loop through the schedules\nfor schedule_name, schedule in schedules:\n    N = sum(schedule)\n    T = len(schedule)\n    print(f\"Schedule: {schedule_name} {schedule}, N: {N}, T: {T}\")\n    \n    avg_waiting_time, avg_overtime = simulate_schedule(schedule, service_time_no_shows, d, nr_simulations)\n    \n    print(f\"Average waiting time: {avg_waiting_time}, average overtime: {avg_overtime}\")\n    results_dict['average_waiting_time'].append(avg_waiting_time)\n    results_dict['average_overtime'].append(avg_overtime)\n\nSchedule: Calibration [2, 0, 0, 0, 0, 1], N: 3, T: 6\nAverage waiting time: 1.8586666666666853, average overtime: 1.383\nSchedule: Uniform [2, 2, 2, 2], N: 8, T: 4\nAverage waiting time: 11.778125, average overtime: 23.936\nSchedule: Decreasing [5, 2, 1, 0], N: 8, T: 4\nAverage waiting time: 16.84875, average overtime: 24.423\nSchedule: Increasing [0, 1, 2, 5], N: 8, T: 4\nAverage waiting time: 12.573375, average overtime: 29.933\nSchedule: Front-heavy [4, 4, 0, 0], N: 8, T: 4\nAverage waiting time: 16.5705, average overtime: 23.602\nSchedule: Back-heavy [0, 0, 4, 4], N: 8, T: 4\nAverage waiting time: 16.546, average overtime: 33.711\nSchedule: Alternating [4, 0, 4, 0], N: 8, T: 4\nAverage waiting time: 14.32475, average overtime: 23.923\nSchedule: Bailey-rule [2, 1, 1, 1, 1, 1, 1], N: 8, T: 7\nAverage waiting time: 6.356375, average overtime: 9.496\n\n\n\n\n2.4.6.3 Step 3: Exact calculation\nFor each schedule instance run 10 evaluations of the objective value using the exact method and calculate the average waiting time and overtime.\n\n# Loop through the schedules, run 10 evaluations, calculate average waiting time and overtime for each schedule, calculate average computation times and store the results in the results dictionary\n\nfor schedule_name, schedule in schedules:\n    N = sum(schedule)\n    T = len(schedule)\n    print(f\"Schedule: {schedule_name} {schedule}, N: {N}, T: {T}\")\n    convolutions = compute_convolutions(service_time, N, q)\n    \n    total_time = 0\n    # Exact calculation over 10 evaluations\n    for i in range(10):\n        start_time = time.time()\n        # Calculate the objective value using the exact method\n        waiting_time, overtime = calculate_objective_serv_time_lookup(schedule, d, convolutions)\n        elapsed_time = time.time() - start_time\n        total_time += elapsed_time\n        \n    avg_time = total_time / 10\n    print(f\"Expected waiting time: {waiting_time / N}, Expected overtime: {overtime}\")\n    results_dict['expected_waiting_time'].append(waiting_time / N)\n    results_dict['expected_overtime'].append(overtime)\n    results_dict['average_computation_time'].append(avg_time)\n\nSchedule: Calibration [2, 0, 0, 0, 0, 1], N: 3, T: 6\nExpected waiting time: 1.83, Expected overtime: 1.35\nSchedule: Uniform [2, 2, 2, 2], N: 8, T: 4\nExpected waiting time: 11.816608810500004, Expected overtime: 24.064827562971374\nSchedule: Decreasing [5, 2, 1, 0], N: 8, T: 4\nExpected waiting time: 16.715096633250006, Expected overtime: 23.92331748953545\nSchedule: Increasing [0, 1, 2, 5], N: 8, T: 4\nExpected waiting time: 12.5150625, Expected overtime: 29.856120798600017\nSchedule: Front-heavy [4, 4, 0, 0], N: 8, T: 4\nExpected waiting time: 16.715970000000002, Expected overtime: 23.92482686022145\nSchedule: Back-heavy [0, 0, 4, 4], N: 8, T: 4\nExpected waiting time: 16.715970000000002, Expected overtime: 33.92194762296002\nSchedule: Alternating [4, 0, 4, 0], N: 8, T: 4\nExpected waiting time: 14.233406400000002, Expected overtime: 23.95841024194273\nSchedule: Bailey-rule [2, 1, 1, 1, 1, 1, 1], N: 8, T: 7\nExpected waiting time: 6.439653759761102, Expected overtime: 9.820826086143853",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#results",
    "href": "function-testing.html#results",
    "title": "2  Evaluator functions testing",
    "section": "2.5 Results",
    "text": "2.5 Results\nComparison of the results of the exact calculations with the results of the Monte Carlo simulations.\n\ndf_results = pd.DataFrame.from_dict(results_dict)\ndf_results\n\n\n\n\n\n\n\n\n\nschedule_name\naverage_waiting_time\naverage_overtime\nexpected_waiting_time\nexpected_overtime\naverage_computation_time\n\n\n\n\n0\nCalibration\n1.858667\n1.383\n1.830000\n1.350000\n0.000183\n\n\n1\nUniform\n11.778125\n23.936\n11.816609\n24.064828\n0.000217\n\n\n2\nDecreasing\n16.848750\n24.423\n16.715097\n23.923317\n0.000189\n\n\n3\nIncreasing\n12.573375\n29.933\n12.515063\n29.856121\n0.000136\n\n\n4\nFront-heavy\n16.570500\n23.602\n16.715970\n23.924827\n0.000058\n\n\n5\nBack-heavy\n16.546000\n33.711\n16.715970\n33.921948\n0.000056\n\n\n6\nAlternating\n14.324750\n23.923\n14.233406\n23.958410\n0.000055\n\n\n7\nBailey-rule\n6.356375\n9.496\n6.439654\n9.820826\n0.000123\n\n\n\n\n\n\n\n\n\n# Extract schedule names from the dataframe\nschedule_names = df_results['schedule_name'].tolist()\n\n# Create new x-values for simulation and exact results\nx_sim = [f\"{s}&lt;br&gt;Simulation\" for s in schedule_names]\nx_exact = [f\"{s}&lt;br&gt;Exact\" for s in schedule_names]\n\n# Extract values from the dataframe\nsim_wait = df_results['average_waiting_time'].tolist()\nsim_over = df_results['average_overtime'].tolist()\nexact_wait = df_results['expected_waiting_time'].tolist()\nexact_over = df_results['expected_overtime'].tolist()\n\n# Create a combined category list with an empty category between the two groups\ncategories = x_sim + [\"\"] + x_exact\n\n# Create the figure\nfig = go.Figure()\n\n# Simulation bar traces (stacked)\nfig.add_trace(go.Bar(\n    x=x_sim,\n    y=sim_wait,\n    name='Waiting Time',\n    marker_color='blue'\n))\nfig.add_trace(go.Bar(\n    x=x_sim,\n    y=sim_over,\n    name='Overtime',\n    marker_color='red'\n))\n\n# Exact bar traces (stacked)\nfig.add_trace(go.Bar(\n    x=x_exact,\n    y=exact_wait,\n    name='Waiting Time',\n    marker_color='blue',\n    showlegend=False  # legend already shown for waiting time above\n))\nfig.add_trace(go.Bar(\n    x=x_exact,\n    y=exact_over,\n    name='Overtime',\n    marker_color='red',\n    showlegend=False  # legend already shown for overtime above\n))\n\n# Update x-axis to use the full category array (which includes the gap)\nfig.update_xaxes(\n    tickangle=45,\n    categoryorder='array',\n    categoryarray=categories\n)\n\n# Optionally, adjust the vertical dotted line.\n# For example, you can remove it if the gap is sufficient or reposition it.\nfig.update_layout(\n    title=\"Comparison of Simulation vs. Exact Results\",\n    xaxis_title=\"Schedule Type\",\n    yaxis_title=\"Time\",\n    barmode='stack',\n    shapes=[\n        dict(\n            type=\"line\",\n            xref=\"paper\", x0=0.5, x1=0.5,\n            yref=\"paper\", y0=0, y1=1,\n            line=dict(\n                color=\"black\",\n                width=2,\n                dash=\"dot\"\n            )\n        )\n    ]\n)\n\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#discussion",
    "href": "function-testing.html#discussion",
    "title": "2  Evaluator functions testing",
    "section": "2.6 Discussion",
    "text": "2.6 Discussion\nThe results show that the exact calculations and the Monte Carlo simulations are in good agreement. The average waiting times and overtimes are very close for both methods. The computation times for the exact calculations are also reasonable, indicating that the functions are efficient - at least for these limited instances.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "function-testing.html#timeline",
    "href": "function-testing.html#timeline",
    "title": "2  Evaluator functions testing",
    "section": "2.7 Timeline",
    "text": "2.7 Timeline\nThis experiment has been started on 07-03-2025 and is expected to be finished on 14-03-2025.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluator functions testing</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html",
    "href": "xgboost-pairwise-ranking-large.html",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "",
    "text": "3.1 Objective\nObjective: Testing the performance of an XGBoost model trained for ranking pairwise schedules.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#background",
    "href": "xgboost-pairwise-ranking-large.html#background",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.2 Background",
    "text": "3.2 Background\nIn this experiment we develop a Machine Learning model using XGBoost that can evaluate two neighboring schedules and rank them according to preference. This ranking model can be applied to quickly guide the search process towards a ‘good enough’ solution.\nThe choice of using an ordinal model instead of a cardinal model is based on the consideration that it is significantly easier to determine whether alternative A is superior to B than to quantify the exact difference between A and B. This makes intuitive sense when considering the scenario of holding two identical-looking packages and deciding which one is heavier, as opposed to estimating the precise weight difference between them. (ho_ordinal_2000?).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#hypothesis",
    "href": "xgboost-pairwise-ranking-large.html#hypothesis",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.3 Hypothesis",
    "text": "3.3 Hypothesis\nAn XGBoost ranking model achieves superior computational efficiency compared to evaluating each element of a pair individually, leading to faster overall performance in ranking tasks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#methodology",
    "href": "xgboost-pairwise-ranking-large.html#methodology",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.4 Methodology",
    "text": "3.4 Methodology\n\n3.4.1 Tools and Materials\nWe use packages from Scikit-learn to prepare training data and evaluate the model and the XGBClassifier interface from the XGBoost library.\n\nimport time\nimport math\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.base import clone\nimport xgboost as xgb\nfrom xgboost.callback import TrainingCallback\nimport plotly.graph_objects as go\nimport pickle\nimport random\nfrom scipy.optimize import minimize\nfrom itertools import combinations\n\n\n\n3.4.2 Experimental Design\nTo compare an XGBoost Machine Learning model with a simple evaluation of each individual element of the pair, we will use a pairwise ranking approach. The objective is to rank two neighboring schedules according to preference.\n\nfrom functions import compute_convolutions\n\nN = 22 # Number of patients\nT = 20 # Number of intervals\nd = 5 # Length of each interval\nmax_s = 20 # Maximum service time\nq = 0.20 # Probability of a scheduled patient not showing up\nw = 0.1 # Weight for the waiting time in objective function\nl = 10\nnum_schedules = 100000 # Number of schedules to sample\n\n# Create service time distribution\ndef generate_weighted_list(max_s, l, i):\n    # Initialize an array of T+1 values, starting with zero\n    values = np.zeros(T + 1)\n    \n    # Objective function: Sum of squared differences between current weighted average and the desired l\n    def objective(x):\n        weighted_avg = np.dot(np.arange(1, T + 1), x) / np.sum(x)\n        return (weighted_avg - l) ** 2\n\n    # Constraint: The sum of the values from index 1 to T must be 1\n    constraints = ({\n        'type': 'eq',\n        'fun': lambda x: np.sum(x) - 1\n    })\n    \n    # Bounds: Each value should be between 0 and 1\n    bounds = [(0, 1)] * T\n\n    # Initial guess: Random distribution that sums to 1\n    initial_guess = np.random.dirichlet(np.ones(T))\n\n    # Optimization: Minimize the objective function subject to the sum and bounds constraints\n    result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n\n    # Set the values in the array (index 0 remains 0)\n    values[1:] = result.x\n\n    # Now we need to reorder the values as per the new requirement\n    first_part = np.sort(values[1:i+1])  # Sort the first 'i' values in ascending order\n    second_part = np.sort(values[i+1:])[::-1]  # Sort the remaining 'T-i' values in descending order\n    \n    # Combine the sorted parts back together\n    values[1:i+1] = first_part\n    values[i+1:] = second_part\n    \n    return values\n\ni = 5  # First 5 highest values in ascending order, rest in descending order\ns = generate_weighted_list(max_s, l, i)\nprint(s)\nprint(\"Sum:\", np.sum(s[1:]))  # This should be 1\nprint(\"Weighted service time:\", np.dot(np.arange(1, T + 1), s[1:]))  # This should be close to l\n\nconvolutions = compute_convolutions(s, N, q)\nfile_path_parameters = f\"datasets/parameters_{N}_{T}_{l}.pkl\"\nwith open(file_path_parameters, 'wb') as f:\n    pickle.dump({\n      'N': N,\n      'T': T,\n      'd': d,\n      'max_s': max_s,\n      'q': q,\n      'w': w,\n      'l': l,\n      'num_schedules': num_schedules,\n      'convolutions': convolutions\n      }, f)\n    print(f\"Data saved successfully to '{file_path_parameters}'\")\n\n[0.         0.00472961 0.00547349 0.02376334 0.07811626 0.21629653\n 0.18745622 0.07728913 0.06585553 0.05433682 0.04484383 0.03761993\n 0.03287155 0.03238309 0.03088835 0.02729436 0.02370938 0.02034264\n 0.01849632 0.0171894  0.00104421]\nSum: 1.0000000000605846\nWeighted service time: 8.087691274725234\nData saved successfully to 'datasets/parameters_22_20_10.pkl'\n\n\nWe will create a random set of pairs of neighboring schedules with \\(N = 22\\) patients and \\(T = 20\\) intervals of length \\(d = 5\\).\nA neighbor of a schedule x is considered a schedule x’ where single patients have been shifted one interval to the left. Eg: ([2,1,1,2], [1,2,0,3]) are neighbors and ([2,1,1,2], [2,1,3,0]) are not, because [1,2,0,3] - [2,1,1,2] = [-1, 1, -1, 1] and [2,1,3,0] - [2,1,1,2] = [0, 0, 2, -2].\nService times will have a discrete distribution. The probability of a scheduled patient not showing up will be \\(q = 0.2\\).\nThe objective function will be the weighted average of the total waiting time of all patients and overtime. The model will be trained to predict which of the two neighboring schedules has the lowest objective value. The prediction time will be recorded. Then the same schedules will be evaluated by computing the objective value and then ranked.\n\n\n3.4.3 Variables\n\nIndependent Variables: A list of tuples with pairs of neighboring schedules.\nDependent Variables: A list with rankings for each tuple of pairwise schedules. Eg: If the rank for ([2,1,1], [1,1,2]) equals 0 this means that the schedule with index 0 ([2,1,1]) has the lowest objective value.\n\n\n\n3.4.4 Data Collection\nThe data set will be generated using simulation in which random samples will be drawn from the population of all possible schedules. For each sample a random neighboring schedule will be created.\n\n\n3.4.5 Sample Size and Selection\nSample Size: The total population size equals \\({{N + T -1}\\choose{N}} \\approx\\) 244663.0 mln. For this experiment we will be using a relatively small sample of 100000 pairs of schedules.\nSample Selection: The samples will be drawn from a lexicographic order of possible schedules in order to accurately reflect the combinatorial nature of the problem and to ensure unbiased sampling from the entire combinatorial space.\n\n\n3.4.6 Experimental Procedure\nThe experiment involves multiple steps, beginning with data preparation and concluding with model evaluation.The diagram below illustrates the sequence of steps.\n\n\n\n\n\ngraph TD\n    A[\"From population\"] --&gt;|\"Sample\"| B[\"Random subset\"]\n    B --&gt; |Create neighbors| C[\"Features: Schedule pairs\"]\n    C --&gt; |Calculate objectives| D[\"Objective values\"]\n    D --&gt; |Rank objectives| E[\"Labels: Rankings\"]\n    E --&gt; |\"Split dataset\"| F[\"Training set\"]\n    E --&gt; |\"Split dataset\"| G[\"Test set\"]\n    F --&gt; |\"Train\"| H[\"Model\"]\n    H[\"Model\"] --&gt; |\"Apply\"| G[\"Test set\"]\n    G[\"Test set\"] --&gt; |\"Evaluate\"| I[\"Performance\"]\n\n\n\n\n\n\nStep 1: Randomly select a subset of schedules.\n\nfrom functions import create_random_schedules\n\nstart = time.time()\n# schedules = random_combination_with_replacement(T, N, num_schedules)\nschedules = create_random_schedules(T, N, num_schedules)\nprint(f\"Sampled: {len(schedules):,} schedules\\n\")\nh = random.choices(range(len(schedules)), k=7)\nprint(f\"Sampled schedules: {h}\")\nfor i in h:\n    print(f\"Schedule: {schedules[i]}\")\nend = time.time()\ndata_prep_time = end - start\n\nprint(f\"\\nProcessing time: {data_prep_time} seconds\\n\")\n\nSampled: 100,000 schedules\n\nSampled schedules: [57312, 34464, 40931, 44803, 40761, 98298, 12022]\nSchedule: [1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 2, 2, 2, 0, 2, 1, 0, 0, 2]\nSchedule: [1, 1, 3, 1, 3, 0, 2, 0, 1, 0, 1, 3, 1, 0, 1, 1, 0, 0, 2, 1]\nSchedule: [1, 1, 0, 2, 3, 0, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 0, 0, 0, 1]\nSchedule: [0, 1, 3, 1, 0, 3, 2, 2, 0, 0, 0, 0, 2, 3, 1, 0, 2, 2, 0, 0]\nSchedule: [1, 1, 2, 0, 1, 2, 1, 2, 0, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 4]\nSchedule: [0, 2, 2, 0, 3, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 1, 1, 0, 3, 1]\nSchedule: [0, 0, 0, 4, 0, 0, 1, 2, 1, 1, 1, 2, 1, 1, 3, 2, 1, 1, 1, 0]\n\nProcessing time: 0.7732117176055908 seconds\n\n\n\nStep 2: Create pairs of neighboring schedules.\n\nfrom functions import get_v_star\n\ndef create_neighbors_list(s: list[int], v_star: np.ndarray) -&gt; (list[int], list[int]):\n    \"\"\"\n    Create a set of pairs of schedules that are from the same neighborhood.\n    \n    Parameters:\n      s (list[int]): A list of integers with |s| = T and sum N.\n      v_star (np.ndarray): Precomputed vectors V* of length T.\n      \n    Returns:\n      tuple(list[int], list[int]): A pair of schedules.\n    \"\"\"\n    T = len(s)\n\n    # Precompute binomial coefficients (weights for random.choices)\n    binom_coeff = [math.comb(T, i) for i in range(1, T)]\n\n    # Choose a random value of i with the corresponding probability\n    i = random.choices(range(1, T), weights=binom_coeff)[0]\n\n    # Instead of generating the full list of combinations, sample one directly\n    j = random.sample(range(T), i)\n    \n    s_p = s.copy()\n    for k in j:\n        s_temp = np.array(s_p) + v_star[k]\n        s_temp = s_temp.astype(int)\n        if np.all(s_temp &gt;= 0):\n            s_p = s_temp.astype(int).tolist()\n        \n    return s, s_p\n\nstart = time.time()\nv_star = get_v_star(T)\nneighbors_list = [create_neighbors_list(schedule, v_star) for schedule in schedules] # This can be done in parellel to improve speed\nend = time.time()\nfor i in h:\n    original_schedule = neighbors_list[i][0]\n    neighbor_schedule = neighbors_list[i][1]\n    difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]\n    print(f\"Neighbors\\n{original_schedule}\\n{neighbor_schedule}\\n{difference}\")\ntraining_set_feat_time = end - start\nprint(f\"\\nProcessing time: {training_set_feat_time} seconds\\n\")\n\nNeighbors\n[1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 2, 2, 2, 0, 2, 1, 0, 0, 2]\n[1, 1, 1, 0, 1, 0, 2, 2, 0, 2, 0, 3, 2, 1, 0, 3, 0, 1, 0, 2]\n[0, 0, 0, 1, -1, 1, 0, -1, 1, -1, 1, -1, 0, 1, 0, -1, 1, -1, 0, 0]\nNeighbors\n[1, 1, 3, 1, 3, 0, 2, 0, 1, 0, 1, 3, 1, 0, 1, 1, 0, 0, 2, 1]\n[2, 0, 4, 1, 2, 0, 2, 1, 0, 1, 0, 3, 1, 1, 1, 0, 0, 0, 2, 1]\n[-1, 1, -1, 0, 1, 0, 0, -1, 1, -1, 1, 0, 0, -1, 0, 1, 0, 0, 0, 0]\nNeighbors\n[1, 1, 0, 2, 3, 0, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 0, 0, 0, 1]\n[0, 2, 0, 2, 2, 0, 2, 1, 2, 1, 2, 2, 1, 0, 2, 1, 0, 0, 1, 1]\n[1, -1, 0, 0, 1, 0, -1, 1, -1, 1, 0, -1, 0, 1, 0, 0, 0, 0, -1, 0]\nNeighbors\n[0, 1, 3, 1, 0, 3, 2, 2, 0, 0, 0, 0, 2, 3, 1, 0, 2, 2, 0, 0]\n[1, 0, 3, 1, 1, 3, 1, 2, 0, 0, 0, 0, 3, 3, 0, 0, 3, 1, 0, 0]\n[-1, 1, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, -1, 0, 1, 0, -1, 1, 0, 0]\nNeighbors\n[1, 1, 2, 0, 1, 2, 1, 2, 0, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 4]\n[1, 0, 3, 0, 0, 3, 0, 2, 1, 1, 0, 2, 0, 1, 0, 1, 2, 0, 0, 5]\n[0, 1, -1, 0, 1, -1, 1, 0, -1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 0, -1]\nNeighbors\n[0, 2, 2, 0, 3, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 1, 1, 0, 3, 1]\n[0, 3, 1, 0, 3, 3, 0, 1, 0, 0, 3, 1, 1, 0, 0, 1, 1, 0, 3, 1]\n[0, -1, 1, 0, 0, -1, 1, -1, 1, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0]\nNeighbors\n[0, 0, 0, 4, 0, 0, 1, 2, 1, 1, 1, 2, 1, 1, 3, 2, 1, 1, 1, 0]\n[0, 0, 0, 4, 0, 0, 2, 1, 2, 0, 2, 2, 0, 2, 3, 2, 0, 2, 0, 0]\n[0, 0, 0, 0, 0, 0, -1, 1, -1, 1, -1, 0, 1, -1, 0, 0, 1, -1, 1, 0]\n\nProcessing time: 9.192085981369019 seconds\n\n\n\nStep 3: For each schedule in each pair calculate the objective. For each pair save the index of the schedule that has the lowest objective value.\n\nfrom functions import calculate_objective_serv_time_lookup\n\nobjectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[0], d, convolutions)]\n]\nstart = time.time()\nobjectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[1], d, convolutions)]\n]\nend = time.time()\ntraining_set_lab_time = end - start\nobjectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]\nrankings = np.argmin(objectives, axis=1).tolist()\nfor i in range(5):\n    print(f\"Objectives: {objectives[i]}, Ranking: {rankings[i]}\")\n\nprint(f\"\\nProcessing time: {training_set_lab_time} seconds\\n\")\n\n# Saving neighbors_list and objectives to a pickle file\n\nfile_path_neighbors = f\"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl\"\nwith open(file_path_neighbors, 'wb') as f:\n    pickle.dump({'neighbors_list': neighbors_list, 'objectives': objectives, 'rankings': rankings}, f)\n    print(f\"Data saved successfully to '{file_path_neighbors}'\")\n\nObjectives: [97.84962176825093, 97.4040045804486], Ranking: 1\nObjectives: [103.45852094128446, 105.48723639837132], Ranking: 0\nObjectives: [106.39196263180865, 106.74202099105216], Ranking: 0\nObjectives: [112.34514063428506, 113.64501918435718], Ranking: 0\nObjectives: [95.81316011904904, 102.89636923803397], Ranking: 0\n\nProcessing time: 44.366554975509644 seconds\n\nData saved successfully to 'datasets/neighbors_and_objectives_22_20_10.pkl'\n\n\nStep 4: Create training and test sets.\n\n# Prepare the dataset\nX = []\nfor neighbors in neighbors_list:\n    X.append(neighbors[0] + neighbors[1])\n\nX = np.array(X)\ny = np.array(rankings)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 5: Train the XGBoost model.\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B[Initialize StratifiedKFold]\n    B --&gt; C[Initialize XGBClassifier]\n    C --&gt; D[Set results as empty list]\n    D --&gt; E[Loop through each split of cv split]\n    E --&gt; F[Get train and test indices]\n    F --&gt; G[Split X and y into X_train, X_test, y_train, y_test]\n    G --&gt; H[Clone the classifier]\n    H --&gt; I[Call fit_and_score function]\n    I --&gt; J[Fit the estimator]\n    J --&gt; K[Score on training set]\n    J --&gt; L[Score on test set]\n    K --&gt; M[Return estimator, train_score, test_score]\n    L --&gt; M\n    M --&gt; N[Append the results]\n    N --&gt; E\n    E --&gt; O[Loop ends]\n    O --&gt; P[Print results]\n    P --&gt; Q[End]\n\n\n\n\n\n\n\nclass CustomCallback(TrainingCallback):\n    def __init__(self, period=10):\n        self.period = period\n\n    def after_iteration(self, model, epoch, evals_log):\n        if (epoch + 1) % self.period == 0:\n            print(f\"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}\")\n        return False\n    \ndef fit_and_score(estimator, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0\n    )\n\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return estimator, train_score, test_score\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n\n# Initialize the XGBClassifier without early stopping here\n# Load the best trial parameters from a JSON file.\nwith open(\"model_params.json\", \"r\") as f:\n    model_params = json.load(f)\n    \n# Initialize the EarlyStopping callback with validation dataset\nearly_stop = xgb.callback.EarlyStopping(\n    rounds=10, metric_name='logloss', data_name='validation_0', save_best=True\n)\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n    early_stopping_rounds=9,\n    #callbacks=[CustomCallback(period=50), early_stop],\n    callbacks=[CustomCallback(period=50)],\n)\nprint(\"Params: \")\nfor key, value in model_params.items():\n    print(f\" {key}: {value}\")\n\nstart = time.time()\nresults = []\n\nfor train_idx, test_idx in cv.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    est, train_score, test_score = fit_and_score(\n        clone(clf), X_train, X_test, y_train, y_test\n    )\n    results.append((est, train_score, test_score))\nend = time.time()\ntraining_time = end - start\nprint(f\"\\nTraining time: {training_time} seconds\\n\")\n\nParams: \n max_depth: 6\n min_child_weight: 1\n gamma: 0.1\n subsample: 0.8\n colsample_bytree: 0.8\n learning_rate: 0.1\n n_estimators: 100\nEpoch 49, Evaluation log: 0.3672803153709974\nEpoch 99, Evaluation log: 0.31412757577895534\nEpoch 49, Evaluation log: 0.36687244859449564\nEpoch 99, Evaluation log: 0.31512261563866634\nEpoch 49, Evaluation log: 0.3672984418308828\nEpoch 99, Evaluation log: 0.3165740518869599\nEpoch 49, Evaluation log: 0.36681636140057816\nEpoch 99, Evaluation log: 0.31441448706467634\nEpoch 49, Evaluation log: 0.3725866555017419\nEpoch 99, Evaluation log: 0.31964048068043194\n\nTraining time: 2.835527181625366 seconds\n\n\n\nStep 6: To evaluate the performance of the XGBoost ranking model, we will use Stratified K-Fold Cross-Validation with 5 splits, ensuring each fold maintains the same class distribution as the original dataset. Using StratifiedKFold(n_splits=5, shuffle=True, random_state=94), the dataset will be divided into five folds. In each iteration, the model will be trained on four folds and evaluated on the remaining fold. A custom callback, CustomCallback(period=10), will print the evaluation log every 10 epochs.\nThe fit_and_score function will fit the model and score it on both the training and test sets, storing the results for each fold. This provides insight into the model’s performance across different subsets of the data, helps in understanding how well the model generalizes to unseen data and identifies potential overfitting or underfitting issues. The overall processing time for the cross-validation will also be recorded.\n\n# Print results\nfor i, (est, train_score, test_score) in enumerate(results):\n    print(f\"Fold {i+1} - Train Score (Accuracy): {train_score:.4f}, Test Score (Accuracy): {test_score:.4f}\")\n\nFold 1 - Train Score (Accuracy): 0.8881, Test Score (Accuracy): 0.8808\nFold 2 - Train Score (Accuracy): 0.8869, Test Score (Accuracy): 0.8791\nFold 3 - Train Score (Accuracy): 0.8871, Test Score (Accuracy): 0.8796\nFold 4 - Train Score (Accuracy): 0.8867, Test Score (Accuracy): 0.8781\nFold 5 - Train Score (Accuracy): 0.8872, Test Score (Accuracy): 0.8737\n\n\nTraining the model on the entire dataset provides a final model that has learned from all available data. Recording the training time helps in understanding the computational efficiency and scalability of the model with the given hyperparameters.\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nclf.save_model('models/classifier_large_instance.json')\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\\n\")\n\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\nTraining accuracy: 88.42%\n\n\nTraining time: 0.3803718090057373 seconds\n\n\n\n\n\n3.4.7 Validation\nGenerating test schedules and calculating their objectives and rankings allows us to create a new dataset for evaluating the model’s performance on unseen data.\n\nnum_test_schedules = 1000\n\n#test_schedules = random_combination_with_replacement(T, N, num_test_schedules)\ntest_schedules = create_random_schedules(T, N, num_test_schedules)\n\ntest_neighbors = [create_neighbors_list(test_schedule, v_star) for test_schedule in test_schedules] # This can be done in parellel to improve speed\n\nprint(f\"Sampled: {len(test_schedules)} schedules\\n\")\n\ntest_objectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[0], d, convolutions)]\n]\n# Start time measurement for the evaluation\nstart = time.time()\ntest_objectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[1], d, convolutions)]\n]\ntest_rankings = [0 if test_obj &lt; test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]\nend = time.time()\nevaluation_time = end - start\n\n# Combine the objectives for each pair for later processing\ntest_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]\n\nprint(f\"\\nEvaluation time: {evaluation_time} seconds\\n\")\n\nfor i in range(6):\n    print(f\"Neighbors: {test_neighbors[i]},\\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\\n\")\n\nSampled: 1000 schedules\n\n\nEvaluation time: 0.5026698112487793 seconds\n\nNeighbors: ([0, 2, 0, 3, 0, 2, 2, 1, 1, 0, 5, 0, 2, 0, 0, 0, 1, 2, 1, 0], [1, 1, 0, 3, 1, 2, 1, 2, 0, 1, 4, 1, 1, 0, 0, 0, 2, 1, 1, 0]),\nObjectives: [113.15229701849515, 108.78314877135188], Ranking: 1\n\nNeighbors: ([0, 2, 1, 1, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 2, 4, 0, 1, 1, 1], [0, 3, 1, 0, 0, 2, 2, 0, 1, 0, 0, 2, 1, 2, 1, 4, 0, 2, 0, 1]),\nObjectives: [97.93749394061308, 100.30846042461545], Ranking: 0\n\nNeighbors: ([0, 2, 1, 0, 0, 2, 1, 0, 2, 3, 0, 0, 1, 1, 3, 1, 1, 1, 2, 1], [0, 2, 1, 0, 1, 1, 1, 1, 1, 3, 0, 0, 1, 2, 3, 1, 1, 0, 2, 1]),\nObjectives: [100.57395749962478, 98.69466853276066], Ranking: 1\n\nNeighbors: ([3, 2, 1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 0, 1, 3, 1, 0, 1, 0], [3, 3, 0, 3, 1, 0, 3, 0, 1, 0, 0, 1, 1, 1, 0, 4, 0, 1, 0, 0]),\nObjectives: [107.42377770980353, 110.83394748469374], Ranking: 0\n\nNeighbors: ([0, 0, 1, 1, 2, 3, 1, 1, 2, 1, 0, 0, 3, 1, 0, 0, 0, 4, 1, 1], [0, 0, 1, 2, 1, 4, 0, 1, 2, 1, 0, 1, 2, 1, 0, 0, 1, 4, 0, 1]),\nObjectives: [114.96851821211455, 115.77477154166539], Ranking: 0\n\nNeighbors: ([2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 0, 2, 1, 0], [2, 1, 0, 1, 0, 1, 0, 2, 0, 1, 2, 1, 3, 1, 1, 3, 0, 1, 2, 0]),\nObjectives: [93.2717262726313, 95.89469721620854], Ranking: 0\n\n\n\nMaking predictions on new data and comparing them to the actual rankings provides an evaluation of the model’s performance in practical applications. Recording the prediction time helps in understanding the model’s efficiency during inference.\n\ninput_X = test_neighbors\nX_new = []\nfor test_neighbor in input_X:\n    X_new.append(test_neighbor[0] + test_neighbor[1])\n    \n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.004590034484863281 seconds\n\ntest_rankings = [1 0 1 0 0 0], \ny_pred = [1 0 1 0 0 1], \ny_pred_proba = \n[[0.0524953  0.9475047 ]\n [0.93004686 0.06995316]\n [0.36613423 0.6338658 ]\n [0.9602932  0.0397068 ]\n [0.8588403  0.14115971]\n [0.44058472 0.5594153 ]]\n\n\nCalculating the ambiguousness of the predicted probabilities helps in understanding the model’s confidence in its predictions. High ambiguousness indicates uncertain predictions, while low ambiguousness indicates confident predictions.\nAmbiguousness is calculated using the formula for entropy:\n\\[\nH(X) = - \\sum_{i} p(x_i) \\log_b p(x_i)\n\\]\nWhere in our case:\n\n\\(H(X)\\) is the ambiguousness of the random variable \\(X\\) - the set of probability scores for the predicted rankings,\n\\(p(x_i)\\) is probability score \\(x_i\\),\n\\(\\log_b\\) is the logarithm with base \\(b\\) (here \\(\\log_2\\) as we have two predicted values),\nThe sum is taken over all possible outcomes of \\(X\\).\n\nCalculating cumulative error rate and cumulative accuracy helps in understanding how the model’s performance evolves over the dataset.\nVisualizing the relationship between ambiguousness and error provides insights into how uncertainty in the model’s predictions correlates with its accuracy. This can help in identifying patterns and understanding the conditions under which the model performs well or poorly.\n\nfrom functions import calculate_ambiguousness\n\nerrors = np.abs(y_pred - np.array(test_rankings))\n\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9)))\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()\n\n                                                \n\n\n\n\n3.4.8 Hyperparameter Optimization\nIn the initial model the choice of hyperparameters was based on default values, examples from demo’s or trial and error. To improve the model’s performance, we applied a hyperparameter optimization technique to find the best set of hyperparameters. We used a grid search with cross-validation to find the optimal hyperparameters for the XGBoost model. The grid search was performed over a predefined set of hyperparameters, and the best hyperparameters were selected based on the model’s performance on the validation set. The best hyperparameters were then used to train the final model.\n\nfrom functions import compare_json\n\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n    \ndifferences = compare_json(model_params, best_trial_params)\n\nparams_tbl = pd.DataFrame(differences)\nparams_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)\nprint(params_tbl)\n\n                      max_depth     gamma  subsample  colsample_bytree  \\\nbase parameters               6  0.100000   0.800000          0.800000   \noptimized parameters          5  0.304548   0.781029          0.922528   \n\n                      learning_rate  n_estimators  \nbase parameters            0.100000           100  \noptimized parameters       0.239488           490  \n\n\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\n# Load the best trial parameters from a JSON file.\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\")\n\n\nTraining time: 1.481233835220337 seconds\n\nTraining accuracy: 95.32%\n\n\n\n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.004647970199584961 seconds\n\ntest_rankings = [1 0 1 0 0 0], \ny_pred = [1 0 1 0 0 1], \ny_pred_proba = \n[[0.00308669 0.9969133 ]\n [0.94755095 0.05244903]\n [0.07875067 0.92124933]\n [0.99712896 0.00287102]\n [0.8615469  0.13845316]\n [0.20537233 0.79462767]]\n\n\n\nerrors = np.abs(y_pred - np.array(test_rankings))\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors, \"Schedules\": test_neighbors, \"Objectives\": test_objectives}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n\n\n\n\n\n\n\nAmbiguousness\nError\nSchedules\nObjectives\nCumulative error rate\nCumulative accuracy\n\n\n\n\n43\n0.000096\n0\n([1, 2, 2, 2, 0, 1, 0, 0, 0, 3, 3, 1, 3, 1, 1,...\n[104.35368979701357, 111.0632656829444]\n0.0\n1.0\n\n\n695\n0.000141\n0\n([1, 0, 1, 2, 2, 2, 0, 2, 4, 0, 0, 0, 0, 0, 3,...\n[106.2571238003049, 115.98836164855177]\n0.0\n1.0\n\n\n248\n0.000142\n0\n([1, 0, 2, 1, 1, 1, 0, 1, 0, 1, 0, 4, 1, 2, 0,...\n[93.62090350895505, 105.79665995100123]\n0.0\n1.0\n\n\n203\n0.000170\n0\n([1, 2, 0, 3, 1, 1, 0, 1, 1, 0, 2, 0, 0, 2, 0,...\n[89.16363968864053, 95.7580526938525]\n0.0\n1.0\n\n\n272\n0.000182\n0\n([1, 0, 2, 1, 2, 1, 4, 1, 0, 0, 1, 0, 1, 2, 1,...\n[103.70742569097223, 117.14750457048893]\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9),\n                    customdata=df[[\"Schedules\", \"Objectives\"]],\n                    hovertemplate=\n                        \"Ambiguousness: %{x} &lt;br&gt;\" +\n                        \"Error: %{y} &lt;br&gt;\" +\n                        \"Schedules: %{customdata[0][0]} / %{customdata[0][1]} &lt;br&gt;\" +\n                        \"Objectives: %{customdata[1]} &lt;br&gt;\"\n                    ))\n                  \nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#results",
    "href": "xgboost-pairwise-ranking-large.html#results",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.5 Results",
    "text": "3.5 Results\nWe wanted to test whether an XGBoost classification model could be used to assess and rank the quality of pairs of schedules. For performance benchmarking we use the conventional calculation method utilizing Lindley recursions.\nWe trained the XGBoost ranking model with a limited set of features (schedules) and labels (objectives). The total number of possible schedules is approximately 244663.0 million. For training and evaluation, we sampled 200000 schedules and corresponding neighbors. Generating the feature and label set took a total of 54.3319 seconds, with the calculation of objective values accounting for 44.3666 seconds.\nThe model demonstrates strong and consistent performance with high accuracies both for training, testing and validation (92.0%) with good generalization and stability. Total training time for the final model was 1.4812 seconds. The evaluation of 1000 test schedules took 0.0046 seconds for the the XGBoost model and 0.5027 for the conventional method, which is an improvement of 108X.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#discussion",
    "href": "xgboost-pairwise-ranking-large.html#discussion",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.6 Discussion",
    "text": "3.6 Discussion\n\ntraining_time = round(modeling_time, 4)\nconventional_time = round(evaluation_time, 4)\nxgboost_time = round(prediction_time, 4)\n\n# Define time values for plotting\ntime_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds\n\n# Calculate evaluations for method 1\nmethod1_evaluations = np.where(time_values &gt;= training_time, (time_values - training_time) / xgboost_time * 1000, 0)\n\n# Calculate evaluations for method 2\nmethod2_evaluations = time_values / conventional_time * 1000\n\n# Create line chart\nfig = go.Figure()\n\n# Add method 1 trace\nfig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))\n\n# Add method 2 trace\nfig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))\n\n# Update layout\nfig.update_layout(\n    title=\"Speed comparison between XGBoost ranking model and conventional method\",\n    xaxis_title=\"Time (seconds)\",\n    yaxis_title=\"Number of Evaluations\",\n    legend_title=\"Methods\",\n    template=\"plotly_white\"\n)\n\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#timeline",
    "href": "xgboost-pairwise-ranking-large.html#timeline",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.7 Timeline",
    "text": "3.7 Timeline\nThis experiment was started on 26-04-2025. The expected completion date is 26-04-2025.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large.html#references",
    "href": "xgboost-pairwise-ranking-large.html#references",
    "title": "3  Large instance XGBoost classification model for pairwise ranking",
    "section": "3.8 References",
    "text": "3.8 References",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "",
    "text": "4.1 Objective\nObjective: Testing the performance of an XGBoost model trained for ranking pairwise schedules taken from a neighborhood around quasi optimal initial schedule (Bailey-Welch).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#background",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#background",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.2 Background",
    "text": "4.2 Background\nIn a previous experiment we developed a Machine Learning model using XGBoost that can evaluate two neighboring schedules and rank them according to preference. For evaluation random schedules were sampled from the full solution set.\nThe full solution set however contains many schedules that are obviously not optimal. Adding them to the training set would provide the model with rather useless knowledge. Therefore in this experiment we only sample pairs of schedules taken from within the vicinity of a ‘good’ starting point.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#hypothesis",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#hypothesis",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.3 Hypothesis",
    "text": "4.3 Hypothesis\nAn XGBoost ranking model achieves superior computational efficiency compared to evaluating each element of a pair individually, leading to faster overall performance in ranking tasks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#methodology",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#methodology",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.4 Methodology",
    "text": "4.4 Methodology\n\n4.4.1 Tools and Materials\nWe use packages from Scikit-learn to prepare training data and evaluate the model and the XGBClassifier interface from the XGBoost library.\n\nimport time\nimport math\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.base import clone\nimport xgboost as xgb\nfrom xgboost.callback import TrainingCallback\nimport plotly.graph_objects as go\nimport pickle\nimport random\nfrom scipy.optimize import minimize\nfrom itertools import combinations\n\n\n\n4.4.2 Experimental Design\nTo compare an XGBoost Machine Learning model with a simple evaluation of each individual element of the pair, we will use a pairwise ranking approach. The objective is to rank two neighboring schedules according to preference.\n\nfrom functions import compute_convolutions, bailey_welch_schedule\n\nN = 22 # Number of patients\nT = 20 # Number of intervals\nd = 5 # Length of each interval\nmax_s = 20 # Maximum service time\nq = 0.20 # Probability of a scheduled patient not showing up\nw = 0.1 # Weight for the waiting time in objective function\nl = 10\nnum_schedules = 100000 # Number of schedules to sample\n\n# Create service time distribution\ndef generate_weighted_list(max_s, l, i):\n    \"\"\"\n    Generates a service time probability distribution using optimization.\n\n    This function creates a discrete probability distribution over T possible\n    service times (from 1 to T). It uses optimization (SLSQP) to find a\n    distribution whose weighted average service time is as close as possible\n    to a target value 'l', subject to the constraint that the probabilities\n    sum to 1 and each probability is between 0 and 1.\n\n    After finding the distribution, it sorts the probabilities: the first 'i'\n    probabilities (corresponding to service times 1 to i) are sorted in\n    ascending order, and the remaining probabilities (service times i+1 to T)\n    are sorted in descending order.\n\n    Note:\n        - This function relies on a globally defined integer 'T', representing\n          the maximum service time considered (or number of probability bins).\n        - The parameter 'max_s' is accepted but not used directly within this\n          function's optimization or sorting logic as shown. It might be\n          related to how 'T' is determined externally.\n        - Requires NumPy and SciPy libraries (specifically scipy.optimize.minimize).\n\n    Args:\n        max_s (any): Maximum service time parameter (currently unused in the\n                     provided function body's core logic).\n        l (float): The target weighted average service time for the distribution.\n        i (int): The index determining the sorting split point. Probabilities\n                 for service times 1 to 'i' are sorted ascendingly, and\n                 probabilities for service times 'i+1' to 'T' are sorted\n                 descendingly. Must be between 1 and T-1 for meaningful sorting.\n\n    Returns:\n        numpy.ndarray: An array of size T+1. The first element (index 0) is 0.\n                       Elements from index 1 to T represent the calculated\n                       and sorted probability distribution, summing to 1.\n                       Returns None if optimization fails.\n    \"\"\"\n    # Initialize an array of T+1 values, starting with zero\n    # Index 0 is unused for probability, indices 1 to T hold the distribution\n    values = np.zeros(T + 1)\n\n    # --- Inner helper function for optimization ---\n    def objective(x):\n        \"\"\"Objective function: Squared difference between weighted average and target l.\"\"\"\n        # Calculate weighted average: sum(index * probability) / sum(probability)\n        # Since sum(probability) is constrained to 1, it simplifies.\n        weighted_avg = np.dot(np.arange(1, T + 1), x) # Corresponds to sum(k * P(ServiceTime=k))\n        return (weighted_avg - l) ** 2\n\n    # --- Constraints for optimization ---\n    # Constraint 1: The sum of the probabilities (x[0] to x[T-1]) must be 1\n    constraints = ({\n        'type': 'eq',\n        'fun': lambda x: np.sum(x) - 1\n    })\n\n    # Bounds: Each probability value x[k] must be between 0 and 1\n    # Creates a list of T tuples, e.g., [(0, 1), (0, 1), ..., (0, 1)]\n    bounds = [(0, 1)] * T\n\n    # Initial guess: Use Dirichlet distribution to get a random distribution that sums to 1\n    # Provides a starting point for the optimizer. np.ones(T) gives equal weights initially.\n    initial_guess = np.random.dirichlet(np.ones(T))\n\n    # --- Perform Optimization ---\n    # Minimize the objective function subject to the sum and bounds constraints\n    # using the Sequential Least Squares Programming (SLSQP) method.\n    result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n\n    # Check if optimization was successful\n    if not result.success:\n        print(f\"Warning: Optimization failed! Message: {result.message}\")\n        # Handle failure case, e.g., return None or raise an error\n        return None # Or potentially return a default distribution\n\n    # Assign the optimized probabilities (result.x) to the correct slice of the values array\n    # result.x contains the T probabilities for service times 1 to T.\n    values[1:] = result.x\n\n    # --- Reorder the values based on the index 'i' ---\n    # Ensure 'i' is within a valid range for slicing and sorting\n    if not (0 &lt; i &lt; T):\n       print(f\"Warning: Index 'i' ({i}) is outside the valid range (1 to {T-1}). Sorting might be trivial.\")\n       # Adjust i or handle as an error depending on requirements\n       i = max(1, min(i, T - 1)) # Clamp i to a safe range for demonstration\n\n    # Sort the first 'i' probabilities (indices 1 to i) in ascending order\n    first_part = np.sort(values[1:i+1])\n    # Sort the remaining 'T-i' probabilities (indices i+1 to T) in descending order\n    second_part = np.sort(values[i+1:])[::-1] # [::-1] reverses the sorted array\n\n    # Combine the sorted parts back into the 'values' array\n    values[1:i+1] = first_part\n    values[i+1:] = second_part\n\n    # Return the final array with the sorted probability distribution\n    return values\n\ni = 5  # First 5 highest values in ascending order, rest in descending order\ns = generate_weighted_list(max_s, l, i)\nprint(s)\nprint(\"Sum:\", np.sum(s[1:]))  # This should be 1\nprint(\"Weighted service time:\", np.dot(np.arange(len(s)), s))  # This should be close to l\ninitial_x = bailey_welch_schedule(T, d, N, s)\nprint(f\"Initial schedule: {initial_x}\")\nconvolutions = compute_convolutions(s, N, q)\nfile_path_parameters = f\"datasets/parameters_{N}_{T}_{l}.pkl\"\nwith open(file_path_parameters, 'wb') as f:\n    pickle.dump({\n      'N': N,\n      'T': T,\n      'd': d,\n      'max_s': max_s,\n      'q': q,\n      'w': w,\n      'l': l,\n      'num_schedules': num_schedules,\n      'convolutions': convolutions\n      }, f)\n    print(f\"Data saved successfully to '{file_path_parameters}'\")\n\n[0.         0.013441   0.04953763 0.05804027 0.07608704 0.14082563\n 0.10378238 0.10027031 0.07332262 0.06789071 0.0672376  0.0670405\n 0.06391841 0.04989426 0.02620283 0.01148471 0.00946419 0.00746621\n 0.00501735 0.00496714 0.00410921]\nSum: 0.9999999999999999\nWeighted service time: 7.7270999155687266\nInitial schedule: [2, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 8]\nData saved successfully to 'datasets/parameters_22_20_10.pkl'\n\n\nWe will create a random set of pairs of neighboring schedules from within the neighborhood around schedule [2, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 8].\nA neighbor of a schedule x is considered a schedule x’ where single patients have been shifted one interval to the left. Eg: ([2,1,1,2], [1,2,0,3]) are neighbors and ([2,1,1,2], [2,1,3,0]) are not, because [1,2,0,3] - [2,1,1,2] = [-1, 1, -1, 1] and [2,1,3,0] - [2,1,1,2] = [0, 0, 2, -2].\nService times will have a discrete distribution. The probability of a scheduled patient not showing up will be \\(q = 0.2\\).\nThe objective function will be the weighted average of the total waiting time of all patients and overtime. The model will be trained to predict which of the two neighboring schedules has the lowest objective value. The prediction time will be recorded. Then the same schedules will be evaluated by computing the objective value and then ranked.\n\n\n4.4.3 Variables\n\nIndependent Variables: A list of tuples with pairs of neighboring schedules.\nDependent Variables: A list with rankings for each tuple of pairwise schedules. Eg: If the rank for ([2,1,1], [1,1,2]) equals 0 this means that the schedule with index 0 ([2,1,1]) has the lowest objective value.\n\n\n\n4.4.4 Data Collection\nThe data set will be generated using simulation in which random samples will be drawn from the population of all possible schedules. For each sample a random neighboring schedule will be created.\n\n\n4.4.5 Sample Size and Selection\nSample Size: The total population size equals \\({{N + T -1}\\choose{N}} \\approx\\) 244663.0 mln. For this experiment we will be using a relatively small sample of 100000 pairs of schedules.\nSample Selection: The samples will be drawn from a lexicographic order of possible schedules in order to accurately reflect the combinatorial nature of the problem and to ensure unbiased sampling from the entire combinatorial space.\n\n\n4.4.6 Experimental Procedure\nThe experiment involves multiple steps, beginning with data preparation and concluding with model evaluation.The diagram below illustrates the sequence of steps.\n\n\n\n\n\ngraph TD\n    A[\"From population\"] --&gt;|\"Sample\"| B[\"Random subset\"]\n    B --&gt; |Create neighbors| C[\"Features: Schedule pairs\"]\n    C --&gt; |Calculate objectives| D[\"Objective values\"]\n    D --&gt; |Rank objectives| E[\"Labels: Rankings\"]\n    E --&gt; |\"Split dataset\"| F[\"Training set\"]\n    E --&gt; |\"Split dataset\"| G[\"Test set\"]\n    F --&gt; |\"Train\"| H[\"Model\"]\n    H[\"Model\"] --&gt; |\"Apply\"| G[\"Test set\"]\n    G[\"Test set\"] --&gt; |\"Evaluate\"| I[\"Performance\"]\n\n\n\n\n\n\nStep 1: Create pairs of neighboring schedules. A set of 100000 schedules will be sampled from the neighborhood of the initial schedule. For each schedule a pair of neighbors will be created. The order of the neighbors will be randomly switched to create a more diverse training set. The time taken to sample the schedules and create the neighbors will be recorded.\n\nfrom functions import get_v_star, get_neighborhood\n\ndef sample_neighbors_list(x: list[int], v_star: np.ndarray, all = True) -&gt; (list[int], list[int]):\n    \"\"\"\n    Create a set of pairs of schedules that are from the same neighborhood.\n    \n    Parameters:\n      x (list[int]): A list of integers with |s| = T and sum N.\n      v_star (np.ndarray): Precomputed vectors V* of length T.\n      \n    Returns:\n      tuple(list[int], list[int]): A pair of schedules.\n    \"\"\"\n    T = len(x)\n\n    # Precompute binomial coefficients (weights for random.choices)\n    binom_coeff = [math.comb(T, i) for i in range(1, T)]\n\n    # Choose a random value of i with the corresponding probability\n    i = random.choices(range(1, T), weights=binom_coeff)[0]\n\n    # Instead of generating the full list of combinations, sample one directly\n    j = random.sample(range(T), i)\n    \n    x_p = x.copy()\n    for k in j:\n        x_temp = np.array(x_p) + v_star[k]\n        x_temp = x_temp.astype(int)\n        if np.all(x_temp &gt;= 0):\n            x_p = x_temp.astype(int).tolist()\n    if all:\n        return x, x_p\n    else:    \n        return x_p\n\nstart = time.time()\nv_star = get_v_star(T)\n# Sample a set of schedules from the neighborhood of the initial schedule\nneighbors_selection = [sample_neighbors_list(initial_x, v_star, all = False) for i in range(num_schedules)] # This can be done in parallel to improve speed\nprint(len(neighbors_selection))\nend = time.time()\n# For the sampled schedules, create the neighbors\nneighbors_list = [sample_neighbors_list(schedule, v_star) for schedule in neighbors_selection]\n# Randomly switch the order of the neighbors\nneighbors_list = [neighbor if random.random() &lt; 0.5 else neighbor[::-1] for neighbor in neighbors_list]\nend = time.time()\nh = random.choices(range(num_schedules), k=7)\nprint(f\"Sampled schedules: {h}\")\nfor i in h:\n    original_schedule = neighbors_list[i][0]\n    neighbor_schedule = neighbors_list[i][1]\n    difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]\n    print(f\"Neighbors\\n{original_schedule}\\n{neighbor_schedule}\\n{difference}\")\ntraining_set_feat_time = end - start\nprint(f\"\\nProcessing time: {training_set_feat_time} seconds\\n\")\n\n100000\nSampled schedules: [40845, 15551, 41551, 42805, 88032, 87605, 906]\nNeighbors\n[1, 1, 0, 2, 0, 0, 2, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 2, 8]\n[2, 0, 0, 2, 0, 1, 1, 0, 2, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 8]\n[-1, 1, 0, 0, 0, -1, 1, 0, -1, 0, 1, 0, -1, 0, 1, -1, 1, -1, 1, 0]\nNeighbors\n[2, 0, 0, 3, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1, 0, 8]\n[3, 0, 0, 2, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 7]\n[-1, 0, 0, 1, -1, 0, 0, 0, 1, -1, 0, 1, -1, 0, 0, 1, 0, 0, -1, 1]\nNeighbors\n[1, 0, 1, 2, 0, 0, 1, 0, 1, 2, 0, 1, 0, 0, 1, 1, 1, 0, 2, 8]\n[2, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 2, 0, 1, 8]\n[-1, 0, 0, 1, 0, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 1, -1, 0, 1, 0]\nNeighbors\n[1, 1, 0, 0, 1, 0, 1, 2, 0, 0, 2, 0, 1, 0, 0, 3, 0, 0, 2, 8]\n[1, 2, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 1, 0, 0, 2, 1, 0, 1, 8]\n[0, -1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 1, -1, 0, 1, 0]\nNeighbors\n[2, 0, 0, 2, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 2, 0, 1, 1, 0, 8]\n[2, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 1, 0, 9]\n[0, 0, 0, 0, 1, 0, -1, 0, 1, -1, 0, 1, -1, 0, 1, -1, 1, 0, 0, -1]\nNeighbors\n[2, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 1, 9]\n[2, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 1, 1, 9]\n[0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 1, -1, 1, 0, 0, 0, 0, -1, 0, 0]\nNeighbors\n[1, 1, 0, 2, 0, 0, 1, 1, 0, 0, 1, 1, 2, 0, 0, 1, 1, 1, 0, 9]\n[2, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 1, 8]\n[-1, 1, -1, 1, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 0, 1, -1, 1, -1, 1]\n\nProcessing time: 18.287192821502686 seconds\n\n\n\nStep 2: For each schedule in each pair calculate the objective. For each pair save the index of the schedule that has the lowest objective value.\n\nfrom functions import calculate_objective_serv_time_lookup\n\nobjectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[0], d, convolutions)]\n]\nstart = time.time()\nobjectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for neighbor in neighbors_list\n    for result in [calculate_objective_serv_time_lookup(neighbor[1], d, convolutions)]\n]\nend = time.time()\ntraining_set_lab_time = end - start\nobjectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]\nrankings = np.argmin(objectives, axis=1).tolist()\nfor i in range(5):\n    print(f\"Objectives: {objectives[i]}, Ranking: {rankings[i]}\")\n\nprint(f\"\\nProcessing time: {training_set_lab_time} seconds\\n\")\n\n# Step 1: Flatten the objectives into a 1D array\nflattened_data = [value for sublist in objectives for value in sublist]\n\n# Step 2: Find the index of the minimum value\nmin_index = np.argmin(flattened_data)\n\n# Step 3: Convert that index back to the original 2D structure\nrow_index = min_index // 2  # Assuming each inner list has 2 values\ncol_index = min_index % 2\n\nprint(f\"The minimum objective value is at index [{row_index}][{col_index}].\\nThis is schedule: {neighbors_list[row_index][col_index]} with objective value {objectives[row_index][col_index]}.\")\n\nfile_path_best_schedule = f\"datasets/best_schedule_{N}_{T}_{l}.pkl\"\nwith open(file_path_best_schedule, 'wb') as f:\n    pickle.dump({'best_schedule':neighbors_list[row_index][col_index], 'objective': objectives[row_index][col_index]}, f)\n    print(f\"Data saved successfully to '{file_path_best_schedule}'\")\n\nprint(f\"\\nAverage ranking: {np.mean(rankings)}\\n\")\n\n# Saving neighbors_list and objectives to a pickle file\nfile_path_neighbors = f\"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl\"\nwith open(file_path_neighbors, 'wb') as f:\n    pickle.dump({'neighbors_list': neighbors_list, 'objectives': objectives, 'rankings': rankings}, f)\n    print(f\"Data saved successfully to '{file_path_neighbors}'\")\n\nObjectives: [79.50693818434377, 81.45848909377791], Ranking: 0\nObjectives: [88.83009174280377, 82.12923689182381], Ranking: 1\nObjectives: [83.7270131391846, 94.57667158096393], Ranking: 0\nObjectives: [84.23459851181002, 81.62268592233806], Ranking: 1\nObjectives: [80.46132583888905, 76.49262005144834], Ranking: 1\n\nProcessing time: 33.72524380683899 seconds\n\nThe minimum objective value is at index [73721][0].\nThis is schedule: [3, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 6] with objective value 72.06648924027994.\nData saved successfully to 'datasets/best_schedule_22_20_10.pkl'\n\nAverage ranking: 0.49868\n\nData saved successfully to 'datasets/neighbors_and_objectives_22_20_10.pkl'\n\n\nStep 3: Create training and test sets.\n\n# Prepare the dataset\nX = []\nfor neighbors in neighbors_list:\n    X.append(neighbors[0] + neighbors[1])\n\nX = np.array(X)\ny = np.array(rankings)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 4: Train the XGBoost model.\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B[Initialize StratifiedKFold]\n    B --&gt; C[Initialize XGBClassifier]\n    C --&gt; D[Set results as empty list]\n    D --&gt; E[Loop through each split of cv split]\n    E --&gt; F[Get train and test indices]\n    F --&gt; G[Split X and y into X_train, X_test, y_train, y_test]\n    G --&gt; H[Clone the classifier]\n    H --&gt; I[Call fit_and_score function]\n    I --&gt; J[Fit the estimator]\n    J --&gt; K[Score on training set]\n    J --&gt; L[Score on test set]\n    K --&gt; M[Return estimator, train_score, test_score]\n    L --&gt; M\n    M --&gt; N[Append the results]\n    N --&gt; E\n    E --&gt; O[Loop ends]\n    O --&gt; P[Print results]\n    P --&gt; Q[End]\n\n\n\n\n\n\n\nclass CustomCallback(TrainingCallback):\n    def __init__(self, period=10):\n        self.period = period\n\n    def after_iteration(self, model, epoch, evals_log):\n        if (epoch + 1) % self.period == 0:\n            print(f\"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}\")\n        return False\n    \ndef fit_and_score(estimator, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0\n    )\n\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return estimator, train_score, test_score\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n\n# Initialize the XGBClassifier without early stopping here\n# Load the best trial parameters from a JSON file.\nwith open(\"model_params.json\", \"r\") as f:\n    model_params = json.load(f)\n    \n# Initialize the EarlyStopping callback with validation dataset\nearly_stop = xgb.callback.EarlyStopping(\n    rounds=10, metric_name='logloss', data_name='validation_0', save_best=True\n)\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n    early_stopping_rounds=9,\n    #callbacks=[CustomCallback(period=50), early_stop],\n    callbacks=[CustomCallback(period=50)],\n)\nprint(\"Params: \")\nfor key, value in model_params.items():\n    print(f\" {key}: {value}\")\n\nstart = time.time()\nresults = []\n\nfor train_idx, test_idx in cv.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    est, train_score, test_score = fit_and_score(\n        clone(clf), X_train, X_test, y_train, y_test\n    )\n    results.append((est, train_score, test_score))\nend = time.time()\ntraining_time = end - start\nprint(f\"\\nTraining time: {training_time} seconds\\n\")\n\nParams: \n max_depth: 6\n min_child_weight: 1\n gamma: 0.1\n subsample: 0.8\n colsample_bytree: 0.8\n learning_rate: 0.1\n n_estimators: 100\nEpoch 49, Evaluation log: 0.30141328097726217\nEpoch 99, Evaluation log: 0.22781096739732892\nEpoch 49, Evaluation log: 0.3011711457826663\nEpoch 99, Evaluation log: 0.2319554012069886\nEpoch 49, Evaluation log: 0.3010658121422632\nEpoch 99, Evaluation log: 0.22841725708864397\nEpoch 49, Evaluation log: 0.3024460541829001\nEpoch 99, Evaluation log: 0.22977375936789904\nEpoch 49, Evaluation log: 0.2952395353531931\nEpoch 99, Evaluation log: 0.22460256981629936\n\nTraining time: 2.679650068283081 seconds\n\n\n\nStep 5: To evaluate the performance of the XGBoost ranking model, we will use Stratified K-Fold Cross-Validation with 5 splits, ensuring each fold maintains the same class distribution as the original dataset. Using StratifiedKFold(n_splits=5, shuffle=True, random_state=94), the dataset will be divided into five folds. In each iteration, the model will be trained on four folds and evaluated on the remaining fold. A custom callback, CustomCallback(period=10), will print the evaluation log every 10 epochs.\nThe fit_and_score function will fit the model and score it on both the training and test sets, storing the results for each fold. This provides insight into the model’s performance across different subsets of the data, helps in understanding how well the model generalizes to unseen data and identifies potential overfitting or underfitting issues. The overall processing time for the cross-validation will also be recorded.\n\n# Print results\nfor i, (est, train_score, test_score) in enumerate(results):\n    print(f\"Fold {i+1} - Train Score (Accuracy): {train_score:.4f}, Test Score (Accuracy): {test_score:.4f}\")\n\nFold 1 - Train Score (Accuracy): 0.9313, Test Score (Accuracy): 0.9224\nFold 2 - Train Score (Accuracy): 0.9316, Test Score (Accuracy): 0.9197\nFold 3 - Train Score (Accuracy): 0.9310, Test Score (Accuracy): 0.9201\nFold 4 - Train Score (Accuracy): 0.9319, Test Score (Accuracy): 0.9214\nFold 5 - Train Score (Accuracy): 0.9311, Test Score (Accuracy): 0.9253\n\n\nTraining the model on the entire dataset provides a final model that has learned from all available data. Recording the training time helps in understanding the computational efficiency and scalability of the model with the given hyperparameters.\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=model_params[\"max_depth\"],\n    min_child_weight=model_params[\"min_child_weight\"],\n    gamma=model_params[\"gamma\"],\n    subsample=model_params[\"subsample\"],\n    colsample_bytree=model_params[\"colsample_bytree\"],\n    learning_rate=model_params[\"learning_rate\"],\n    n_estimators=model_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nclf.save_model('models/classifier_large_instance.json')\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\\n\")\n\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\nTraining accuracy: 92.96%\n\n\nTraining time: 0.3552968502044678 seconds\n\n\n\n\n\n4.4.7 Validation\nGenerating test schedules and calculating their objectives and rankings allows us to create a new dataset for evaluating the model’s performance on unseen data.\n\nnum_test_schedules = 1000\n\n#test_schedules = random_combination_with_replacement(T, N, num_test_schedules)\ntest_schedules = [sample_neighbors_list(initial_x, v_star, all = False) for i in range(num_test_schedules)]\n\ntest_neighbors = [sample_neighbors_list(test_schedule, v_star) for test_schedule in test_schedules] # This can be done in parellel to improve speed\n\nprint(f\"Sampled: {len(test_schedules)} schedules\\n\")\n\ntest_objectives_schedule_1 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[0], d, convolutions)]\n]\n# Start time measurement for the evaluation\nstart = time.time()\ntest_objectives_schedule_2 = [\n    w * result[0] + (1 - w) * result[1]\n    for test_neighbor in test_neighbors\n    for result in [calculate_objective_serv_time_lookup(test_neighbor[1], d, convolutions)]\n]\ntest_rankings = [0 if test_obj &lt; test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]\nend = time.time()\nevaluation_time = end - start\n\n# Combine the objectives for each pair for later processing\ntest_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]\n\nprint(f\"\\nEvaluation time: {evaluation_time} seconds\\n\")\n\nfor i in range(6):\n    print(f\"Neighbors: {test_neighbors[i]},\\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\\n\")\n\nSampled: 1000 schedules\n\n\nEvaluation time: 0.3741741180419922 seconds\n\nNeighbors: ([2, 2, 0, 1, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 1, 0, 8], [3, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 1, 1, 0, 1, 1, 1, 1, 0, 7]),\nObjectives: [77.61197706020278, 75.0475465517728], Ranking: 1\n\nNeighbors: ([3, 0, 1, 0, 1, 0, 2, 0, 1, 0, 1, 1, 1, 0, 0, 2, 0, 1, 1, 7], [2, 0, 1, 1, 0, 0, 2, 1, 1, 0, 1, 0, 1, 0, 0, 2, 1, 1, 0, 8]),\nObjectives: [75.75015028461324, 81.20429622496788], Ranking: 0\n\nNeighbors: ([2, 1, 0, 2, 0, 0, 1, 1, 0, 2, 0, 1, 1, 0, 0, 2, 0, 1, 0, 8], [1, 1, 0, 2, 0, 0, 1, 1, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 0, 8]),\nObjectives: [76.98954681663314, 79.64027469540282], Ranking: 0\n\nNeighbors: ([2, 1, 0, 2, 0, 0, 2, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 8], [3, 0, 1, 1, 0, 0, 3, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 1, 0, 8]),\nObjectives: [77.50213048086938, 77.82604624619995], Ranking: 0\n\nNeighbors: ([3, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 7], [3, 0, 2, 0, 0, 1, 0, 1, 1, 1, 0, 0, 2, 0, 2, 0, 1, 1, 0, 7]),\nObjectives: [74.82688345441781, 75.34919311521634], Ranking: 0\n\nNeighbors: ([1, 1, 0, 1, 1, 0, 1, 1, 0, 2, 0, 0, 1, 2, 0, 1, 0, 0, 1, 9], [0, 1, 1, 0, 1, 0, 1, 1, 0, 2, 0, 0, 2, 1, 0, 1, 0, 1, 0, 10]),\nObjectives: [83.91823775787537, 92.58214312977748], Ranking: 0\n\n\n\nMaking predictions on new data and comparing them to the actual rankings provides an evaluation of the model’s performance in practical applications. Recording the prediction time helps in understanding the model’s efficiency during inference.\n\ninput_X = test_neighbors\nX_new = []\nfor test_neighbor in input_X:\n    X_new.append(test_neighbor[0] + test_neighbor[1])\n    \n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.004274845123291016 seconds\n\ntest_rankings = [1 0 0 0 0 0], \ny_pred = [1 0 0 1 0 0], \ny_pred_proba = \n[[0.04008907 0.9599109 ]\n [0.97358394 0.02641609]\n [0.80036056 0.19963947]\n [0.399023   0.600977  ]\n [0.6312184  0.36878163]\n [0.9973532  0.00264681]]\n\n\nCalculating the ambiguousness of the predicted probabilities helps in understanding the model’s confidence in its predictions. High ambiguousness indicates uncertain predictions, while low ambiguousness indicates confident predictions.\nAmbiguousness is calculated using the formula for entropy:\n\\[\nH(X) = - \\sum_{i} p(x_i) \\log_b p(x_i)\n\\]\nWhere in our case:\n\n\\(H(X)\\) is the ambiguousness of the random variable \\(X\\) - the set of probability scores for the predicted rankings,\n\\(p(x_i)\\) is probability score \\(x_i\\),\n\\(\\log_b\\) is the logarithm with base \\(b\\) (here \\(\\log_2\\) as we have two predicted values),\nThe sum is taken over all possible outcomes of \\(X\\).\n\nCalculating cumulative error rate and cumulative accuracy helps in understanding how the model’s performance evolves over the dataset.\nVisualizing the relationship between ambiguousness and error provides insights into how uncertainty in the model’s predictions correlates with its accuracy. This can help in identifying patterns and understanding the conditions under which the model performs well or poorly.\n\nfrom functions import calculate_ambiguousness\n\nerrors = np.abs(y_pred - np.array(test_rankings))\n\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9)))\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()\n\n                                                \n\n\n\n\n4.4.8 Hyperparameter Optimization\nIn the initial model the choice of hyperparameters was based on default values, examples from demo’s or trial and error. To improve the model’s performance, we applied a hyperparameter optimization technique to find the best set of hyperparameters. We used a grid search with cross-validation to find the optimal hyperparameters for the XGBoost model. The grid search was performed over a predefined set of hyperparameters, and the best hyperparameters were selected based on the model’s performance on the validation set. The best hyperparameters were then used to train the final model.\n\nfrom functions import compare_json\n\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n    \ndifferences = compare_json(model_params, best_trial_params)\n\nparams_tbl = pd.DataFrame(differences)\nparams_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)\nprint(params_tbl)\n\n                      max_depth     gamma  subsample  colsample_bytree  \\\nbase parameters               6  0.100000   0.800000          0.800000   \noptimized parameters          5  0.304548   0.781029          0.922528   \n\n                      learning_rate  n_estimators  \nbase parameters            0.100000           100  \noptimized parameters       0.239488           490  \n\n\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\n# Load the best trial parameters from a JSON file.\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\n# Calculate and print the training accuracy\ntraining_accuracy = clf.score(X, y)\nprint(f\"Training accuracy: {training_accuracy * 100:.2f}%\")\n\n\nTraining time: 1.7021079063415527 seconds\n\nTraining accuracy: 98.24%\n\n\n\n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.00463104248046875 seconds\n\ntest_rankings = [1 0 0 0 0 0], \ny_pred = [1 0 0 0 0 0], \ny_pred_proba = \n[[2.4157763e-04 9.9975842e-01]\n [9.9996793e-01 3.2085922e-05]\n [9.5100594e-01 4.8994072e-02]\n [5.8669710e-01 4.1330290e-01]\n [9.3675846e-01 6.3241541e-02]\n [9.9997610e-01 2.3898403e-05]]\n\n\n\nerrors = np.abs(y_pred - np.array(test_rankings))\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors, \"Schedules\": test_neighbors, \"Objectives\": test_objectives}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n\n\n\n\n\n\n\nAmbiguousness\nError\nSchedules\nObjectives\nCumulative error rate\nCumulative accuracy\n\n\n\n\n564\n6.573361e-09\n0\n([1, 1, 0, 2, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,...\n[79.78868805835292, 90.90558576117596]\n0.0\n1.0\n\n\n681\n4.366743e-08\n0\n([1, 1, 0, 2, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,...\n[79.5725207726945, 88.72079522787317]\n0.0\n1.0\n\n\n533\n1.180772e-07\n0\n([2, 0, 1, 0, 1, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0,...\n[81.28032469411701, 90.66640889399675]\n0.0\n1.0\n\n\n373\n1.376338e-07\n0\n([2, 0, 0, 2, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,...\n[79.66591257721217, 89.8662444629968]\n0.0\n1.0\n\n\n403\n4.579138e-07\n0\n([2, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 0, 2, 0, 1,...\n[81.72719890855272, 91.29598539168785]\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9),\n                    customdata=df[[\"Schedules\", \"Objectives\"]],\n                    hovertemplate=\n                        \"Ambiguousness: %{x} &lt;br&gt;\" +\n                        \"Error: %{y} &lt;br&gt;\" +\n                        \"Schedules: %{customdata[0][0]} / %{customdata[0][1]} &lt;br&gt;\" +\n                        \"Objectives: %{customdata[1]} &lt;br&gt;\"\n                    ))\n                  \nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(\n    title={\n        'text': f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n        'y': 0.95,  # Keep the title slightly higher\n        'x': 0.02,\n        'xanchor': 'left',\n        'yanchor': 'top'\n    },\n    xaxis_title=\"Ambiguousness\",\n    yaxis_title=\"Error / Accuracy\",\n    hoverlabel=dict(font=dict(color='white')),\n    margin=dict(t=70)  # Add more space at the top of the chart\n)\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#results",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#results",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.5 Results",
    "text": "4.5 Results\nWe wanted to test whether an XGBoost classification model could be used to assess and rank the quality of pairs of schedules. For performance benchmarking we use the conventional calculation method utilizing Lindley recursions.\nWe trained the XGBoost ranking model with a limited set of features (schedules) and labels (objectives). The total number of possible schedules is approximately 244663.0 million. For training and evaluation, we sampled 200000 schedules and corresponding neighbors. Generating the feature and label set took a total of 52.0124 seconds, with the calculation of objective values accounting for 33.7252 seconds.\nThe model demonstrates strong and consistent performance with high accuracies both for training, testing and validation (94.99%) with good generalization and stability. Total training time for the final model was 1.7021 seconds. The evaluation of 1000 test schedules took 0.0046 seconds for the the XGBoost model and 0.3742 for the conventional method, which is an improvement of 80X.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#discussion",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#discussion",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.6 Discussion",
    "text": "4.6 Discussion\n\ntraining_time = round(modeling_time, 4)\nconventional_time = round(evaluation_time, 4)\nxgboost_time = round(prediction_time, 4)\n\n# Define time values for plotting\ntime_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds\n\n# Calculate evaluations for method 1\nmethod1_evaluations = np.where(time_values &gt;= training_time, (time_values - training_time) / xgboost_time * 1000, 0)\n\n# Calculate evaluations for method 2\nmethod2_evaluations = time_values / conventional_time * 1000\n\n# Create line chart\nfig = go.Figure()\n\n# Add method 1 trace\nfig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))\n\n# Add method 2 trace\nfig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))\n\n# Update layout\nfig.update_layout(\n    title=\"Speed comparison between XGBoost ranking model and conventional method\",\n    xaxis_title=\"Time (seconds)\",\n    yaxis_title=\"Number of Evaluations\",\n    legend_title=\"Methods\",\n    template=\"plotly_white\"\n)\n\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#timeline",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#timeline",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.7 Timeline",
    "text": "4.7 Timeline\nThis experiment was started on 26-04-2025. The expected completion date is 26-04-2025.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking-large-w-bailey-welch.html#references",
    "href": "xgboost-pairwise-ranking-large-w-bailey-welch.html#references",
    "title": "4  Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule",
    "section": "4.8 References",
    "text": "4.8 References",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Large instance XGBoost classification model for pairwise ranking with Bailey-Welch rule</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html",
    "href": "local-search-ranking-large.html",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "",
    "text": "5.1 Objective\nTest the working and performance of a previously trained XGBoost Ranking model in a local search application.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#background",
    "href": "local-search-ranking-large.html#background",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.2 Background",
    "text": "5.2 Background\nIn previous experiments, we trained an XGBoost Classifier model to predict the objective values of neighboring schedules. In this experiment, we will use the trained models to perform a local search to find the best schedule.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#hypothesis",
    "href": "local-search-ranking-large.html#hypothesis",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.3 Hypothesis",
    "text": "5.3 Hypothesis\nThe XGBoost Classifier model will be able to efficiently guide the local search algorithm to find a schedule with a lower objective value than the initial schedule.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#methodology",
    "href": "local-search-ranking-large.html#methodology",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.4 Methodology",
    "text": "5.4 Methodology\n\n5.4.1 Tools and Materials\n\nimport numpy as np\nimport json\nimport time\nfrom itertools import chain, combinations\nimport sys\nfrom math import comb  # Python 3.8 and later\nimport xgboost as xgb\nimport pickle\nfrom typing import List, Tuple, Dict, Iterable, TypeVar, Union, Any\n\nimport logging\nimport sys # Needed for StreamHandler in order to enable explicit console output\n\n# Logging configuration\nlog_level = logging.DEBUG\nlog_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n\n# Log to a file instead of to the console:\nlogging.basicConfig(level=log_level, format=log_format, filename='search.log', filemode='w')\n\n# Get a logger instance\nlogger = logging.getLogger(__name__)\n\n\n\n5.4.2 Load Parameters\n\nN = 22 # Number of patients\nT = 20 # Number of intervals\nl = 10 # Target service time length\n\nfile_path_parameters = f\"datasets/parameters_{N}_{T}_{l}.pkl\" # For retrieving saved scheduling parameters\n# Load the data from the pickle file\nwith open(file_path_parameters, 'rb') as f:\n    data_params = pickle.load(f)\n\nN = data_params['N'] # Number of patients\nT = data_params['T'] # Number of intervals\nd = data_params['d'] # Length of each interval\nmax_s = data_params['max_s'] # Maximum service time\nq = data_params['q'] # Probability of a scheduled patient not showing up\nw = data_params['w'] # Weight for the waiting time in objective function\nl = data_params['l']\n  \nnum_schedules = data_params['num_schedules'] # Size of training set\nconvolutions = data_params['convolutions'] # Service time distributions used in training phase adjusted for no-shows\nprint(f\"Parameters loaded: N={N}, T={T}, l={l}, d={d}, max_s={max_s}, q={q}, w={w}, num_schedules={num_schedules}\")\n\nParameters loaded: N=22, T=20, l=10, d=5, max_s=20, q=0.2, w=0.1, num_schedules=100000\n\n\n\n\n5.4.3 Experimental Design\nWe will use the trained XGBoost Classifier model to guide a local search algorithm to find the best schedule. The local search algorithm will start with an initial schedule and iteratively explore the neighborhood of the current schedule to find a better one. As an initial schedule, we will use the schedule with the lowest objective value from the training dataset that was used to train the XGBoost Classifier model.\n\n\n5.4.4 Variables\n\nIndependent Variables:\n\nInitial schedule, trained XGBoost Classifier\n\nDependent Variables:\n\nSpeed, accuracy, and convergence of the local search algorithm.\n\n\n\n\n5.4.5 Data Collection\nWe will use the training dataset to initialize the local search algorithm.\n\n\n5.4.6 Sample Size and Selection\n\n\n5.4.7 Experimental Procedure\n\n\n\n\n\ngraph TD\n                A[Start] --&gt; B(\"Initialize schedule x\");\n                B --&gt; C{\"Iterate through all subsets U of V*\"};\n                C -- \"For each U\" --&gt; D{\"Compute y = x + sum(v in U)\"};\n                D -- \"Check y &gt;= 0\" --&gt; E{\"Compute cost C(y)\"};\n                E --&gt; F{\"Is C(y) &lt; C(x)?\"};\n                F -- \"Yes\" --&gt; G[\"Update x := y\"];\n                G --&gt; C;\n                F -- \"No\" --&gt; H{\"Finished iterating all U?\"};\n                H -- \"Yes\" --&gt; I[\"End: x is optimal schedule\"];\n                H -- \"No\" --&gt; C;\n                D -- \"If y &lt; 0\" --&gt; C;",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#results",
    "href": "local-search-ranking-large.html#results",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.5 Results",
    "text": "5.5 Results\n\n5.5.1 Load the initial best schedule.\nStart with the best solution found so far \\(\\{x^*, C(x^*)\\}\\) from the training set.\n\n# Load the best solution from the training dataset\nfile_path_schedules = f\"datasets/best_schedule_{N}_{T}_{l}.pkl\"\n# Load the data from the pickle file\nwith open(file_path_schedules, 'rb') as f:\n    best_schedule_data = pickle.load(f)\n    \nprint(f\"The data has following keys: {[key for key in best_schedule_data.keys()]}\")\n\nprint(f\"The current best schedule is: {best_schedule_data['best_schedule']} with objective value {best_schedule_data['objective']}.\")\n\n# Set the initial schedule to the best solution from the training dataset\ninitial_schedule = best_schedule_data['best_schedule']\n\nThe data has following keys: ['best_schedule', 'objective']\nThe current best schedule is: [3, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 6] with objective value 72.06648924027994.\n\n\n\n\n5.5.2 Generate the neighborhood of \\(x^*\\).\n\n5.5.2.1 Define \\(V^*\\) and \\(U_t\\).\nDefine the vectors \\(V^*\\) as follows:\n\\[\n\\left\\{\n\\begin{array}{c}\n\\vec{v_1}, \\\\\n\\vec{v_2}, \\\\\n\\vec{v_3}, \\\\\n\\vdots \\\\\n\\vec{v_{T-1}}, \\\\\n\\vec{v_T} \\\\\n\\end{array}\n\\right\\} =\n\\left\\{\n\\begin{array}{c}\n(-1, 0,...., 0, 1), \\\\\n(1, -1, 0,...., 0), \\\\\n(0, 1, -1,...., 0), \\\\\n\\vdots \\\\\n(0,...., 1, -1, 0), \\\\\n(0,...., 0, 1, -1) \\\\\n\\end{array}\n\\right\\}\n\\]\nDefine \\(U_t\\) as the set of all possible subsets of \\(V^*\\) such that each subset contains exactly \\(t\\) elements, i.e.,\n\\[\nU_t = \\{ S \\subsetneq V^* \\mid |S| = t \\}, \\quad t \\in \\{1, 2, \\dots, T\\}.\n\\]\n\nfrom functions import get_v_star\n\ndef powerset(iterable, size=1):\n    \"powerset([1,2,3], 2) --&gt; (1,2) (1,3) (2,3)\"\n    return [[i for i in item] for item in combinations(iterable, size)]\n  \nx = initial_schedule\n\n# Generate a matrix 'v_star' using the 'get_v_star' function\nv_star = get_v_star(T)\n\n# Generate all possible non-empty subsets (powerset) of the set {0, 1, 2, ..., t-1}\n# 'ids' will be a list of tuples, where each tuple is a subset of indices\nsize = 2\nids = powerset(range(T), size)\nlen(ids)\nids[:T]\n\n[[0, 1],\n [0, 2],\n [0, 3],\n [0, 4],\n [0, 5],\n [0, 6],\n [0, 7],\n [0, 8],\n [0, 9],\n [0, 10],\n [0, 11],\n [0, 12],\n [0, 13],\n [0, 14],\n [0, 15],\n [0, 16],\n [0, 17],\n [0, 18],\n [0, 19],\n [1, 2]]\n\n\n\n\n5.5.2.2 Define the neighborhood of \\(x\\)\nDefine the neighborhood of \\(x\\) as all vectors of the form \\(x + u_{tk}, \\forall \\, u_{tk} \\in U_t\\).\n\nfrom functions import get_neighborhood\ntest_nh = get_neighborhood(x, v_star, ids)\nprint(f\"All neighborhoods with {size} patients switched:\\n x = {np.array(x)}: \\n {test_nh}\")\n\nAll neighborhoods with 2 patients switched:\n x = [3 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 6]: \n [[2 1 0 ... 1 1 7]\n [2 0 1 ... 1 1 7]\n [2 0 1 ... 1 1 7]\n ...\n [3 0 1 ... 1 0 6]\n [3 0 1 ... 0 2 5]\n [3 0 1 ... 2 1 5]]\n\n\n\n\n\n5.5.3 Local search algorithm with prediction\nLoad the pre-trained model and use it for evaluating schedules within a local search algorithm. The search algorithm checks for false positives (prediction improvement = “True”, actual is improvement = “False”) and false negatives (prediction improvement = “False”, actual is improvement = “True”). In both cases the model is updated using the schedules and associated objective values (rankings).\n\n\n\n\n\ngraph TD\n    %% --- Part 1: Initialization & Outer Loop ---\n\n    A[Start: local_search_predict_update] --&gt; B{Inputs: x, w, v_star, clf, params, size, restarts, threshold};\n    B --&gt; C{\"Validate Inputs (clf, x length)\"};\n    C -- Valid --&gt; D[\"Initialize: x_star, T, restart_count=0, t=1\"];\n    C -- Invalid --&gt; Z_Err1[\"Raise ValueError\"];\n    D --&gt; E{\"Calculate Initial cost_star\"};\n    E -- Success --&gt; F{\"Outer Loop: t &lt;= size AND restart_count &lt; restarts?\"};\n    E -- Error --&gt; Z_Err2[\"Return x_star, clf\"];\n\n    %% Connections FROM other parts back to the Outer Loop check (F)\n    Connector_O([From Part 2: Break Inner Loop]) --&gt; F;\n    Connector_CC_Yes([From Part 3: Found Better at Level t]) --&gt; F;\n    Connector_DD([From Part 3: Incremented t]) --&gt; F;\n\n    %% Connections TO other parts\n    F -- No --&gt; Y[\"End: Return x_star, clf\"];\n    F -- Yes --&gt; G[\"Generate Neighborhood (level t)\"];\n    G --&gt; Connector_H([To Part 2: Start Inner Loop]);\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n    %% --- Part 2: Inner Loop - Neighbor Evaluation ---\n\n    Connector_G([From Part 1: Generate Neighborhood]) --&gt; H{\"Inner Loop: For each neighbor\"};\n\n    H -- Next Neighbor --&gt; I{\"Predict with clf: prediction, P(0)\"};\n    I -- Error Predicting --&gt; I_Err[\"Log Error, Assume P=0\"];\n    I_Err --&gt; J;\n    I -- Success --&gt; J{\"Perform Expensive Check? (Pred=1 OR P(0) &lt; threshold)\"};\n    J -- No --&gt; H_Next[Next Neighbor]; %% Skip expensive check\n    J -- Yes --&gt; K{\"Calculate True Cost (Expensive Objective Func)\"};\n    K -- Error --&gt; K_Err[\"Log Error\"];\n    K_Err --&gt; H_Next;\n    K -- Success --&gt; L{\"Is neighbor truly better? (cost_neighbor &lt; cost_star)\"};\n\n    %% Path 1: Improvement Found\n    L -- Yes --&gt; M[\"Update x_star, cost_star, T\"];\n    M --&gt; N[\"Set found_better=True, t=1, restart_count++\"];\n    N --&gt; O[\"Break Inner Loop\"];\n    O --&gt; Connector_F1([To Part 1: Outer Loop Check]); %% Connects back to F\n\n    %% Path 2: No Improvement\n    L -- No --&gt; P{\"Misprediction? (Pred=1 AND Actual=0)\"};\n    P -- No --&gt; Q[\"Log Borderline/Correct Pred=0\"];\n    Q --&gt; H_Next;\n    P -- Yes --&gt; R[\"Log Misprediction\"];\n    R --&gt; Connector_S([To Part 3: Start Retraining]); %% Trigger Retraining\n\n    %% Loop Control\n    H_Next --&gt; H; %% Process next neighbor\n    H -- End of Neighbors --&gt; BB{\"End Inner Loop\"};\n    BB --&gt; Connector_BB([To Part 3: Check Level Result]);\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n    %% --- Part 3: Retraining Sub-routine & Loop Control ---\n\n    %% Retraining Sub-routine Start\n    Connector_R([From Part 2: Misprediction Detected]) --&gt; S[\"Start Retraining Sub-routine\"];\n    subgraph Retraining Sub-routine\n        direction TB\n        S --&gt; T{\"Calculate True Costs for ALL neighbors at level t\"};\n        T --&gt; U{\"Opportunistic Better Found during Cost Calc?\"};\n        U -- Yes --&gt; V[\"Update x_star, cost_star, T\"];\n        V --&gt; W[\"Set found_better_retrain=True\"];\n        W --&gt; X[\"Collect Data: Append features/labels for update\"];\n        U -- No --&gt; X;\n        X --&gt; X_Loop{\"More neighbors to process for retraining?\"};\n        X_Loop -- Yes --&gt; T;\n        X_Loop -- No --&gt; Y_Fit{\"Fit clf incrementally\"};\n        Y_Fit -- Error --&gt; Y_FitErr[\"Log Fit Error\"];\n        Y_FitErr --&gt; Z_CheckOpp{\"Check if found_better_retrain?\"};\n        Y_Fit -- Success --&gt; Z_CheckOpp;\n    end\n\n    %% Retraining Outcome\n    Z_CheckOpp -- Yes --&gt; AA[\"Set found_better=True, t=1, restart_count++\"];\n    AA --&gt; Connector_O([To Part 1: Outer Loop Check via Break]); %% Connects back to F via O\n    Z_CheckOpp -- No --&gt; Connector_H_Next([To Part 2: Next Neighbor]); %% Retraining finished, continue inner loop\n\n    %% Inner Loop Finished - Level Control Logic\n    Connector_BB([From Part 2: End Inner Loop]) --&gt; CC{\"Found better solution at level t?\"};\n    CC -- Yes --&gt; Connector_F2([To Part 1: Outer Loop Check]); %% Restart checks from t=1\n    CC -- No --&gt; DD[\"Increment t\"];\n    DD --&gt; Connector_F3([To Part 1: Outer Loop Check]); %% Continue outer loop with next t\n\n\n\n\n\n\n\ndef local_search_predict_update(\n    x: List[int],\n    w: float,\n    v_star: np.ndarray,\n    clf: xgb.XGBClassifier,\n    obj_func_params: Dict[str, Any],\n    size: int = 2,\n    restarts: int = 3,\n    check_proba_threshold: float = 0.7\n) -&gt; Tuple[List[int], xgb.XGBClassifier]:\n    \"\"\"\n    Performs local search guided by an XGBClassifier, minimizing expensive\n    objective calls. Verifies prediction=0 if P(class=0) is below threshold.\n    Updates the classifier incrementally when mispredictions (P=1/A=0 or P=0/A=1) occur.\n    Uses logging instead of print statements. T is inferred from len(x).\n\n    Args:\n        x (List[int]): Starting point.\n        w (float): Weight for combining objectives.\n        v_star (np.ndarray): Current best overall solution (used for guidance).\n        clf (xgb.XGBClassifier): Pre-trained XGBoost Classifier.\n        obj_func_params (Dict[str, Any]): Parameters for objective function.\n        size (int, optional): Max neighborhood size. Defaults to 2.\n        restarts (int, optional): Max restarts. Defaults to 3.\n        check_proba_threshold (float, optional): Threshold for P(class=0) verification. Defaults to 0.7.\n\n    Returns:\n        Tuple[List[int], xgb.XGBClassifier]: Best solution found and updated classifier.\n    \"\"\"\n    if not isinstance(getattr(clf, 'n_estimators', None), int):\n        logger.error(\"clf.n_estimators must be an integer, but found %s\", getattr(clf, 'n_estimators', None))\n        raise ValueError(f\"clf.n_estimators must be an integer, but found {getattr(clf, 'n_estimators', None)}\")\n\n    x_star = list(x) # Work with a copy\n    T = len(x_star) # Infer T from the length\n\n    if T &lt;= 0:\n        logger.error(\"Input schedule x cannot be empty (length must be positive).\")\n        raise ValueError(\"Input schedule x cannot be empty (length must be positive).\")\n\n    restart_count = 0\n    t = 1 # Start with neighborhood size 1\n\n    # Calculate initial cost\n    try:\n        logger.info(\"Calculating initial cost...\")\n        objectives_star = calculate_objective_serv_time_lookup(x_star, **obj_func_params)\n        cost_star = w * objectives_star[0] + (1 - w) * objectives_star[1]\n        logger.info(\"Initial solution cost: %.4f\", cost_star)\n    except Exception as e:\n        logger.exception(\"Error calculating initial cost: %s\", e)\n        return x_star, clf # Return current best and original classifier on error\n\n    while t &lt;= size and restart_count &lt; restarts:\n        logger.info(\"--- Running local search level t=%d (Restart %d/%d) ---\", t, restart_count, restarts)\n\n        ids_gen_iterable = powerset(range(T), t)\n        # Pass x_star (current best) to neighborhood generation\n        neighborhood_iter = get_neighborhood(x_star, v_star, ids_gen_iterable)\n\n        found_better_solution_at_level_t = False\n        neighbors_data_at_level_t: List[Dict[str, Any]] = [] # Store data for potential retraining\n        neighbors_processed_count = 0\n\n        for neighbor_np in neighborhood_iter:\n            neighbors_processed_count += 1\n            neighbor = neighbor_np.tolist()\n            neighbor_info = {\"schedule\": neighbor, \"cost\": None, \"true_label\": None, \"prediction\": None}\n            neighbors_data_at_level_t.append(neighbor_info) # Add neighbor info early\n\n            expected_feature_len = getattr(clf, 'n_features_in_', None)\n            current_feature_len = len(x_star) + len(neighbor) # Feature is concatenation\n            if expected_feature_len is not None and current_feature_len != expected_feature_len:\n                logger.warning(\"Feature length mismatch for neighbor %d. Expected %d, got %d. Skipping.\",\n                               neighbors_processed_count, expected_feature_len, current_feature_len)\n                continue\n\n            feature_pair = x_star + neighbor # Concatenate current best and neighbor\n\n            # 1. Predict using the CHEAP classifier\n            prediction = 0 # Default prediction\n            proba_class_0 = 1.0 # Default probability\n            try:\n                prediction = clf.predict([feature_pair])[0]\n                proba = clf.predict_proba([feature_pair])[0]\n                # Ensure proba has expected structure (e.g., 2 elements for binary class)\n                if len(proba) &gt; 0:\n                   proba_class_0 = proba[0]\n                else:\n                   logger.warning(\"Predict_proba returned unexpected structure: %s. Using default P(0)=1.0\", proba)\n            except Exception as e:\n                logger.warning(\"Error predicting for neighbor %d: %s. Assuming prediction=0.\", neighbors_processed_count, e)\n                # Keep default prediction=0, proba_class_0=1.0\n\n            neighbor_info[\"prediction\"] = prediction # Store prediction\n            logger.debug(\"  Neighbor %d: Predicted=%d (P(0)=%.3f)\", neighbors_processed_count, prediction, proba_class_0)\n\n            # 2. Decide whether to perform expensive check\n            perform_expensive_check = False\n            check_reason = \"\"\n\n            if prediction == 1:\n                perform_expensive_check = True\n                check_reason = \"Predicted 1\"\n            elif proba_class_0 &lt; check_proba_threshold:\n                perform_expensive_check = True\n                check_reason = f\"Borderline P(0) &lt; {check_proba_threshold}\"\n            else: # prediction == 0 and proba_class_0 &gt;= threshold\n                logger.debug(\"  -&gt; Skipping objective function call (Confident P=0).\")\n\n            # 3. Perform EXPENSIVE check if needed\n            if perform_expensive_check:\n                logger.debug(\"  -&gt; Verifying (%s)...\", check_reason)\n                try:\n                    objectives_neighbor = calculate_objective_serv_time_lookup(neighbor, **obj_func_params)\n                    cost_neighbor = w * objectives_neighbor[0] + (1 - w) * objectives_neighbor[1]\n                    is_truly_better = cost_neighbor &lt; cost_star\n                    true_label = 1 if is_truly_better else 0\n\n                    # Store results in neighbor_info\n                    neighbor_info[\"cost\"] = cost_neighbor\n                    neighbor_info[\"true_label\"] = true_label\n\n                    logger.debug(\"     True Cost=%.4f (Current Best=%.4f) -&gt; Actual Better=%s\",\n                                 cost_neighbor, cost_star, is_truly_better)\n\n                    # 4. Check for Misprediction (False Positive or False Negative)\n                    misprediction = (prediction == 1 and not is_truly_better) or \\\n                                    (prediction == 0 and is_truly_better)\n\n                    opportunistic_update_occurred = False\n                    if misprediction:\n                        misprediction_type = \"False Positive (P=1, A=0)\" if prediction == 1 else \"False Negative (P=0, A=1)\"\n                        logger.warning(\"     Misprediction! (%s). Triggering retraining process.\", misprediction_type)\n\n                        # --- Retraining Sub-routine ---\n                        features_for_update: List[List[int]] = []\n                        labels_for_update: List[int] = []\n                        best_opportunistic_neighbor = None\n                        best_opportunistic_cost = cost_star # Initialize with current best cost\n\n                        logger.info(\"     Calculating true costs for %d neighbors at level %d for retraining...\",\n                                    len(neighbors_data_at_level_t), t)\n\n                        for n_idx, n_info in enumerate(neighbors_data_at_level_t):\n                            n_schedule = n_info[\"schedule\"]\n                            n_cost = n_info[\"cost\"]\n                            n_true_label = n_info[\"true_label\"]\n\n                            # Check feature length compatibility again before using for training\n                            n_feature_len = len(x_star) + len(n_schedule)\n                            if expected_feature_len is not None and n_feature_len != expected_feature_len:\n                                logger.warning(\"       Skipping neighbor %d during retraining prep due to length mismatch.\", n_idx+1)\n                                continue\n\n                            # Calculate cost if not already done (e.g., for neighbors skipped earlier)\n                            if n_cost is None or n_true_label is None:\n                                try:\n                                    logger.debug(\"       Calculating missing cost for neighbor %d...\", n_idx+1)\n                                    n_objectives = calculate_objective_serv_time_lookup(n_schedule, **obj_func_params)\n                                    n_cost = w * n_objectives[0] + (1 - w) * n_objectives[1]\n                                    n_is_better = n_cost &lt; cost_star\n                                    n_true_label = 1 if n_is_better else 0\n                                    n_info[\"cost\"] = n_cost # Update info cache\n                                    n_info[\"true_label\"] = n_true_label\n                                except Exception as e:\n                                    logger.warning(\"       Error calculating cost for neighbor %d (%s) during retraining: %s. Skipping.\",\n                                                   n_idx+1, n_schedule, e)\n                                    continue # Skip this neighbor for training data\n\n                            # Prepare data for fitting\n                            n_feature_pair = x_star + n_schedule\n                            features_for_update.append(n_feature_pair)\n                            labels_for_update.append(n_true_label)\n                            logger.debug(\"       Neighbor %d: Cost=%.4f, True Label=%d (Used for training)\",\n                                         n_idx+1, n_cost, n_true_label)\n\n                            # Check for opportunistic update\n                            if n_true_label == 1 and n_cost &lt; best_opportunistic_cost:\n                                logger.info(\"       Opportunistic Update Candidate! Found/Confirmed better neighbor (%d) during cost calculation.\", n_idx+1)\n                                best_opportunistic_neighbor = list(n_schedule) # Store the schedule\n                                best_opportunistic_cost = n_cost # Update best cost found *during retraining*\n                                opportunistic_update_occurred = True\n\n\n                        # Perform incremental fit if data was gathered\n                        if features_for_update:\n                            logger.info(\"     Fitting model incrementally with %d data points...\", len(labels_for_update))\n                            try:\n                                X_update = np.array(features_for_update)\n                                y_update = np.array(labels_for_update)\n                                # Ensure clf is fitted before incremental update if it's the first time\n                                if not getattr(clf, 'fitted_', False) and hasattr(clf, '_Booster'):\n                                     # This check might be needed depending on XGBoost version and initial state\n                                     logger.warning(\"Classifier seems unfitted before incremental update. Attempting initial fit.\")\n                                     # This might require a baseline dataset - handle appropriately\n                                     # For now, we assume clf is pre-fitted or handles this internally.\n\n                                clf.fit(X_update, y_update, xgb_model=clf.get_booster()) # Pass the existing booster\n                                logger.info(\"     Model update complete.\")\n                            except Exception as e:\n                                logger.exception(\"     Error during incremental model update: %s\", e)\n                        else:\n                            logger.warning(\"     No valid data gathered for retraining.\")\n\n                        # If an opportunistic update was found, apply it now\n                        if opportunistic_update_occurred:\n                             logger.info(f\"     Applying opportunistic update. New best: {best_opportunistic_neighbor} with cost = {best_opportunistic_cost}.\")\n                             x_star = best_opportunistic_neighbor # Use the best one found\n                             cost_star = best_opportunistic_cost\n                             T = len(x_star)\n                        # End of Retraining Sub-routine\n\n                    # 5. Handle Updates & Loop Control\n                    if opportunistic_update_occurred:\n                        found_better_solution_at_level_t = True # Mark improvement found\n                        t = 1 # Reset level\n                        restart_count += 1\n                        logger.info(\"     Restarting search from t=1 due to opportunistic update during retraining. Restart count: %d\", restart_count)\n                        break # Exit inner loop (for neighbor_np in neighborhood_iter)\n\n                    elif is_truly_better: # True Positive or handled False Negative (update to the current neighbor)\n                        logger.info(f\"     Confirmed better solution (or handled FN). Updating x_star to {neighbor} with cost = {cost_neighbor}.\")\n                        x_star = list(neighbor)\n                        cost_star = cost_neighbor\n                        T = len(x_star)\n                        found_better_solution_at_level_t = True\n                        t = 1 # Reset level\n                        restart_count += 1\n                        logger.info(\"     Restarting search from t=1. Restart count: %d\", restart_count)\n                        break # Exit inner loop (for neighbor_np in neighborhood_iter)\n\n                    # else: (Not truly better and no opportunistic update) -&gt; continue to next neighbor implicitly\n\n                except Exception as e:\n                    logger.warning(\"  Error calculating objective or handling result for neighbor %d (%s): %s.\",\n                                   neighbors_processed_count, neighbor, e)\n            # End of 'if perform_expensive_check:'\n\n        # --- End of neighbor loop (for neighbor_np in neighborhood_iter) ---\n\n        # If we finished the loop for level t without finding a better solution (or breaking early)\n        if not found_better_solution_at_level_t:\n            if neighbors_processed_count &gt; 0:\n                logger.info(\"No improving solution found or confirmed at level t=%d.\", t)\n            else:\n                logger.info(\"No neighbors generated or processed at level t=%d.\", t)\n            t += 1 # Move to the next neighborhood size level\n\n    # --- End of outer while loop ---\n    logger.info(\"Local search finished after %d restarts or reaching max size %d.\", restart_count, size)\n    logger.info(\"Final solution: %s\", x_star)\n    logger.info(\"Final cost: %.4f\", cost_star)\n\n    return x_star, clf\n\n\nfrom functions import calculate_objective_serv_time_lookup\nstart = time.time()\n# Define the path to the saved model\nmodel_path = \"models/classifier_large_instance.json\" # Make sure this path is correct\n\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n)\n\n# Load the model directly from the file path\nclf.load_model(model_path)\n\nintial_objectives = calculate_objective_serv_time_lookup(x, d, convolutions)\ninitial_c_star = w * intial_objectives[0] + (1 - w) * intial_objectives[1]\nx_star = local_search_predict_update(x, w, v_star, clf, {'d': d, 'convolutions': convolutions}, size=T, restarts=T)[0]\nfinal_objectives = calculate_objective_serv_time_lookup(x_star, d, convolutions)\nfinal_c_star = w * final_objectives[0] + (1 - w) * final_objectives[1]\nend = time.time()\nprint(f\"\\nInitial schedule: {x}, with objective value: {initial_c_star}.\\nFinal schedule: {x_star}, with objective value: {final_c_star}. Search time {end - start:.2f} seconds.\")\n\n\nInitial schedule: [3, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 6], with objective value: 72.06648924027994.\nFinal schedule: [3, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 5], with objective value: 71.44063501950922. Search time 463.91 seconds.\n\n\n\n\n5.5.4 Run the conventional local search algorithm for validation\nWe will run a conventional local search algorithm to evaluate the new method, assessing both the quality of the results and its computational efficiency.\n\nfrom functions import local_search\n# Computing optimal solution with real cost\nprint(f\"Initial schedule: {x}\")\nstart = time.time()\ntest_x = local_search(x, d, convolutions, w, v_star, T, echo=True)\nend = time.time()\n\nInitial schedule: [3, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 6]\nInitial solution: [3 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 6], cost: 72.06648924027994\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 15\nFound better solution: [3 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 6], cost: 72.05397911405149\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 15\nFound better solution: [3 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 6], cost: 71.89864633967379\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 15\nFound better solution: [3 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 6], cost: 71.89529812336691\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 15\nFound better solution: [3 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 6], cost: 71.82107961062698\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 15\nFound better solution: [3 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 6], cost: 71.77211054845532\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 15\nFound better solution: [3 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 6], cost: 71.75446345503127\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 15\nFound better solution: [3 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 5], cost: 71.44063501950922\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 16\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 124\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 620\nRunning local search with switching 4 patient(s)\nSize of neighborhood: 2246\nRunning local search with switching 5 patient(s)\nSize of neighborhood: 6272\nRunning local search with switching 6 patient(s)\nSize of neighborhood: 14018\nRunning local search with switching 7 patient(s)\nSize of neighborhood: 25688\nRunning local search with switching 8 patient(s)\nSize of neighborhood: 39209\nRunning local search with switching 9 patient(s)\nSize of neighborhood: 50348\nRunning local search with switching 10 patient(s)\nSize of neighborhood: 54692\nRunning local search with switching 11 patient(s)\nSize of neighborhood: 50348\nRunning local search with switching 12 patient(s)\nSize of neighborhood: 39209\nRunning local search with switching 13 patient(s)\nSize of neighborhood: 25688\nRunning local search with switching 14 patient(s)\nSize of neighborhood: 14018\nFound better solution: [2 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 5], cost: 71.41729696172152\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 17\nFound better solution: [2 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 5], cost: 71.3263484363905\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 17\nFound better solution: [2 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 5], cost: 71.32108910644956\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 17\nFound better solution: [2 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 5], cost: 71.15932446024479\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 17\nFound better solution: [2 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 5], cost: 71.15324024190812\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 17\nFound better solution: [2 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 5], cost: 71.0973710860295\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 17\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 139\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 728\nRunning local search with switching 4 patient(s)\nSize of neighborhood: 2743\nRunning local search with switching 5 patient(s)\nSize of neighborhood: 7913\nRunning local search with switching 6 patient(s)\nSize of neighborhood: 18152\nRunning local search with switching 7 patient(s)\nSize of neighborhood: 33931\nRunning local search with switching 8 patient(s)\nSize of neighborhood: 52520\nRunning local search with switching 9 patient(s)\nSize of neighborhood: 68003\nRunning local search with switching 10 patient(s)\nSize of neighborhood: 74074\nRunning local search with switching 11 patient(s)\nSize of neighborhood: 68003\nRunning local search with switching 12 patient(s)\nSize of neighborhood: 52520\nRunning local search with switching 13 patient(s)\nSize of neighborhood: 33931\nRunning local search with switching 14 patient(s)\nSize of neighborhood: 18152\nRunning local search with switching 15 patient(s)\nSize of neighborhood: 7913\nRunning local search with switching 16 patient(s)\nSize of neighborhood: 2743\nRunning local search with switching 17 patient(s)\nSize of neighborhood: 728\nRunning local search with switching 18 patient(s)\nSize of neighborhood: 139\nRunning local search with switching 19 patient(s)\nSize of neighborhood: 17\n\n\n\nprint(f\"Initial schedule: {x}\\nFinal schedule: {test_x[0]}\\nDifference: {test_x[0] - x}\\nObjective value: {test_x[1]}. Search time: {end - start:.2f} seconds.\")\ntest_res = calculate_objective_serv_time_lookup(test_x[0], d, convolutions)\n\nInitial schedule: [3, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 6]\nFinal schedule: [2 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 5]\nDifference: [-1  1  0  1 -1  0  0  0  0  0  0  1  0 -1  0  1  0  0  0 -1]\nObjective value: 71.0973710860295. Search time: 348.37 seconds.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#discussion",
    "href": "local-search-ranking-large.html#discussion",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.6 Discussion",
    "text": "5.6 Discussion",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#timeline",
    "href": "local-search-ranking-large.html#timeline",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.7 Timeline",
    "text": "5.7 Timeline\nThis experiment was started on 01-04-2025 and concluded on 17-04-2025",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "local-search-ranking-large.html#references",
    "href": "local-search-ranking-large.html#references",
    "title": "5  Large instance local search with trained XGBoost regressor model",
    "section": "5.8 References",
    "text": "5.8 References",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Large instance local search with trained XGBoost regressor model</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "6  References",
    "section": "",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "service-time-with-no-shows.html",
    "href": "service-time-with-no-shows.html",
    "title": "service_time_with_no_shows",
    "section": "",
    "text": "Function Documentation\nservice_time_with_no_shows(s: List[float], q: float) -&gt; List[float]",
    "crumbs": [
      "Function documentation",
      "`service_time_with_no_shows`"
    ]
  },
  {
    "objectID": "service-time-with-no-shows.html#function-documentation",
    "href": "service-time-with-no-shows.html#function-documentation",
    "title": "service_time_with_no_shows",
    "section": "",
    "text": "Description\nAdjusts a distribution of service times to account for no-shows. The function scales the original service time distribution by the probability of a patient showing up (i.e., 1 - q) and then adds the no-show probability q to the service time for zero time slots.\n\n\nParameters\n\ns (List[float]): The original service time probability distribution. This list represents the probabilities associated with different service times.\nq (float): The probability of no-shows. This value should be between 0 and 1.\n\n\n\nReturns\n\nList[float]: The adjusted service time probability distribution where the no-show probability has been incorporated into the probability of zero service time.\n\n\n\nExample\n\nfrom functions import service_time_with_no_shows\n\n# Example usage\noriginal_distribution = [0.0, 0.5, 0.3, 0.2]\nno_show_probability = 0.1\nadjusted_distribution = service_time_with_no_shows(original_distribution, no_show_probability)\nprint(\"Adjusted distribution:\", adjusted_distribution)\n\nAdjusted distribution: [0.1, 0.45, 0.27, 0.18000000000000002]\n\n\n\nimport unittest\n\nclass TestServiceTimeWithNoShows(unittest.TestCase):\n    def test_adjust_distribution(self):\n        # Test with a known distribution and no-show probability\n        original_distribution = [0.0, 0.5, 0.3, 0.2]\n        no_show_probability = 0.1\n        \n        # Expected adjustment: second element 0.1, \n        # other elements: multiplied by 0.9\n        expected_distribution = [0.1, 0.45, 0.27, 0.18]\n        \n        result = service_time_with_no_shows(original_distribution, no_show_probability)\n        \n        # Using almost equal check due to floating point arithmetic\n        for r, e in zip(result, expected_distribution):\n            self.assertAlmostEqual(r, e, places=5)\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`service_time_with_no_shows`"
    ]
  },
  {
    "objectID": "compute-convolutions.html",
    "href": "compute-convolutions.html",
    "title": "compute_convolutions",
    "section": "",
    "text": "Function Documentation\ncompute_convolutions(probabilities: List[float], N: int, q: float = 0.0) -&gt; Dict[int, np.ndarray]",
    "crumbs": [
      "Function documentation",
      "`compute_convolutions`"
    ]
  },
  {
    "objectID": "compute-convolutions.html#function-documentation",
    "href": "compute-convolutions.html#function-documentation",
    "title": "compute_convolutions",
    "section": "",
    "text": "Description\nComputes the k-fold convolution of a given probability mass function (PMF) for k from 1 up to N. Before computing the convolutions, the PMF is adjusted for no-shows using the provided no-show probability q via the service_time_with_no_shows function. Convolution is performed using NumPy’s np.convolve.\n\n\nParameters\n\nprobabilities (List[float]): The original PMF represented as a list where the index corresponds to a value (for instance, a service time) and the value at that index is its probability. This function is generic and does not have to be used solely for service times.\nN (int): The maximum number of convolutions to compute.\nq (float, optional): The probability of a no-show. Defaults to 0.0.\n\n\n\nReturns\n\nDict[int, np.ndarray]: A dictionary where each key k (with 1 ≤ k ≤ N) corresponds to the PMF resulting from the k-fold convolution of the adjusted PMF.\n\n\n\nExample\n\nimport numpy as np\nfrom functions import compute_convolutions, service_time_with_no_shows\n\n# Example usage\noriginal_pmf = [0.0, 0.5, 0.3, 0.2]\nN = 3\nno_show_probability = 0.1\n\nconvs = compute_convolutions(original_pmf, N, no_show_probability)\nfor k, pmf in convs.items():\n    print(f\"{k}-fold convolution: {pmf}\")\n\n1-fold convolution: [0.1  0.45 0.27 0.18]\n2-fold convolution: [0.01   0.09   0.2565 0.279  0.2349 0.0972 0.0324]\n3-fold convolution: [0.001    0.0135   0.06885  0.169425 0.234495 0.236925 0.160623 0.083106\n 0.026244 0.005832]\n\n\n\nimport unittest\n\nclass TestComputeConvolutions(unittest.TestCase):\n    def test_single_convolution(self):\n        # When N = 1, the result should be the adjusted PMF\n        original_pmf = [0.0, 0.5, 0.3, 0.2]\n        no_show_probability = 0.1\n        N = 1\n        expected = np.array(service_time_with_no_shows(original_pmf, no_show_probability))\n        result = compute_convolutions(original_pmf, N, no_show_probability)\n        self.assertTrue(np.allclose(result[1], expected), \"Single convolution test failed\")\n\n    def test_multiple_convolutions(self):\n        # Test for N = 3 using a simple PMF\n        original_pmf = [0.0, 0.5, 0.3, 0.2]\n        no_show_probability = 0.0  # No adjustment for simplicity\n        N = 3\n        result = compute_convolutions(original_pmf, N, no_show_probability)\n\n        # For N=1, result is the original pmf\n        self.assertTrue(np.allclose(result[1], np.array(original_pmf)))\n\n        # For higher convolutions, ensure the sum of probabilities remains 1 (within numerical precision)\n        for k in range(1, N + 1):\n            self.assertAlmostEqual(np.sum(result[k]), 1.0, places=5, msg=f\"Sum of probabilities for {k}-fold convolution is not 1\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.001s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`compute_convolutions`"
    ]
  },
  {
    "objectID": "calculate-objective-serv-time-lookup.html",
    "href": "calculate-objective-serv-time-lookup.html",
    "title": "calculate_objective_serv_time_lookup",
    "section": "",
    "text": "Function Documentation\ncalculate_objective_serv_time_lookup(schedule: List[int], d: int, convolutions: dict) -&gt; Tuple[float, float]",
    "crumbs": [
      "Function documentation",
      "`calculate_objective_serv_time_lookup`"
    ]
  },
  {
    "objectID": "calculate-objective-serv-time-lookup.html#function-documentation",
    "href": "calculate-objective-serv-time-lookup.html#function-documentation",
    "title": "calculate_objective_serv_time_lookup",
    "section": "",
    "text": "Description\nThis notebook provides documentation for the function calculate_objective_serv_time_lookup, which calculates an objective value (in terms of expected waiting time and expected spillover) based on a given schedule and pre-computed convolutions of a probability mass function (PMF).\nThe function uses the following inputs:\n\nschedule: A list of integers representing the number of patients scheduled in each time slot.\nd: An integer indicating the duration threshold for a time slot.\nconvolutions: A dictionary of precomputed convolutions of the service time PMF. The key 1 should correspond to the adjusted service time distribution (for example, one adjusted for no-shows), while keys greater than 1 are used for multiple patients in a time slot.\n\nThe function returns a tuple:\n\newt: The sum of expected waiting times over the schedule.\nesp: The expected spillover time (or overtime) after the final time slot.",
    "crumbs": [
      "Function documentation",
      "`calculate_objective_serv_time_lookup`"
    ]
  },
  {
    "objectID": "calculate-objective-serv-time-lookup.html#example-usage",
    "href": "calculate-objective-serv-time-lookup.html#example-usage",
    "title": "calculate_objective_serv_time_lookup",
    "section": "Example Usage",
    "text": "Example Usage\nA trivial example using a precomputed convolution dictionary with a degenerate PMF (i.e. always zero service time) is provided in the unit tests below.\n\nimport numpy as np\nfrom typing import List, Dict, Tuple\nfrom functions import service_time_with_no_shows, compute_convolutions, calculate_objective_serv_time_lookup\n\n# For demonstration purposes, we use a trivial convolution dictionary.\noriginal_distribution = [0.0, 0.5, 0.3, 0.2]\nno_show_probability = 0.1\nadjusted_distribution = service_time_with_no_shows(original_distribution, no_show_probability)\nschedule_example = [2, 0, 0, 0, 0, 0, 1]\nN = sum(schedule_example)\nconvolutions_example = compute_convolutions(original_distribution, N, no_show_probability)\nd_example = 1\newt, esp = calculate_objective_serv_time_lookup(schedule_example, d_example, convolutions_example)\nprint(\"Adjusted Service Time Distribution: \", adjusted_distribution)\nprint(\"Expected Adjusted Service Time: \", np.dot(range(len(adjusted_distribution)), adjusted_distribution))\nprint(\"Expected Waiting Time:\", ewt)\nprint(\"Expected Spillover:\", esp)\n\nAdjusted Service Time Distribution:  [0.1, 0.45, 0.27, 0.18000000000000002]\nExpected Adjusted Service Time:  1.53\nExpected Waiting Time: 1.53\nExpected Spillover: 0.6300000000000001\n\n\n\nimport unittest\n\nclass TestCalculateObjectiveServTimeLookup(unittest.TestCase):\n    def setUp(self):\n        # Create a convolution dictionary\n        self.convolutions = convolutions_example\n        self.d = d_example\n\n    def test_single_time_slot(self):\n        # With one patient there will be no waiting and spillover (overtime) can be calculated by hand.\n        schedule = [1]\n        ewt, esp = calculate_objective_serv_time_lookup(schedule, self.d, self.convolutions)\n        self.assertAlmostEqual(ewt, 0.0, places=5, msg=\"Expected waiting time should be 0\")\n        self.assertAlmostEqual(esp, 0.6300000000000001, places=5, msg=\"Expected spillover should be 0\")\n\n    def test_zero_patients(self):\n        # If no patients are scheduled in a time slot, the process simply advances in time.\n        schedule = [0]\n        ewt, esp = calculate_objective_serv_time_lookup(schedule, self.d, self.convolutions)\n        self.assertAlmostEqual(ewt, 0.0, places=5, msg=\"Expected waiting time should be 0 when no patients\")\n        self.assertAlmostEqual(esp, 0.0, places=5, msg=\"Expected spillover should be 0 when no patients\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.001s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`calculate_objective_serv_time_lookup`"
    ]
  },
  {
    "objectID": "get-neighborhood.html",
    "href": "get-neighborhood.html",
    "title": "get_neighborhood",
    "section": "",
    "text": "Function Documentation\nget_neighborhood(x: Union[List[int], np.ndarray], v_star: np.ndarray, ids: List[List[int]], verbose: bool = False) -&gt; np.ndarray",
    "crumbs": [
      "Function documentation",
      "`get_neighborhood`"
    ]
  },
  {
    "objectID": "get-neighborhood.html#function-documentation",
    "href": "get-neighborhood.html#function-documentation",
    "title": "get_neighborhood",
    "section": "",
    "text": "Description\nThe get_neighborhood function computes a set of neighbor solutions by adding together selected rows from the array v_star to an initial solution vector x. The selection of rows is determined by the list of index lists ids, where each inner list represents a combination of indices. After generating the candidate neighbors, the function filters out any that contain negative values. An optional verbose flag provides debugging output during execution.\n\n\nParameters\n\nx (Union[List[int], np.ndarray]):\nThe current solution vector. Can be provided as a list of integers or as a NumPy array.\nv_star (np.ndarray):\nA 2D NumPy array where each row is an adjustment vector. These vectors are used to modify the current solution to explore its neighborhood.\nids (List[List[int]]):\nA list of index lists, where each inner list specifies which rows from v_star to sum together. Each combination represents a potential adjustment to the current solution.\nverbose (bool, optional):\nA flag indicating whether to print debugging information (e.g., intermediate computations, progress messages). Defaults to False.\n\n\n\nReturns\n\nnp.ndarray:\nA 2D NumPy array where each row is a neighbor solution (i.e., the result of adding a valid combination of adjustment vectors from v_star to x). Only neighbors with all non-negative entries are included in the output.\n\n\n\nExample\n\nimport numpy as np\nfrom functions import get_neighborhood, get_v_star, powerset\n\n# Define an initial solution vector\nx = [3, 2, 1]\n\n# Generate adjustment vectors using get_v_star\n# For instance, create a set of cyclic adjustment vectors of length 3\nv_star = get_v_star(3)\n\n# Generate combinations of indices (e.g., using a powerset for switching 1 patient)\nids = powerset(range(3), size=1)\n\n# Generate the neighborhood (neighbors with non-negative entries only)\nneighbors = get_neighborhood(x, v_star, ids, echo=True)\nprint(\"Neighbor solutions:\")\nprint(neighbors)\n\nPrinting every 50th result\nv_star[0]: [-1  0  1]\nx, x', delta:\n[3 2 1],\n[2 2 2],\n[-1  0  1]\n-----------------\nv_star[1]: [ 1 -1  0]\nv_star[2]: [ 0  1 -1]\nSize of raw neighborhood: 3\nFiltered out: 0 schedules with negative values.\nSize of filtered neighborhood: 3\nNeighbor solutions:\n[[2 2 2]\n [4 1 1]\n [3 3 0]]\n\n\n\nimport unittest\nimport numpy as np\nfrom functions import get_neighborhood, get_v_star, powerset\n\nclass TestGetNeighborhood(unittest.TestCase):\n    def test_non_negative_neighbors(self):\n        # Test with a simple solution vector and adjustment vectors\n        x = [3, 2, 1]\n        v_star = get_v_star(3)\n        ids = powerset(range(3), size=1)\n        \n        neighbors = get_neighborhood(x, v_star, ids, echo=False)\n        \n        # Ensure that no neighbor has negative entries\n        self.assertTrue(np.all(neighbors &gt;= 0), \"Some neighbor solutions contain negative values\")\n    \n    def test_neighborhood_shape(self):\n        # Test that the neighborhood returns a NumPy array with the proper dimensions\n        x = [3, 2, 1]\n        v_star = get_v_star(3)\n        ids = powerset(range(3), size=1)\n        neighbors = get_neighborhood(x, v_star, ids, echo=False)\n        self.assertIsInstance(neighbors, np.ndarray, \"Neighborhood is not a NumPy array\")\n        # The number of rows should equal the number of valid combinations in ids (after filtering negatives)\n        self.assertLessEqual(neighbors.shape[0], len(ids), \"Neighborhood size is larger than expected\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\n..\n----------------------------------------------------------------------\nRan 2 tests in 0.001s\n\nOK",
    "crumbs": [
      "Function documentation",
      "`get_neighborhood`"
    ]
  },
  {
    "objectID": "local-search.html",
    "href": "local-search.html",
    "title": "local_search",
    "section": "",
    "text": "Function Documentation\nlocal_search(x: Union[List[int], np.ndarray], d: int, convolutions: Dict[int, np.ndarray], w: float, v_star: np.ndarray, size: int = 2, echo: bool = False) -&gt; Tuple[np.ndarray, float]",
    "crumbs": [
      "Function documentation",
      "`local_search`"
    ]
  },
  {
    "objectID": "local-search.html#function-documentation",
    "href": "local-search.html#function-documentation",
    "title": "local_search",
    "section": "",
    "text": "Description\nThe local_search function optimizes a schedule by iteratively exploring its neighborhood. Starting with an initial solution x, the function computes its objective value using the precomputed convolutions of the service time probability mass function. The neighborhood is generated by combining adjustment vectors from v_star (using a powerset-based approach) and filtering out candidates that contain negative values. The search continues until no further improvement is found for neighborhoods up to the specified size. The objective function combines expected average waiting time per patient and spillover time weighted by w.\n\n\nParameters\n\nx (Union[List[int], np.ndarray]):\nThe initial solution vector representing the schedule. It can be provided as a list of integers or as a NumPy array.\nd (int):\nThe duration threshold for a time slot. It is used to adjust the service process and waiting time distribution.\nconvolutions (Dict[int, np.ndarray]):\nA dictionary containing precomputed convolutions of the service time PMF. The key 1 represents the adjusted service time distribution, and other keys represent the convolution for the corresponding number of scheduled patients.\nw (float):\nThe weighting factor for combining the two performance objectives: expected waiting time and expected spillover time.\nv_star (np.ndarray):\nA 2D NumPy array of adjustment vectors. Each row in v_star is used to modify the current solution vector in order to generate its neighborhood.\nsize (int, optional):\nThe maximum number of patients to switch (i.e., the size of the neighborhood to explore) during the local search. Defaults to 2.\necho (bool, optional):\nA flag that, when set to True, prints progress and debugging messages during the search process. Defaults to False.\n\n\n\nReturns\n\nTuple[np.ndarray, float]:\nA tuple containing:\n\nThe best solution found as a 1D NumPy array.\nThe corresponding cost (objective value) as a float.\n\n\n\n\nExample\n\nimport numpy as np\nfrom functions import local_search, calculate_objective_serv_time_lookup, compute_convolutions, get_v_star, powerset\n\nfrom typing import List, Dict, Tuple, Union\n\ndef ways_to_distribute(N: int, T: int) -&gt; List[List[int]]:\n    \"\"\"\n    Compute all possible ways to distribute N identical items into T bins.\n    \n    Each distribution is represented as a list of T nonnegative integers whose sum is N.\n    \n    Parameters:\n        N (int): Total number of identical items.\n        T (int): Number of bins.\n        \n    Returns:\n        List[List[int]]: A list of distributions. Each distribution is a list of T integers that sum to N.\n        \n    Example:\n        &gt;&gt;&gt; ways_to_distribute(3, 2)\n        [[0, 3], [1, 2], [2, 1], [3, 0]]\n    \"\"\"\n    # Base case: only one bin left, all items must go into it.\n    if T == 1:\n        return [[N]]\n    \n    distributions = []\n    # Iterate over possible numbers of items in the first bin\n    for i in range(N + 1):\n        # Recursively distribute the remaining items among the remaining bins.\n        for distribution in ways_to_distribute(N - i, T - 1):\n            distributions.append([i] + distribution)\n            \n    return distributions\n  \ndef choose_best_solution(solutions: List[np.ndarray], d: int, convs: Dict[int, np.ndarray], w: float, v_star: np.ndarray) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Choose the best solution from a list of solutions based on the objective function.\n    \n    Parameters:\n        solutions (List[np.ndarray]): A list of solution vectors.\n        d (int): Duration threshold for a time slot.\n        convs (Dict[int, np.ndarray]): Precomputed convolutions of the service time PMF.\n        w (float): Weighting factor for the objective function.\n        \n    Returns:\n        Tuple[np.ndarray, float]: The best solution and its corresponding cost.\n    \"\"\"\n    best_solution = None\n    best_cost = float('inf')\n    \n    for solution in solutions:\n        waiting_time, spillover = calculate_objective_serv_time_lookup(solution, d, convs)\n        cost = w * waiting_time /N + (1 - w) * spillover\n        if cost &lt; best_cost:\n            best_solution = solution\n            best_cost = cost\n            \n    return np.array(best_solution), best_cost\n\n# Example schedule: initial solution vector\nx_initial = [3, 2, 1, 0]\nT = len(x_initial)\nN = sum(x_initial)\n\n# Duration threshold for a time slot\nd = 5\n\n# Example probability mass function and no-show probability\nservice_time = np.zeros(11)\nservice_time[3] = 0.2\nservice_time[5] = 0.3\nservice_time[8] = 0.5\nq = 0.1\n\n# Compute convolutions (precomputed service time distributions)\nconvs = compute_convolutions(service_time, N=N, q=q)\n\n# Weighting factor for the objective function\nw = 0.5\n\n# Generate adjustment vectors for the schedule (v_star)\nv_star = get_v_star(len(x_initial))\n\n# Perform local search to optimize the schedule\nbest_solution, best_cost = local_search(x_initial, d, convs, w, v_star, size=T, echo=True)\n\nprint(\"Best Solution:\", best_solution)\nprint(\"Best Cost:\", best_cost)\n\nInitial solution: [3 2 1 0], cost: 37.71594467672401\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 3\nFound better solution: [2 2 1 1], cost: 30.52386358592401\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 4\nFound better solution: [1 2 1 2], cost: 25.53066071370001\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 4\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 6\nFound better solution: [1 1 1 3], cost: 22.474543162500005\nRunning local search with switching 1 patient(s)\nSize of neighborhood: 4\nRunning local search with switching 2 patient(s)\nSize of neighborhood: 6\nRunning local search with switching 3 patient(s)\nSize of neighborhood: 4\nBest Solution: [1 1 1 3]\nBest Cost: 22.474543162500005\n\n\n\nimport unittest\nimport numpy as np\nfrom functions import local_search, compute_convolutions, get_v_star\n\nclass TestLocalSearch(unittest.TestCase):\n    def test_local_search_improvement(self):\n        # Set up a simple test with a known schedule and parameters\n        x_initial = [3, 2, 1, 0]\n        T = len(x_initial)\n        N = sum(x_initial)\n        d = 5\n        service_time = np.zeros(11)\n        service_time[3] = 0.2\n        service_time[5] = 0.3\n        service_time[8] = 0.5\n        q = 0.1\n        convs = compute_convolutions(service_time, N=N, q=q)\n        w = 0.5\n        v_star = get_v_star(len(x_initial))\n        \n        # Perform local search\n        best_solution, best_cost = local_search(x_initial, d, convs, w, v_star, size=T, echo=False)\n        print(\"Best Solution:\", best_solution, \"Best Cost:\", best_cost)\n        \n        # Iterate over all solutions and choose best solution\n        solutions = ways_to_distribute(N, T)\n        best_solution_brute, best_cost_brute = choose_best_solution(solutions, d, convs, w, v_star)\n        print(\"Best Brute-force Solution:\", best_solution_brute, \"Best Brute-force Cost:\", best_cost_brute)\n        \n        # Verify that the local search solution is equal to the brute-force solution\n        self.assertTrue(np.array_equal(best_solution, best_solution_brute), \"The local search solution should match the brute-force solution.\")\n        \n        # Verify that the returned solution has the same length as the initial schedule\n        self.assertEqual(len(best_solution), len(x_initial), \"The optimized solution should have the same length as the initial solution.\")\n        \n        # Check that the cost is a float and that a solution is returned\n        self.assertIsInstance(best_cost, float, \"Cost should be a float value.\")\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n\nInitial solution: [3 2 1 0], cost: 37.71594467672401\nBest Solution: [1 1 1 3] Best Cost: 22.474543162500005\nBest Brute-force Solution: [2 1 1 2] Best Brute-force Cost: 9.894705450000002\n\n\nF\n======================================================================\nFAIL: test_local_search_improvement (__main__.TestLocalSearch.test_local_search_improvement)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/var/folders/gf/gtt1mww524x0q33rqlwsmjw80000gn/T/ipykernel_3116/3482521855.py\", line 31, in test_local_search_improvement\n    self.assertTrue(np.array_equal(best_solution, best_solution_brute), \"The local search solution should match the brute-force solution.\")\nAssertionError: False is not true : The local search solution should match the brute-force solution.\n\n----------------------------------------------------------------------\nRan 1 test in 0.013s\n\nFAILED (failures=1)",
    "crumbs": [
      "Function documentation",
      "`local_search`"
    ]
  }
]