{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Large instance XGBoost classification model for pairwise ranking\n",
        "---"
      ],
      "id": "96175de7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective\n",
        "\n",
        "**Objective**: *Testing the performance of an XGBoost model trained for ranking pairwise schedules.*\n",
        "\n",
        "## Background\n",
        "\n",
        "*In this experiment we develop a Machine Learning model using XGBoost that can evaluate two neighboring schedules and rank them according to preference. This ranking model can be applied to quickly guide the search process towards a 'good enough' solution.*\n",
        "\n",
        "*The choice of using an ordinal model instead of a cardinal model is based on the consideration that it is significantly easier to determine whether alternative A is superior to B than to quantify the exact difference between A and B. This makes intuitive sense when considering the scenario of holding two identical-looking packages and deciding which one is heavier, as opposed to estimating the precise weight difference between them. [@ho_ordinal_2000].*\n",
        "\n",
        "## Hypothesis\n",
        "\n",
        "*An XGBoost ranking model achieves superior computational efficiency compared to evaluating each element of a pair individually, leading to faster overall performance in ranking tasks.*\n",
        "\n",
        "## Methodology\n",
        "\n",
        "### Tools and Materials\n",
        "\n",
        "*We use packages from [Scikit-learn](https://scikit-learn.org/stable/index.html) to prepare training data and evaluate the model and the `XGBClassifier` interface from the [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html) library.*\n"
      ],
      "id": "5f7eaf8f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "import math\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.base import clone\n",
        "import xgboost as xgb\n",
        "from xgboost.callback import TrainingCallback\n",
        "import plotly.graph_objects as go\n",
        "import pickle\n",
        "import random\n",
        "from scipy.optimize import minimize\n",
        "from itertools import combinations"
      ],
      "id": "61c39482",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experimental Design\n",
        "\n",
        "*To compare an XGBoost Machine Learning model with a simple evaluation of each individual element of the pair, we will use a pairwise ranking approach. The objective is to rank two neighboring schedules according to preference.*\n"
      ],
      "id": "3f34caf7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functions import compute_convolutions\n",
        "\n",
        "N = 22 # Number of patients\n",
        "T = 20 # Number of intervals\n",
        "d = 5 # Length of each interval\n",
        "max_s = 20 # Maximum service time\n",
        "q = 0.20 # Probability of a scheduled patient not showing up\n",
        "w = 0.1 # Weight for the waiting time in objective function\n",
        "l = 10\n",
        "num_schedules = 100000 # Number of schedules to sample\n",
        "\n",
        "# Create service time distribution\n",
        "def generate_weighted_list(max_s, l, i):\n",
        "    # Initialize an array of T+1 values, starting with zero\n",
        "    values = np.zeros(T + 1)\n",
        "    \n",
        "    # Objective function: Sum of squared differences between current weighted average and the desired l\n",
        "    def objective(x):\n",
        "        weighted_avg = np.dot(np.arange(1, T + 1), x) / np.sum(x)\n",
        "        return (weighted_avg - l) ** 2\n",
        "\n",
        "    # Constraint: The sum of the values from index 1 to T must be 1\n",
        "    constraints = ({\n",
        "        'type': 'eq',\n",
        "        'fun': lambda x: np.sum(x) - 1\n",
        "    })\n",
        "    \n",
        "    # Bounds: Each value should be between 0 and 1\n",
        "    bounds = [(0, 1)] * T\n",
        "\n",
        "    # Initial guess: Random distribution that sums to 1\n",
        "    initial_guess = np.random.dirichlet(np.ones(T))\n",
        "\n",
        "    # Optimization: Minimize the objective function subject to the sum and bounds constraints\n",
        "    result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "\n",
        "    # Set the values in the array (index 0 remains 0)\n",
        "    values[1:] = result.x\n",
        "\n",
        "    # Now we need to reorder the values as per the new requirement\n",
        "    first_part = np.sort(values[1:i+1])  # Sort the first 'i' values in ascending order\n",
        "    second_part = np.sort(values[i+1:])[::-1]  # Sort the remaining 'T-i' values in descending order\n",
        "    \n",
        "    # Combine the sorted parts back together\n",
        "    values[1:i+1] = first_part\n",
        "    values[i+1:] = second_part\n",
        "    \n",
        "    return values\n",
        "\n",
        "i = 5  # First 5 highest values in ascending order, rest in descending order\n",
        "s = generate_weighted_list(max_s, l, i)\n",
        "print(s)\n",
        "print(\"Sum:\", np.sum(s[1:]))  # This should be 1\n",
        "print(\"Weighted service time:\", np.dot(np.arange(1, T + 1), s[1:]))  # This should be close to l\n",
        "\n",
        "convolutions = compute_convolutions(s, N, q)\n",
        "file_path_parameters = f\"datasets/parameters_{N}_{T}_{l}.pkl\"\n",
        "with open(file_path_parameters, 'wb') as f:\n",
        "    pickle.dump({\n",
        "      'N': N,\n",
        "      'T': T,\n",
        "      'd': d,\n",
        "      'max_s': max_s,\n",
        "      'q': q,\n",
        "      'w': w,\n",
        "      'l': l,\n",
        "      'num_schedules': num_schedules,\n",
        "      'convolutions': convolutions\n",
        "      }, f)\n",
        "    print(f\"Data saved successfully to '{file_path_parameters}'\")"
      ],
      "id": "6509d633",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*We will create a random set of pairs of neighboring schedules with* $N = `{python} N`$ patients and $T = `{python} T`$ intervals of length $d = `{python} d`$.\n",
        "\n",
        "*A neighbor of a schedule x is considered a schedule x' where single patients have been shifted one interval to the left. Eg: (\\[2,1,1,2\\], \\[1,2,0,3\\]) are neighbors and (\\[2,1,1,2\\], \\[2,1,3,0\\]) are not, because \\[1,2,0,3\\] - \\[2,1,1,2\\] = \\[-1, 1, -1, 1\\] and \\[2,1,3,0\\] - \\[2,1,1,2\\] = \\[0, 0, 2, -2\\].*\n",
        "\n",
        "*Service times will have a discrete distribution. The probability of a scheduled patient not showing up will be* $q = `{python} q`$.\n",
        "\n",
        "*The objective function will be the weighted average of the total waiting time of all patients and overtime. The model will be trained to predict which of the two neighboring schedules has the lowest objective value. The prediction time will be recorded. Then the same schedules will be evaluated by computing the objective value and then ranked.*\n",
        "\n",
        "### Variables\n",
        "\n",
        "-   **Independent Variables**: *A list of tuples with pairs of neighboring schedules.*\n",
        "-   **Dependent Variables**: *A list with rankings for each tuple of pairwise schedules. Eg: If the rank for (\\[2,1,1\\], \\[1,1,2\\]) equals 0 this means that the schedule with index 0 (\\[2,1,1\\]) has the lowest objective value.*\n",
        "\n",
        "### Data Collection\n",
        "\n",
        "*The data set will be generated using simulation in which random samples will be drawn from the population of all possible schedules. For each sample a random neighboring schedule will be created.*\n",
        "\n",
        "### Sample Size and Selection\n",
        "\n",
        "**Sample Size**: *The total population size equals* ${{N + T -1}\\choose{N}} \\approx$ `{python} round(math.comb(N + T - 1, N) / 1000000,0)` mln. For this experiment we will be using a relatively small sample of `{python} num_schedules` pairs of schedules.\n",
        "\n",
        "**Sample Selection**: *The samples will be drawn from a lexicographic order of possible schedules in order to accurately reflect the combinatorial nature of the problem and to ensure unbiased sampling from the entire combinatorial space.*\n",
        "\n",
        "### Experimental Procedure\n",
        "\n",
        "*The experiment involves multiple steps, beginning with data preparation and concluding with model evaluation.The diagram below illustrates the sequence of steps.*\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "graph TD\n",
        "    A[\"From population\"] -->|\"Sample\"| B[\"Random subset\"]\n",
        "    B --> |Create neighbors| C[\"Features: Schedule pairs\"]\n",
        "    C --> |Calculate objectives| D[\"Objective values\"]\n",
        "    D --> |Rank objectives| E[\"Labels: Rankings\"]\n",
        "    E --> |\"Split dataset\"| F[\"Training set\"]\n",
        "    E --> |\"Split dataset\"| G[\"Test set\"]\n",
        "    F --> |\"Train\"| H[\"Model\"]\n",
        "    H[\"Model\"] --> |\"Apply\"| G[\"Test set\"]\n",
        "    G[\"Test set\"] --> |\"Evaluate\"| I[\"Performance\"]\n",
        "```\n",
        "\n",
        "\n",
        "**Step 1**: *Randomly select a subset of schedules.*\n"
      ],
      "id": "f6cb3f28"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functions import create_random_schedules\n",
        "\n",
        "start = time.time()\n",
        "# schedules = random_combination_with_replacement(T, N, num_schedules)\n",
        "schedules = create_random_schedules(T, N, num_schedules)\n",
        "print(f\"Sampled: {len(schedules):,} schedules\\n\")\n",
        "h = random.choices(range(len(schedules)), k=7)\n",
        "print(f\"Sampled schedules: {h}\")\n",
        "for i in h:\n",
        "    print(f\"Schedule: {schedules[i]}\")\n",
        "end = time.time()\n",
        "data_prep_time = end - start\n",
        "\n",
        "print(f\"\\nProcessing time: {data_prep_time} seconds\\n\")"
      ],
      "id": "bcbe06b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2**: *Create pairs of neighboring schedules.*\n"
      ],
      "id": "7132274b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functions import get_v_star\n",
        "\n",
        "def create_neighbors_list(s: list[int], v_star: np.ndarray) -> (list[int], list[int]):\n",
        "    \"\"\"\n",
        "    Create a set of pairs of schedules that are from the same neighborhood.\n",
        "    \n",
        "    Parameters:\n",
        "      s (list[int]): A list of integers with |s| = T and sum N.\n",
        "      v_star (np.ndarray): Precomputed vectors V* of length T.\n",
        "      \n",
        "    Returns:\n",
        "      tuple(list[int], list[int]): A pair of schedules.\n",
        "    \"\"\"\n",
        "    T = len(s)\n",
        "\n",
        "    # Precompute binomial coefficients (weights for random.choices)\n",
        "    binom_coeff = [math.comb(T, i) for i in range(1, T)]\n",
        "\n",
        "    # Choose a random value of i with the corresponding probability\n",
        "    i = random.choices(range(1, T), weights=binom_coeff)[0]\n",
        "\n",
        "    # Instead of generating the full list of combinations, sample one directly\n",
        "    j = random.sample(range(T), i)\n",
        "    \n",
        "    s_p = s.copy()\n",
        "    for k in j:\n",
        "        s_temp = np.array(s_p) + v_star[k]\n",
        "        s_temp = s_temp.astype(int)\n",
        "        if np.all(s_temp >= 0):\n",
        "            s_p = s_temp.astype(int).tolist()\n",
        "        \n",
        "    return s, s_p\n",
        "\n",
        "start = time.time()\n",
        "v_star = get_v_star(T)\n",
        "neighbors_list = [create_neighbors_list(schedule, v_star) for schedule in schedules] # This can be done in parellel to improve speed\n",
        "end = time.time()\n",
        "for i in h:\n",
        "    original_schedule = neighbors_list[i][0]\n",
        "    neighbor_schedule = neighbors_list[i][1]\n",
        "    difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]\n",
        "    print(f\"Neighbors\\n{original_schedule}\\n{neighbor_schedule}\\n{difference}\")\n",
        "training_set_feat_time = end - start\n",
        "print(f\"\\nProcessing time: {training_set_feat_time} seconds\\n\")"
      ],
      "id": "846ea42b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3**: *For each schedule in each pair calculate the objective. For each pair save the index of the schedule that has the lowest objective value.*\n"
      ],
      "id": "6af60968"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functions import calculate_objective_serv_time_lookup\n",
        "\n",
        "objectives_schedule_1 = [\n",
        "    w * result[0] + (1 - w) * result[1]\n",
        "    for neighbor in neighbors_list\n",
        "    for result in [calculate_objective_serv_time_lookup(neighbor[0], d, convolutions)]\n",
        "]\n",
        "start = time.time()\n",
        "objectives_schedule_2 = [\n",
        "    w * result[0] + (1 - w) * result[1]\n",
        "    for neighbor in neighbors_list\n",
        "    for result in [calculate_objective_serv_time_lookup(neighbor[1], d, convolutions)]\n",
        "]\n",
        "end = time.time()\n",
        "training_set_lab_time = end - start\n",
        "objectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]\n",
        "rankings = np.argmin(objectives, axis=1).tolist()\n",
        "for i in range(5):\n",
        "    print(f\"Objectives: {objectives[i]}, Ranking: {rankings[i]}\")\n",
        "\n",
        "print(f\"\\nProcessing time: {training_set_lab_time} seconds\\n\")\n",
        "\n",
        "# Saving neighbors_list and objectives to a pickle file\n",
        "\n",
        "file_path_neighbors = f\"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl\"\n",
        "with open(file_path_neighbors, 'wb') as f:\n",
        "    pickle.dump({'neighbors_list': neighbors_list, 'objectives': objectives, 'rankings': rankings}, f)\n",
        "    print(f\"Data saved successfully to '{file_path_neighbors}'\")"
      ],
      "id": "8bb9c49a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4**: *Create training and test sets.*\n"
      ],
      "id": "cdbe5a78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare the dataset\n",
        "X = []\n",
        "for neighbors in neighbors_list:\n",
        "    X.append(neighbors[0] + neighbors[1])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(rankings)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "id": "eecfa9a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 5**: *Train the XGBoost model.*\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "flowchart TD\n",
        "    A[Start] --> B[Initialize StratifiedKFold]\n",
        "    B --> C[Initialize XGBClassifier]\n",
        "    C --> D[Set results as empty list]\n",
        "    D --> E[Loop through each split of cv split]\n",
        "    E --> F[Get train and test indices]\n",
        "    F --> G[Split X and y into X_train, X_test, y_train, y_test]\n",
        "    G --> H[Clone the classifier]\n",
        "    H --> I[Call fit_and_score function]\n",
        "    I --> J[Fit the estimator]\n",
        "    J --> K[Score on training set]\n",
        "    J --> L[Score on test set]\n",
        "    K --> M[Return estimator, train_score, test_score]\n",
        "    L --> M\n",
        "    M --> N[Append the results]\n",
        "    N --> E\n",
        "    E --> O[Loop ends]\n",
        "    O --> P[Print results]\n",
        "    P --> Q[End]\n",
        "```"
      ],
      "id": "b13dd61f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class CustomCallback(TrainingCallback):\n",
        "    def __init__(self, period=10):\n",
        "        self.period = period\n",
        "\n",
        "    def after_iteration(self, model, epoch, evals_log):\n",
        "        if (epoch + 1) % self.period == 0:\n",
        "            print(f\"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}\")\n",
        "        return False\n",
        "    \n",
        "def fit_and_score(estimator, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n",
        "    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0\n",
        "    )\n",
        "\n",
        "    train_score = estimator.score(X_train, y_train)\n",
        "    test_score = estimator.score(X_test, y_test)\n",
        "\n",
        "    return estimator, train_score, test_score\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n",
        "\n",
        "# Initialize the XGBClassifier without early stopping here\n",
        "# Load the best trial parameters from a JSON file.\n",
        "with open(\"model_params.json\", \"r\") as f:\n",
        "    model_params = json.load(f)\n",
        "    \n",
        "# Initialize the EarlyStopping callback with validation dataset\n",
        "early_stop = xgb.callback.EarlyStopping(\n",
        "    rounds=10, metric_name='logloss', data_name='validation_0', save_best=True\n",
        ")\n",
        "\n",
        "clf = xgb.XGBClassifier(\n",
        "    tree_method=\"hist\",\n",
        "    max_depth=model_params[\"max_depth\"],\n",
        "    min_child_weight=model_params[\"min_child_weight\"],\n",
        "    gamma=model_params[\"gamma\"],\n",
        "    subsample=model_params[\"subsample\"],\n",
        "    colsample_bytree=model_params[\"colsample_bytree\"],\n",
        "    learning_rate=model_params[\"learning_rate\"],\n",
        "    n_estimators=model_params[\"n_estimators\"],\n",
        "    early_stopping_rounds=9,\n",
        "    #callbacks=[CustomCallback(period=50), early_stop],\n",
        "    callbacks=[CustomCallback(period=50)],\n",
        ")\n",
        "print(\"Params: \")\n",
        "for key, value in model_params.items():\n",
        "    print(f\" {key}: {value}\")\n",
        "\n",
        "start = time.time()\n",
        "results = []\n",
        "\n",
        "for train_idx, test_idx in cv.split(X, y):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "    est, train_score, test_score = fit_and_score(\n",
        "        clone(clf), X_train, X_test, y_train, y_test\n",
        "    )\n",
        "    results.append((est, train_score, test_score))\n",
        "end = time.time()\n",
        "training_time = end - start\n",
        "print(f\"\\nTraining time: {training_time} seconds\\n\")"
      ],
      "id": "fd54bcd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 6**: *To evaluate the performance of the XGBoost ranking model, we will use Stratified K-Fold Cross-Validation with 5 splits, ensuring each fold maintains the same class distribution as the original dataset. Using StratifiedKFold(n_splits=5, shuffle=True, random_state=94), the dataset will be divided into five folds. In each iteration, the model will be trained on four folds and evaluated on the remaining fold. A custom callback, CustomCallback(period=10), will print the evaluation log every 10 epochs.*\n",
        "\n",
        "*The fit_and_score function will fit the model and score it on both the training and test sets, storing the results for each fold. This provides insight into the model's performance across different subsets of the data, helps in understanding how well the model generalizes to unseen data and identifies potential overfitting or underfitting issues. The overall processing time for the cross-validation will also be recorded.*\n"
      ],
      "id": "77fbb248"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print results\n",
        "for i, (est, train_score, test_score) in enumerate(results):\n",
        "    print(f\"Fold {i+1} - Train Score (Accuracy): {train_score:.4f}, Test Score (Accuracy): {test_score:.4f}\")"
      ],
      "id": "5b270921",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Training the model on the entire dataset provides a final model that has learned from all available data. Recording the training time helps in understanding the computational efficiency and scalability of the model with the given hyperparameters.*\n"
      ],
      "id": "d2c695c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit the model on the entire dataset\n",
        "# Initialize the XGBClassifier without early stopping here\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "clf = xgb.XGBClassifier(\n",
        "    tree_method=\"hist\",\n",
        "    max_depth=model_params[\"max_depth\"],\n",
        "    min_child_weight=model_params[\"min_child_weight\"],\n",
        "    gamma=model_params[\"gamma\"],\n",
        "    subsample=model_params[\"subsample\"],\n",
        "    colsample_bytree=model_params[\"colsample_bytree\"],\n",
        "    learning_rate=model_params[\"learning_rate\"],\n",
        "    n_estimators=model_params[\"n_estimators\"],\n",
        ")\n",
        "\n",
        "clf.fit(X, y)\n",
        "end= time.time()\n",
        "modeling_time = end - start\n",
        "clf.save_model('models/classifier_large_instance.json')\n",
        "\n",
        "# Calculate and print the training accuracy\n",
        "training_accuracy = clf.score(X, y)\n",
        "print(f\"Training accuracy: {training_accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "print(f\"\\nTraining time: {modeling_time} seconds\\n\")"
      ],
      "id": "30dd5ced",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation\n",
        "\n",
        "*Generating test schedules and calculating their objectives and rankings allows us to create a new dataset for evaluating the model's performance on unseen data.*\n"
      ],
      "id": "c396430f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_test_schedules = 1000\n",
        "\n",
        "#test_schedules = random_combination_with_replacement(T, N, num_test_schedules)\n",
        "test_schedules = create_random_schedules(T, N, num_test_schedules)\n",
        "\n",
        "test_neighbors = [create_neighbors_list(test_schedule, v_star) for test_schedule in test_schedules] # This can be done in parellel to improve speed\n",
        "\n",
        "print(f\"Sampled: {len(test_schedules)} schedules\\n\")\n",
        "\n",
        "test_objectives_schedule_1 = [\n",
        "    w * result[0] + (1 - w) * result[1]\n",
        "    for test_neighbor in test_neighbors\n",
        "    for result in [calculate_objective_serv_time_lookup(test_neighbor[0], d, convolutions)]\n",
        "]\n",
        "# Start time measurement for the evaluation\n",
        "start = time.time()\n",
        "test_objectives_schedule_2 = [\n",
        "    w * result[0] + (1 - w) * result[1]\n",
        "    for test_neighbor in test_neighbors\n",
        "    for result in [calculate_objective_serv_time_lookup(test_neighbor[1], d, convolutions)]\n",
        "]\n",
        "test_rankings = [0 if test_obj < test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]\n",
        "end = time.time()\n",
        "evaluation_time = end - start\n",
        "\n",
        "# Combine the objectives for each pair for later processing\n",
        "test_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]\n",
        "\n",
        "print(f\"\\nEvaluation time: {evaluation_time} seconds\\n\")\n",
        "\n",
        "for i in range(6):\n",
        "    print(f\"Neighbors: {test_neighbors[i]},\\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\\n\")"
      ],
      "id": "1f39e57a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Making predictions on new data and comparing them to the actual rankings provides an evaluation of the model's performance in practical applications. Recording the prediction time helps in understanding the model's efficiency during inference.*\n"
      ],
      "id": "e663e5fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "input_X = test_neighbors\n",
        "X_new = []\n",
        "for test_neighbor in input_X:\n",
        "    X_new.append(test_neighbor[0] + test_neighbor[1])\n",
        "    \n",
        "# Predict the target for new data\n",
        "y_pred = clf.predict(X_new)\n",
        "\n",
        "# Probability estimates\n",
        "start = time.time()\n",
        "y_pred_proba = clf.predict_proba(X_new)\n",
        "end = time.time()\n",
        "prediction_time = end - start\n",
        "print(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n",
        "\n",
        "print(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")"
      ],
      "id": "cfb767c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Calculating the ambiguousness of the predicted probabilities helps in understanding the model's confidence in its predictions. High ambiguousness indicates uncertain predictions, while low ambiguousness indicates confident predictions.*\n",
        "\n",
        "*Ambiguousness is calculated using the formula for entropy:*\n",
        "\n",
        "$$\n",
        "H(X) = - \\sum_{i} p(x_i) \\log_b p(x_i)\n",
        "$$\n",
        "\n",
        "*Where in our case:*\n",
        "\n",
        "-   $H(X)$ *is the ambiguousness of the random variable* $X$ *- the set of probability scores for the predicted rankings,*\n",
        "\n",
        "-   $p(x_i)$ *is probability score* $x_i$*,*\n",
        "\n",
        "-   $\\log_b$ *is the logarithm with base* $b$ *(here* $\\log_2$ *as we have two predicted values),*\n",
        "\n",
        "-   *The sum is taken over all possible outcomes of* $X$*.*\n",
        "\n",
        "*Calculating cumulative error rate and cumulative accuracy helps in understanding how the model's performance evolves over the dataset.*\n",
        "\n",
        "*Visualizing the relationship between ambiguousness and error provides insights into how uncertainty in the model's predictions correlates with its accuracy. This can help in identifying patterns and understanding the conditions under which the model performs well or poorly.*\n"
      ],
      "id": "d7e49f56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functions import calculate_ambiguousness\n",
        "\n",
        "errors = np.abs(y_pred - np.array(test_rankings))\n",
        "\n",
        "ambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\n",
        "df = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors}).sort_values(by=\"Ambiguousness\")\n",
        "df['Cumulative error rate'] = df['Error'].expanding().mean()\n",
        "# Calculate cumulative accuracy\n",
        "df['Cumulative accuracy'] = 1 - df['Cumulative error rate']\n",
        "df.head()\n",
        "\n",
        "\n",
        "# Create traces\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n",
        "                    mode=\"markers\",\n",
        "                    name=\"Error\",\n",
        "                    marker=dict(size=9)))\n",
        "fig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n",
        "                    mode=\"lines\",\n",
        "                    name=\"Cum. accuracy\",\n",
        "                    line = dict(width = 3, dash = 'dash')))\n",
        "fig.update_layout(\n",
        "    title={\n",
        "        'text': f\"Error vs Ambiguousness</br></br><sub>n={num_test_schedules}</sub>\",\n",
        "        'y': 0.95,  # Keep the title slightly higher\n",
        "        'x': 0.02,\n",
        "        'xanchor': 'left',\n",
        "        'yanchor': 'top'\n",
        "    },\n",
        "    xaxis_title=\"Ambiguousness\",\n",
        "    yaxis_title=\"Error / Accuracy\",\n",
        "    hoverlabel=dict(font=dict(color='white')),\n",
        "    margin=dict(t=70)  # Add more space at the top of the chart\n",
        ")\n",
        "fig.show()"
      ],
      "id": "6c9b2cef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Optimization\n",
        "\n",
        "*In the initial model the choice of hyperparameters was based on default values, examples from demo's or trial and error. To improve the model's performance, we applied a [hyperparameter optimization technique](https://optuna.org/){target=\"_blank\"} to find the best set of hyperparameters. We used a grid search with cross-validation to find the optimal hyperparameters for the XGBoost model. The grid search was performed over a predefined set of hyperparameters, and the best hyperparameters were selected based on the model's performance on the validation set. The best hyperparameters were then used to train the final model.*\n"
      ],
      "id": "1574296b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functions import compare_json\n",
        "\n",
        "with open(\"best_trial_params.json\", \"r\") as f:\n",
        "    best_trial_params = json.load(f)\n",
        "    \n",
        "differences = compare_json(model_params, best_trial_params)\n",
        "\n",
        "params_tbl = pd.DataFrame(differences)\n",
        "params_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)\n",
        "print(params_tbl)"
      ],
      "id": "18b6d98a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit the model on the entire dataset\n",
        "# Initialize the XGBClassifier without early stopping here\n",
        "\n",
        "# Load the best trial parameters from a JSON file.\n",
        "with open(\"best_trial_params.json\", \"r\") as f:\n",
        "    best_trial_params = json.load(f)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "clf = xgb.XGBClassifier(\n",
        "    tree_method=\"hist\",\n",
        "    max_depth=best_trial_params[\"max_depth\"],\n",
        "    min_child_weight=best_trial_params[\"min_child_weight\"],\n",
        "    gamma=best_trial_params[\"gamma\"],\n",
        "    subsample=best_trial_params[\"subsample\"],\n",
        "    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n",
        "    learning_rate=best_trial_params[\"learning_rate\"],\n",
        "    n_estimators=best_trial_params[\"n_estimators\"],\n",
        ")\n",
        "\n",
        "clf.fit(X, y)\n",
        "end= time.time()\n",
        "modeling_time = end - start\n",
        "print(f\"\\nTraining time: {modeling_time} seconds\\n\")\n",
        "\n",
        "# Calculate and print the training accuracy\n",
        "training_accuracy = clf.score(X, y)\n",
        "print(f\"Training accuracy: {training_accuracy * 100:.2f}%\")"
      ],
      "id": "9868fabd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Predict the target for new data\n",
        "y_pred = clf.predict(X_new)\n",
        "\n",
        "# Probability estimates\n",
        "start = time.time()\n",
        "y_pred_proba = clf.predict_proba(X_new)\n",
        "end = time.time()\n",
        "prediction_time = end - start\n",
        "print(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n",
        "\n",
        "print(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")"
      ],
      "id": "2f93d8cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "errors = np.abs(y_pred - np.array(test_rankings))\n",
        "ambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\n",
        "df = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors, \"Schedules\": test_neighbors, \"Objectives\": test_objectives}).sort_values(by=\"Ambiguousness\")\n",
        "df['Cumulative error rate'] = df['Error'].expanding().mean()\n",
        "# Calculate cumulative accuracy\n",
        "df['Cumulative accuracy'] = 1 - df['Cumulative error rate']\n",
        "df.head()"
      ],
      "id": "9aff1c54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create traces\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n",
        "                    mode=\"markers\",\n",
        "                    name=\"Error\",\n",
        "                    marker=dict(size=9),\n",
        "                    customdata=df[[\"Schedules\", \"Objectives\"]],\n",
        "                    hovertemplate=\n",
        "                        \"Ambiguousness: %{x} <br>\" +\n",
        "                        \"Error: %{y} <br>\" +\n",
        "                        \"Schedules: %{customdata[0][0]} / %{customdata[0][1]} <br>\" +\n",
        "                        \"Objectives: %{customdata[1]} <br>\"\n",
        "                    ))\n",
        "                  \n",
        "fig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n",
        "                    mode=\"lines\",\n",
        "                    name=\"Cum. accuracy\",\n",
        "                    line = dict(width = 3, dash = 'dash')))\n",
        "fig.update_layout(\n",
        "    title={\n",
        "        'text': f\"Error vs Ambiguousness</br></br><sub>n={num_test_schedules}</sub>\",\n",
        "        'y': 0.95,  # Keep the title slightly higher\n",
        "        'x': 0.02,\n",
        "        'xanchor': 'left',\n",
        "        'yanchor': 'top'\n",
        "    },\n",
        "    xaxis_title=\"Ambiguousness\",\n",
        "    yaxis_title=\"Error / Accuracy\",\n",
        "    hoverlabel=dict(font=dict(color='white')),\n",
        "    margin=dict(t=70)  # Add more space at the top of the chart\n",
        ")\n",
        "fig.show()"
      ],
      "id": "13521fc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "*We wanted to test whether an XGBoost classification model could be used to assess and rank the quality of pairs of schedules. For performance benchmarking we use the conventional calculation method utilizing Lindley recursions.*\n",
        "\n",
        "*We trained the XGBoost ranking model with a limited set of features (schedules) and labels (objectives). The total number of possible schedules is approximately `{python} round(math.comb(N + T - 1, N) / 1000000, 0)` million. For training and evaluation, we sampled `{python} 2*num_schedules` schedules and corresponding neighbors. Generating the feature and label set took a total of `{python} round(data_prep_time + training_set_feat_time + training_set_lab_time, 4)` seconds, with the calculation of objective values accounting for `{python} round(training_set_lab_time, 4)` seconds.*\n",
        "\n",
        "*The model demonstrates strong and consistent performance with high accuracies both for training, testing and validation (`{python} round(df[\"Cumulative accuracy\"].min()*100, 2)`%) with good generalization and stability. Total training time for the final model was `{python} round(modeling_time, 4)` seconds. The evaluation of `{python} num_test_schedules` test schedules took `{python} round(prediction_time, 4)` seconds for the the XGBoost model and `{python} round(evaluation_time, 4)` for the conventional method, which is an improvement of `{python} int(evaluation_time/prediction_time)`X.*\n",
        "\n",
        "## Discussion\n"
      ],
      "id": "28a0ea32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_time = round(modeling_time, 4)\n",
        "conventional_time = round(evaluation_time, 4)\n",
        "xgboost_time = round(prediction_time, 4)\n",
        "\n",
        "# Define time values for plotting\n",
        "time_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds\n",
        "\n",
        "# Calculate evaluations for method 1\n",
        "method1_evaluations = np.where(time_values >= training_time, (time_values - training_time) / xgboost_time * 1000, 0)\n",
        "\n",
        "# Calculate evaluations for method 2\n",
        "method2_evaluations = time_values / conventional_time * 1000\n",
        "\n",
        "# Create line chart\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add method 1 trace\n",
        "fig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))\n",
        "\n",
        "# Add method 2 trace\n",
        "fig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title=\"Speed comparison between XGBoost ranking model and conventional method\",\n",
        "    xaxis_title=\"Time (seconds)\",\n",
        "    yaxis_title=\"Number of Evaluations\",\n",
        "    legend_title=\"Methods\",\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "id": "54c8ac6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Timeline\n",
        "\n",
        "*This experiment was started on 26-04-2025. The expected completion date is 26-04-2025.*\n",
        "\n",
        "## References"
      ],
      "id": "2cc68069"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}