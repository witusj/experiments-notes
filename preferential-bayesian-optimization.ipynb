{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Preferential Bayesian Optimization for Outpatient Appointment Scheduling\n",
        "format: html\n",
        "bibliography: references.bib\n",
        "warning: false\n",
        "---"
      ],
      "id": "e9344996"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective\n",
        "\n",
        "This experiment aims to apply Preferential Bayesian Optimization (PBO) to find optimal or near-optimal solutions for the outpatient appointment scheduling problem as defined by @kaandorp_optimal_2007. Specifically, the objective is to minimize a weighted sum of Expected Waiting Time (EWT) and Expected Staff Penalty (ESP) by efficiently searching the space of schedule perturbations. The experiment leverages dictionary-based embeddings (HED) as proposed by @deshwal_bayesian_2023 to handle the high-dimensional combinatorial space of perturbation selection vectors within the PBO framework, drawing on the original preferential-BO formulation of @gonzalez_preferential_2017.\n",
        "\n",
        "## Background\n",
        "\n",
        "We consider an outpatient appointment scheduling problem as described by @kaandorp_optimal_2007 where the schedule is represented by a vector $\\mathbf{x} = (x_0, x_1, \\ldots, x_{T-1})^T$. This vector comprises $T$ components, where $x_j$ denotes the non-negative allocation (number of patients) to time slot $j$, for $j = 0, \\ldots, T-1$. A fundamental constraint is that the total allocation across all time slots must equal a fixed constant $N$:\n",
        "\n",
        "$$\\sum_{j=0}^{T-1} x_j = N$$ where $N$ is the total number of patients to be scheduled. This constraint ensures that the schedule is feasible and respects the total patient load.\n",
        "\n",
        "We require $x_j \\ge 0$ for all $j = 0, \\ldots, T-1$. Consequently, a valid schedule $\\mathbf{x}$ belongs to the feasible set $\\mathcal{F} = { \\mathbf{z} \\in \\mathbb{D}^{T} \\mid \\sum_{j=0}^{T-1} z_j = N, z_j \\ge 0 \\text{ for all } j}$, where $\\mathbb{D}$ is the set of non-negative integers ($\\mathbb{Z}\\_{\\ge 0}$)\n",
        "\n",
        "@kaandorp_optimal_2007 define a neighborhood structure for local search based on perturbation vectors derived from a set of $T$ basis change vectors, $v_i \\in \\mathbb{D}^{T}$, for $i = 0, \\ldots, T-1$. These basis vectors represent elementary shifts of allocation between time slots:\n",
        "\n",
        "-   $v_0 = (-1, 0, \\ldots, 0, 1)$ (Shift unit *from* slot 0 *to* slot $T-1$)\n",
        "-   $v_1 = (1, -1, 0, \\ldots, 0)$ (Shift unit *from* slot 1 *to* slot 0)\n",
        "-   $v_i = (0, \\ldots, 0, \\underbrace{1}*{\\text{pos } i-1}, \\underbrace{-1}*{\\text{pos } i}, 0, \\ldots, 0)$ for $i = 2, \\ldots, T-1$ (Shift unit *from* slot $i$ *to* slot $i-1$)\n",
        "\n",
        "A key property of these basis vectors is that the sum of components for each vector is zero: $\\sum_{j=0}^{T-1} v_{ij} = 0$ for all $i=0, \\ldots, T-1$.\n",
        "\n",
        "Perturbations are constructed using a binary selection vector $\\mathbf{U} = (u_0, u_1, \\ldots, u_{T-1})$, where $u_i \\in {0, 1}$. Each $u_i$ indicates whether the basis change $v_i$ is included in the perturbation. The resulting perturbation vector $\\mathbf{r}(\\mathbf{U}) \\in \\mathbb{D}^{T}$ is the linear combination:\n",
        "\n",
        "$$\n",
        "\\mathbf{r}(\\mathbf{U}) := \\sum_{i=0}^{T-1} u_i v_i\n",
        "$$\n",
        "\n",
        "Since each $v\\_i$ sums to zero, any perturbation $\\mathbf{r}(\\mathbf{U})$ also sums to zero: $\\sum\\_{j=0}^{T-1} r\\_j(\\mathbf{U}) = 0$. This ensures that applying such a perturbation to a valid schedule $\\mathbf{x}$ preserves the total allocation $N$.\n",
        "\n",
        "The neighborhood of a schedule $\\mathbf{x} \\in \\mathcal{F}$, denoted by $\\mathcal{N}(\\mathbf{x})$, comprises all distinct, feasible schedules $\\mathbf{x}'$ reachable by applying a non-zero perturbation $\\mathbf{r}(\\mathbf{U})$ (@kaandorp_optimal_2007, use a slightly different but related neighborhood definition based on combinations of these basis vectors \\[cite: 89, 93, 1645\\]).\n",
        "\n",
        "The objective function to be minimized is a weighted sum of Expected Waiting Time (EWT) and Expected Staff Penalty (ESP), as defined by @kaandorp_optimal_2007:\n",
        "\n",
        "$$\n",
        "C(\\mathbf{x}) = w \\cdot EWT(\\mathbf{x}) + (1-w) \\cdot ESP(\\mathbf{x})\n",
        "$$\n",
        "\n",
        "@kaandorp_optimal_2007 prove that this objective function is multimodular, which guarantees that a local search algorithm using their defined neighborhood converges to the global optimum.\n",
        "\n",
        "However, evaluating this function can be computationally expensive for large $N$ and $T$, and the search space of binary vectors $\\mathbf{U}$ is high-dimensional ($2^T - 2$ possibilities).\n",
        "\n",
        "**Dictionary‐Based Hamming Embeddings.** @deshwal_bayesian_2023 identify that directly modeling high-dimensional binary vectors with a Gaussian process is both statistically and computationally challenging, since the search space grows as $2^d$. They propose Hamming Embedding via Dictionaries (HED): choose a small set of $m$ “dictionary” vectors $\\{a_i\\}\\subset\\{0,1\\}^d$ and embed any candidate $z$ by its Hamming distances $\\phi_i(z)=\\mathrm{Hamming}(a_i,z)$. By using carefully constructed binary-wavelet (Hadamard) dictionaries, they both dramatically reduce input dimensionality $(m\\ll d)$ and obtain provable regret bounds $\\widetilde O(\\sqrt{Tm})$. Empirically on combinatorial tasks (e.g. MAX-SAT, feature selection, compiler flags), BO with HED (“BODi”) converges faster and to better optima than state-of-the-art discrete methods.\n",
        "\n",
        "**Preferential Bayesian Optimization.**\\\n",
        "González et al. introduced the first PBO framework for optimizing a latent black-box function using only pairwise “duels.” They place a Gaussian-process prior over a latent utility function $f$, then squash it through a probit (or logistic) likelihood to model the probability $\\pi_f([x, x'])$ that $x$ is preferred to $x'$. From this they define acquisition functions—such as a Copeland-expected-improvement extension and a “dueling” Thompson–sampling rule—that balance exploration and exploitation by selecting the next pair $[x_t, x'_t]$ to query @gonzalez_preferential_2017.\n",
        "\n",
        "Because the preferential likelihood is non-conjugate, each iteration requires expensive approximate inference (refitting the GP classification model) and an inner optimization over pairs to maximize the acquisition. Although sample-efficient (drastically reducing the number of duels needed to find the Condorcet winner), this per-step computational overhead motivates integrating faster surrogate updates or approximate acquisition schemes—exactly the gap our HED-based embedding approach helps address by reducing problem dimensionality before applying PBO.\n",
        "\n",
        "## Hypothesis\n",
        "\n",
        "By applying PBO with HED embeddings and Thompson sampling, we expect to efficiently identify perturbation vectors $\\mathbf{U}$ that yield significantly improved schedules (lower cost) compared to random or simple local search heuristics.\n",
        "\n",
        "## Methodology\n",
        "We are using Botorch for the PBO implementation [@balandat2020botorch].\n"
      ],
      "id": "a0f84776"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import gpytorch\n",
        "from botorch.models.pairwise_gp import PairwiseGP, PairwiseLaplaceMarginalLogLikelihood\n",
        "from botorch.fit import fit_gpytorch_mll\n",
        "from botorch.models.transforms.input import Normalize\n",
        "from scipy.linalg import hadamard\n",
        "from scipy.optimize import minimize\n",
        "import random\n",
        "from typing import List, Tuple, Dict, Optional, Union\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from functions import (\n",
        "    bailey_welch_schedule,\n",
        "    get_v_star,\n",
        "    compute_convolutions,\n",
        "    calculate_objective_serv_time_lookup,\n",
        ")"
      ],
      "id": "f66a70f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n"
      ],
      "id": "3bc5b507"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Helper: generate_weighted_list ---\n",
        "def generate_weighted_list(max_s: int, l: float, i: int) -> Optional[np.ndarray]:\n",
        "    if not isinstance(max_s, int) or max_s <= 0: return None\n",
        "    if not isinstance(l, (int, float)) or not (1 <= l <= max_s): return None\n",
        "    if not isinstance(i, int) or not (0 <= i < max_s): return None\n",
        "    def objective_fn(x: np.ndarray) -> float:\n",
        "        return (np.dot(np.arange(1, max_s + 1), x) - l) ** 2\n",
        "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1.0})\n",
        "    bounds = [(0, 1)] * max_s\n",
        "    initial_guess = np.random.dirichlet(np.ones(max_s))\n",
        "    try:\n",
        "        result = minimize(objective_fn, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints, options={'maxiter': 300, 'ftol': 1e-9})\n",
        "        if not result.success: return None\n",
        "        optimized_probs = result.x\n",
        "        optimized_probs[optimized_probs < 0] = 0\n",
        "        current_sum = np.sum(optimized_probs)\n",
        "        if not np.isclose(current_sum, 1.0):\n",
        "            if current_sum > 1e-8: optimized_probs /= current_sum\n",
        "            else: return None\n",
        "    except Exception: return None\n",
        "    first_part_probs = optimized_probs[:i] if i > 0 else np.array([])\n",
        "    second_part_probs = optimized_probs[i:]\n",
        "    values = np.zeros(max_s + 1)\n",
        "    if i > 0: values[1 : i + 1] = np.sort(first_part_probs)\n",
        "    values[i + 1 : max_s + 1] = np.sort(second_part_probs)[::-1]\n",
        "    final_sum = np.sum(values[1:])\n",
        "    if not np.isclose(final_sum, 1.0):\n",
        "        if final_sum > 1e-8: values[1:] /= final_sum\n",
        "        else: return None\n",
        "    return values"
      ],
      "id": "1cd549b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Setup\n"
      ],
      "id": "495d0bf3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N_patients = 50\n",
        "T_param = 48\n",
        "d_interval_len = 10\n",
        "max_s_time = 30\n",
        "q_no_show = 0.20\n",
        "w_weight = 0.1\n",
        "l_target_avg_service_time = 14.0\n",
        "i_sorting_split = 10\n",
        "\n",
        "v_star_matrix = get_v_star(T_param)\n",
        "s_dist = generate_weighted_list(max_s_time, l_target_avg_service_time, i_sorting_split)\n",
        "if s_dist is None: raise ValueError(\"Failed to generate service time distribution.\")\n",
        "print(f\"Service time distribution (s): {s_dist.tolist()}\")\n",
        "# Assuming s_dist is already defined\n",
        "fig = go.Figure(data=go.Bar(\n",
        "    x=np.arange(1, len(s_dist) + 1),\n",
        "    y=s_dist,\n",
        "    marker_line_width=1\n",
        "))\n",
        "fig.update_layout(\n",
        "    title=\"Service Time Distribution\",\n",
        "    xaxis_title=\"Service Time (units)\",\n",
        "    yaxis_title=\"Probability\",\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig.show()\n",
        "print(f\"Average generated service time: {np.dot(np.arange(len(s_dist)), s_dist):.4f}\")\n",
        "convolutions_dict = compute_convolutions(s_dist.tolist(), N_patients, q_no_show)\n",
        "X_initial_schedule = np.array(bailey_welch_schedule(T_param, d_interval_len, N_patients, s_dist))\n",
        "print(f\"Initial base schedule (X_vec): {X_initial_schedule.tolist()}\")\n",
        "print(f\"Sum of patients in X_vec: {np.sum(X_initial_schedule)}\")\n",
        "LARGE_PENALTY_VAL = 1e10"
      ],
      "id": "1a5874d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective Function\n"
      ],
      "id": "63a9857a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_objective(U_np: Union[np.ndarray, List[int]], X_vec: np.ndarray, v_star: np.ndarray,\n",
        "                       conv_dict: Dict[int, np.ndarray], d_len: int, w_val: float) -> float:\n",
        "    if not isinstance(U_np, np.ndarray): U_np = np.array(U_np, dtype=int)\n",
        "    if U_np.ndim != 1: raise ValueError(\"Input U must be 1-dimensional\")\n",
        "    if U_np.shape[0] != v_star.shape[0]: raise ValueError(f\"U length {U_np.shape[0]} != V* rows {v_star.shape[0]}.\")\n",
        "    if X_vec.shape[0] != v_star.shape[1]: raise ValueError(f\"X length {X_vec.shape[0]} != V* columns {v_star.shape[1]}.\")\n",
        "    if not np.all((U_np == 0) | (U_np == 1)): raise ValueError(\"Input U must be binary.\")\n",
        "    V_sum = np.sum(v_star[U_np == 1, :], axis=0)\n",
        "    Y_schedule = X_vec + V_sum\n",
        "    if np.all(Y_schedule >= 0) and np.sum(Y_schedule) == np.sum(X_vec):\n",
        "        ewt, esp = calculate_objective_serv_time_lookup(Y_schedule.tolist(), d_len, conv_dict)\n",
        "        return w_val * ewt + (1 - w_val) * esp\n",
        "    return LARGE_PENALTY_VAL\n",
        "\n",
        "U_zeros = np.zeros(T_param, dtype=int)\n",
        "initial_obj_val = evaluate_objective(U_zeros, X_initial_schedule, v_star_matrix, convolutions_dict, d_interval_len, w_weight)\n",
        "print(f\"Initial objective value (U=zeros): {initial_obj_val:.4f}\")"
      ],
      "id": "9a5193e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PBO Setup\n"
      ],
      "id": "ff6b35a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- HED: Binary Wavelet Dictionary Construction ---\n",
        "def get_binary_wavelet_dictionary(T_dim_U: int, m_dict_size: int) -> np.ndarray:\n",
        "    d_hadamard = 1\n",
        "    while d_hadamard < T_dim_U: d_hadamard *= 2\n",
        "    H = hadamard(d_hadamard)\n",
        "    if m_dict_size > d_hadamard: m_dict_size = d_hadamard\n",
        "    dictionary = np.zeros((m_dict_size, T_dim_U), dtype=int)\n",
        "    for i in range(m_dict_size):\n",
        "        dictionary[i, :] = (H[i, :T_dim_U] + 1) // 2\n",
        "    return dictionary\n",
        "\n",
        "def embed_HED(U_np: np.ndarray, dictionary: np.ndarray) -> np.ndarray:\n",
        "    U_np_reshaped = U_np.reshape(1, -1) if U_np.ndim == 1 else U_np\n",
        "    hamming_distances = np.sum(dictionary != U_np_reshaped[:, np.newaxis, :], axis=2).T\n",
        "    return hamming_distances.flatten().astype(float) if U_np.ndim == 1 else hamming_distances.astype(float)\n",
        "\n",
        "m_dictionary_size = min(T_param * 2, 32 if T_param <=20 else 64)\n",
        "hed_dictionary = get_binary_wavelet_dictionary(T_param, m_dictionary_size)\n",
        "print(f\"HED dictionary shape: {hed_dictionary.shape}\")\n",
        "\n",
        "num_total_initial_pairs = 10\n",
        "num_initial_U_zeros_comparisons = 3 # Number of times to compare U_zeros with random U\n",
        "num_purely_random_initial_pairs = num_total_initial_pairs - num_initial_U_zeros_comparisons\n",
        "\n",
        "num_pbo_iterations = 50\n",
        "n_candidates_for_thompson = 200\n",
        "n_thompson_posterior_samples = 20\n",
        "\n",
        "all_U_vectors_list = []\n",
        "all_U_embeddings_tensor = None\n",
        "comparison_pairs_indices = []\n",
        "\n",
        "def add_U_to_master_list(U_np: np.ndarray, embedding: np.ndarray) -> int:\n",
        "    global all_U_vectors_list, all_U_embeddings_tensor\n",
        "    U_np = U_np.astype(int) # Ensure consistent type\n",
        "    for i, existing_U_np_item in enumerate(all_U_vectors_list):\n",
        "        if np.array_equal(existing_U_np_item, U_np): return i\n",
        "    new_index = len(all_U_vectors_list)\n",
        "    all_U_vectors_list.append(U_np.copy())\n",
        "    if embedding.ndim > 1: embedding = embedding.flatten()\n",
        "    embedding_tensor = torch.from_numpy(embedding).double().unsqueeze(0)\n",
        "    if all_U_embeddings_tensor is None: all_U_embeddings_tensor = embedding_tensor\n",
        "    else: all_U_embeddings_tensor = torch.cat([all_U_embeddings_tensor, embedding_tensor], dim=0)\n",
        "    return new_index\n",
        "\n",
        "def generate_random_U_vector(dim: int) -> np.ndarray:\n",
        "    return np.random.randint(0, 2, size=dim, dtype=int)\n",
        "\n",
        "def get_hamming_neighbors(U_vector: np.ndarray, num_neighbors: int, max_flips: int = 1) -> List[np.ndarray]:\n",
        "    neighbors = []\n",
        "    dim = len(U_vector)\n",
        "    if dim == 0: return []\n",
        "    for _ in range(num_neighbors):\n",
        "        neighbor = U_vector.copy()\n",
        "        actual_flips = np.random.randint(1, max_flips + 1)\n",
        "        flip_indices = np.random.choice(dim, size=min(actual_flips, dim), replace=False)\n",
        "        for idx in flip_indices: neighbor[idx] = 1 - neighbor[idx]\n",
        "        neighbors.append(neighbor)\n",
        "    return neighbors"
      ],
      "id": "7f3699a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PBO Initialization\n"
      ],
      "id": "d177bb3b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Starting PBO Initialization...\")\n",
        "best_U_overall = U_zeros.copy() # Start with U_zeros as initial best\n",
        "best_obj_overall = initial_obj_val\n",
        "best_iter_found = 0 # 0 for initial U_zeros evaluation\n",
        "\n",
        "# Add U_zeros to the master list first\n",
        "idx_U_zeros = add_U_to_master_list(U_zeros, embed_HED(U_zeros, hed_dictionary))\n",
        "obj_U_zeros = initial_obj_val # Already calculated\n",
        "\n",
        "# --- Initial comparisons involving U_zeros ---\n",
        "print(f\"Generating {num_initial_U_zeros_comparisons} initial comparisons involving U_zeros...\")\n",
        "for i in range(num_initial_U_zeros_comparisons):\n",
        "    U_competitor = generate_random_U_vector(T_param)\n",
        "    attempts = 0\n",
        "    while np.array_equal(U_competitor, U_zeros) and attempts < 100: # Avoid comparing U_zeros to itself\n",
        "        U_competitor = generate_random_U_vector(T_param)\n",
        "        attempts += 1\n",
        "    if np.array_equal(U_competitor, U_zeros): continue\n",
        "\n",
        "    obj_competitor = evaluate_objective(U_competitor, X_initial_schedule, v_star_matrix, convolutions_dict, d_interval_len, w_weight)\n",
        "    print(f\"  Init U_zeros vs U_rand_{i}: Obj_U_zeros={obj_U_zeros:.4f}, Obj_U_rand={obj_competitor:.4f}\")\n",
        "\n",
        "    if obj_competitor < best_obj_overall:\n",
        "        best_obj_overall = obj_competitor\n",
        "        best_U_overall = U_competitor.copy()\n",
        "        best_iter_found = 0 # Still initialization phase\n",
        "\n",
        "    idx_competitor = add_U_to_master_list(U_competitor, embed_HED(U_competitor, hed_dictionary))\n",
        "    \n",
        "    if not np.isclose(obj_U_zeros, obj_competitor):\n",
        "        pref_pair = None\n",
        "        if obj_U_zeros < obj_competitor: pref_pair = [idx_U_zeros, idx_competitor]\n",
        "        elif obj_competitor < obj_U_zeros: pref_pair = [idx_competitor, idx_U_zeros]\n",
        "        \n",
        "        if pref_pair and pref_pair not in comparison_pairs_indices:\n",
        "            comparison_pairs_indices.append(pref_pair)\n",
        "\n",
        "# --- Purely random initial comparisons (excluding U_zeros unless randomly picked again) ---\n",
        "print(f\"Generating {num_purely_random_initial_pairs} purely random initial comparisons...\")\n",
        "for i in range(num_purely_random_initial_pairs):\n",
        "    U_A = generate_random_U_vector(T_param)\n",
        "    U_B = generate_random_U_vector(T_param)\n",
        "    attempts = 0\n",
        "    while np.array_equal(U_A, U_B) and attempts < 100:\n",
        "        U_B = generate_random_U_vector(T_param)\n",
        "        attempts += 1\n",
        "    if np.array_equal(U_A, U_B): continue\n",
        "\n",
        "    obj_A = evaluate_objective(U_A, X_initial_schedule, v_star_matrix, convolutions_dict, d_interval_len, w_weight)\n",
        "    obj_B = evaluate_objective(U_B, X_initial_schedule, v_star_matrix, convolutions_dict, d_interval_len, w_weight)\n",
        "    print(f\"  Init U_randA_{i} vs U_randB_{i}: Obj_A={obj_A:.4f}, Obj_B={obj_B:.4f}\")\n",
        "\n",
        "\n",
        "    if obj_A < best_obj_overall: best_obj_overall = obj_A; best_U_overall = U_A.copy(); best_iter_found = 0\n",
        "    if obj_B < best_obj_overall: best_obj_overall = obj_B; best_U_overall = U_B.copy(); best_iter_found = 0\n",
        "\n",
        "    idx_A = add_U_to_master_list(U_A, embed_HED(U_A, hed_dictionary))\n",
        "    idx_B = add_U_to_master_list(U_B, embed_HED(U_B, hed_dictionary))\n",
        "    \n",
        "    if not np.isclose(obj_A, obj_B):\n",
        "        pref_pair = None\n",
        "        if obj_A < obj_B: pref_pair = [idx_A, idx_B]\n",
        "        elif obj_B < obj_A: pref_pair = [idx_B, idx_A]\n",
        "\n",
        "        if pref_pair and pref_pair not in comparison_pairs_indices:\n",
        "            comparison_pairs_indices.append(pref_pair)\n",
        "\n",
        "print(f\"Initialization complete. Observed {len(all_U_vectors_list)} unique schedules.\")\n",
        "print(f\"Number of preference pairs: {len(comparison_pairs_indices)}\")\n",
        "if best_U_overall is not None:\n",
        "    print(f\"Best U after initialization: {best_U_overall.tolist()}, Obj: {best_obj_overall:.4f}\")"
      ],
      "id": "322f5b42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PBO Loop\n"
      ],
      "id": "feb2e307"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "# Start timer for PBO loop\n",
        "start_time = time.time()\n",
        "\n",
        "pairwise_model = None\n",
        "\n",
        "for pbo_iter in range(num_pbo_iterations):\n",
        "    print(f\"\\n--- PBO Iteration {pbo_iter + 1}/{num_pbo_iterations} ---\")\n",
        "    if len(comparison_pairs_indices) < 1 or all_U_embeddings_tensor is None or all_U_embeddings_tensor.shape[0] < 2:\n",
        "        print(\"Not enough data for GP. Generating a random pair to add to comparisons.\")\n",
        "        # (Simplified fallback: add one random comparison)\n",
        "        U_A_fallback = generate_random_U_vector(T_param)\n",
        "        U_B_fallback = generate_random_U_vector(T_param)\n",
        "        attempts = 0\n",
        "        while np.array_equal(U_A_fallback, U_B_fallback) and attempts < 100:\n",
        "            U_B_fallback = generate_random_U_vector(T_param)\n",
        "            attempts += 1\n",
        "        if np.array_equal(U_A_fallback, U_B_fallback):\n",
        "            continue\n",
        "\n",
        "        obj_A_fb = evaluate_objective(U_A_fallback, X_initial_schedule, v_star_matrix, convolutions_dict, d_interval_len, w_weight)\n",
        "        obj_B_fb = evaluate_objective(U_B_fallback, X_initial_schedule, v_star_matrix, convolutions_dict, d_interval_len, w_weight)\n",
        "        if obj_A_fb < best_obj_overall:\n",
        "            best_obj_overall = obj_A_fb\n",
        "            best_U_overall = U_A_fallback.copy()\n",
        "            best_iter_found = pbo_iter + 1\n",
        "        if obj_B_fb < best_obj_overall:\n",
        "            best_obj_overall = obj_B_fb\n",
        "            best_U_overall = U_B_fallback.copy()\n",
        "            best_iter_found = pbo_iter + 1\n",
        "\n",
        "        idx_A_fb = add_U_to_master_list(U_A_fallback, embed_HED(U_A_fallback, hed_dictionary))\n",
        "        idx_B_fb = add_U_to_master_list(U_B_fallback, embed_HED(U_B_fallback, hed_dictionary))\n",
        "        if not np.isclose(obj_A_fb, obj_B_fb):\n",
        "            pref_pair_fb = [idx_A_fb, idx_B_fb] if obj_A_fb < obj_B_fb else [idx_B_fb, idx_A_fb]\n",
        "            if pref_pair_fb not in comparison_pairs_indices:\n",
        "                comparison_pairs_indices.append(pref_pair_fb)\n",
        "        if len(comparison_pairs_indices) < 1 or all_U_embeddings_tensor.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "    train_X_gp = all_U_embeddings_tensor.double()\n",
        "    train_Y_gp = torch.tensor(comparison_pairs_indices, dtype=torch.long)\n",
        "    min_bounds = torch.zeros(train_X_gp.shape[-1], dtype=torch.double)\n",
        "    max_bounds = torch.full((train_X_gp.shape[-1],), float(T_param), dtype=torch.double)\n",
        "    input_transform = Normalize(d=train_X_gp.shape[-1], bounds=torch.stack([min_bounds, max_bounds]))\n",
        "\n",
        "    pairwise_model = PairwiseGP(\n",
        "        train_X_gp,\n",
        "        train_Y_gp,\n",
        "        input_transform=input_transform,\n",
        "        covar_module=gpytorch.kernels.ScaleKernel(\n",
        "            gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=train_X_gp.shape[-1])\n",
        "        )\n",
        "    )\n",
        "    mll = PairwiseLaplaceMarginalLogLikelihood(pairwise_model.likelihood, pairwise_model)\n",
        "\n",
        "    U_query1, U_query2 = None, None\n",
        "    try:\n",
        "        fit_gpytorch_mll(mll)\n",
        "        U_cand_pool_list = []\n",
        "        for _ in range(n_candidates_for_thompson // 2):\n",
        "            U_cand_pool_list.append(generate_random_U_vector(T_param))\n",
        "        if best_U_overall is not None:\n",
        "            U_cand_pool_list.extend(\n",
        "                get_hamming_neighbors(best_U_overall, n_candidates_for_thompson // 2, max_flips=2)\n",
        "            )\n",
        "        else:\n",
        "            for _ in range(n_candidates_for_thompson // 2):\n",
        "                U_cand_pool_list.append(generate_random_U_vector(T_param))\n",
        "\n",
        "        unique_U_cand_tuples = {tuple(u.tolist()) for u in U_cand_pool_list}\n",
        "        U_cand_pool_np = np.array([list(t) for t in unique_U_cand_tuples], dtype=int)\n",
        "        if len(U_cand_pool_np) == 0:\n",
        "            raise ValueError(\"Candidate pool is empty.\")\n",
        "\n",
        "        embedded_candidates_np = np.array([embed_HED(u, hed_dictionary) for u in U_cand_pool_np])\n",
        "        embedded_candidates_torch = torch.from_numpy(embedded_candidates_np).double()\n",
        "\n",
        "        pairwise_model.eval()\n",
        "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "            posterior_f = pairwise_model.posterior(input_transform(embedded_candidates_torch))\n",
        "            f_samples = posterior_f.sample(torch.Size([n_thompson_posterior_samples]))\n",
        "\n",
        "        best_indices_per_sample = torch.argmax(f_samples, dim=1)\n",
        "        query_idx1_in_cand_pool = best_indices_per_sample[0].item()\n",
        "        U_query1 = U_cand_pool_np[query_idx1_in_cand_pool]\n",
        "        query_idx2_in_cand_pool = -1\n",
        "\n",
        "        if len(U_cand_pool_np) > 1:\n",
        "            if len(best_indices_per_sample) > 1:\n",
        "                for i in range(1, len(best_indices_per_sample)):\n",
        "                    if best_indices_per_sample[i].item() != query_idx1_in_cand_pool:\n",
        "                        query_idx2_in_cand_pool = best_indices_per_sample[i].item()\n",
        "                        break\n",
        "            if (\n",
        "                query_idx2_in_cand_pool == -1\n",
        "                or query_idx1_in_cand_pool == query_idx2_in_cand_pool\n",
        "            ):\n",
        "                rand_indices = np.arange(len(U_cand_pool_np))\n",
        "                np.random.shuffle(rand_indices)\n",
        "                for rand_idx in rand_indices:\n",
        "                    if rand_idx != query_idx1_in_cand_pool:\n",
        "                        query_idx2_in_cand_pool = rand_idx\n",
        "                        break\n",
        "                U_query2 = (\n",
        "                    U_cand_pool_np[query_idx2_in_cand_pool]\n",
        "                    if query_idx2_in_cand_pool != -1\n",
        "                    else generate_random_U_vector(T_param)\n",
        "                )\n",
        "            else:\n",
        "                U_query2 = U_cand_pool_np[query_idx2_in_cand_pool]\n",
        "        else:\n",
        "            U_query2 = generate_random_U_vector(T_param)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in GP/TS: {e}. Falling back to random.\")\n",
        "        U_query1 = generate_random_U_vector(T_param)\n",
        "        U_query2 = generate_random_U_vector(T_param)\n",
        "\n",
        "    attempts = 0\n",
        "    while np.array_equal(U_query1, U_query2) and attempts < 100:\n",
        "        U_query2 = generate_random_U_vector(T_param)\n",
        "        attempts += 1\n",
        "    if np.array_equal(U_query1, U_query2):\n",
        "        continue\n",
        "\n",
        "    obj_q1 = evaluate_objective(\n",
        "        U_query1,\n",
        "        X_initial_schedule,\n",
        "        v_star_matrix,\n",
        "        convolutions_dict,\n",
        "        d_interval_len,\n",
        "        w_weight,\n",
        "    )\n",
        "    obj_q2 = evaluate_objective(\n",
        "        U_query2,\n",
        "        X_initial_schedule,\n",
        "        v_star_matrix,\n",
        "        convolutions_dict,\n",
        "        d_interval_len,\n",
        "        w_weight,\n",
        "    )\n",
        "    print(f\"  Query 1 (U): {U_query1.tolist()}, Obj: {obj_q1:.4f}\")\n",
        "    print(f\"  Query 2 (U): {U_query2.tolist()}, Obj: {obj_q2:.4f}\")\n",
        "\n",
        "    if obj_q1 < best_obj_overall:\n",
        "        best_obj_overall = obj_q1\n",
        "        best_U_overall = U_query1.copy()\n",
        "        best_iter_found = pbo_iter + 1\n",
        "        print(\n",
        "            f\"  New best U from Q1: {best_U_overall.tolist()}, Obj: {best_obj_overall:.4f} (Iter: {best_iter_found})\"\n",
        "        )\n",
        "    if obj_q2 < best_obj_overall:\n",
        "        best_obj_overall = obj_q2\n",
        "        best_U_overall = U_query2.copy()\n",
        "        best_iter_found = pbo_iter + 1\n",
        "        print(\n",
        "            f\"  New best U from Q2: {best_U_overall.tolist()}, Obj: {best_obj_overall:.4f} (Iter: {best_iter_found})\"\n",
        "        )\n",
        "\n",
        "    idx_q1 = add_U_to_master_list(\n",
        "        U_query1, embed_HED(U_query1, hed_dictionary)\n",
        "    )\n",
        "    idx_q2 = add_U_to_master_list(\n",
        "        U_query2, embed_HED(U_query2, hed_dictionary)\n",
        "    )\n",
        "\n",
        "    if not np.isclose(obj_q1, obj_q2):\n",
        "        pref_pair_iter = [idx_q1, idx_q2] if obj_q1 < obj_q2 else [idx_q2, idx_q1]\n",
        "        print(f\"  Preference: {'Q1 > Q2' if obj_q1 < obj_q2 else 'Q2 > Q1'}\")\n",
        "        if pref_pair_iter not in comparison_pairs_indices:\n",
        "            comparison_pairs_indices.append(pref_pair_iter)\n",
        "        else:\n",
        "            print(\"  (Preference already recorded)\")\n",
        "    else:\n",
        "        print(\"  Preference: Tie\")\n",
        "    print(f\"  Total unique Us: {len(all_U_vectors_list)}, Total preference pairs: {len(comparison_pairs_indices)}\")\n",
        "\n",
        "print(\"\\n--- PBO Experiment Finished ---\")\n",
        "if best_U_overall is not None:\n",
        "    print(f\"Best U vector found (by direct evaluation): {best_U_overall.tolist()}\")\n",
        "    final_Y_schedule = X_initial_schedule + np.sum(v_star_matrix[best_U_overall == 1, :], axis=0)\n",
        "    print(f\"Corresponding Y schedule: {final_Y_schedule.tolist()}\")\n",
        "    print(f\"Objective value: {best_obj_overall:.4f}\")\n",
        "    iter_str = \"during initialization (before PBO iterations)\" if best_iter_found == 0 else f\"at PBO iteration {best_iter_found}\"\n",
        "    print(f\"Found {iter_str}\")\n",
        "    print(f\"Patient count in final Y schedule: {np.sum(final_Y_schedule)}\")\n",
        "else: print(\"No valid U vector found.\")\n",
        "\n",
        "if pairwise_model is not None and all_U_embeddings_tensor is not None and len(all_U_embeddings_tensor) > 0:\n",
        "    pairwise_model.eval()\n",
        "    transformed_all_U_embeddings = input_transform(all_U_embeddings_tensor)\n",
        "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "        posterior_f_all = pairwise_model.posterior(transformed_all_U_embeddings)\n",
        "        mean_f_all = posterior_f_all.mean\n",
        "    best_idx_model = torch.argmax(mean_f_all).item()\n",
        "    U_reco_model = all_U_vectors_list[best_idx_model]\n",
        "    obj_reco_model = evaluate_objective(U_reco_model, X_initial_schedule, v_star_matrix, convolutions_dict, d_interval_len, w_weight)\n",
        "    print(f\"\\nRecommendation based on GP model (highest posterior mean utility over all {len(all_U_vectors_list)} evaluated Us):\")\n",
        "    print(f\"  U vector: {U_reco_model.tolist()}\")\n",
        "    Y_reco = X_initial_schedule + np.sum(v_star_matrix[U_reco_model == 1, :], axis=0)\n",
        "    print(f\"  Corresponding Y schedule: {Y_reco.tolist()}\")\n",
        "    print(f\"  Objective value of this GP recommended U: {obj_reco_model:.4f}\")\n",
        "    print(f\"  GP's posterior mean utility for this U: {mean_f_all[best_idx_model].item():.4f}\")\n",
        "else: print(\"\\nGP model not available for recommendation.\")\n",
        "\n",
        "# End of PBO loop\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n",
        "print(f\"\\n--- PBO Experiment Finished in {elapsed:.2f} seconds ---\")"
      ],
      "id": "99a055f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "\n",
        "In this study, we demonstrated how Preferential Bayesian Optimization (PBO) combined with Hamming Embeddings via Dictionaries (HED) can efficiently navigate the high-dimensional binary space of schedule perturbations and deliver improved outpatient appointment schedules with far fewer objective evaluations than naïve random or local‐search heuristics.\n",
        "\n",
        "### Future Directions\n",
        "\n",
        "**Automated Acquisition–Function Discovery with FunBO.**\\\n",
        "@aglietti_funbo_2024 introduce *FunBO*, a technique that leverages large language models to programmatically generate, refine, and evaluate novel Bayesian‐optimization acquisition functions. By iterating between an LLM prompt (to propose new code variants) and empirical benchmarking, FunBO has discovered bespoke strategies that outperform classical rules like Expected Improvement or Upper Confidence Bound on standard continuous benchmarks.\n",
        "\n",
        "> *Why this matters for HED+PBO:* Our current pipeline uses off‐the‐shelf PBO acquisitions (e.g. Thompson sampling on Hamming‐dictionary inputs). FunBO could be tasked to propose *duel‐specific* acquisition rules—perhaps embedding‐aware diversity measures or adaptive exploration–exploitation schedules—that are tailor‐made for the combinatorial neighborhood of outpatient schedules. Systematically comparing these LLM‐generated acquisitions could lead to even faster convergence or better final schedules, especially in large or highly constrained clinics.\n",
        "\n",
        "**Inner–Loop Amortization via Learned Proposal Policies.**\\\n",
        "@swersky_amortized_2020 train a lightweight neural policy via reinforcement learning to *amortize* the inner maximization of discrete‐space acquisition functions. Instead of exhaustively scanning candidates at each BO step, the network instantly proposes high‐quality points, yielding dramatic per‐iteration speed‐ups on tasks such as protein‐sequence design.\n",
        "\n",
        "> *Why this matters for HED+PBO:* In our implementation, we still sample and score hundreds of Hamming‐neighbors each iteration—incurring nontrivial runtime as $T$ and the dictionary size grow. By training a “deep evolutionary” proposal network on synthetic perturbation‐scheduling tasks (varying $N,T$, service distributions, no‐show rates), we could *replace* that random/Hamming sampling step with a single neural forward pass, enabling near‐real‐time PBO for large‐scale or interactive applications.\n",
        "\n",
        "**Fully Amortized Preferential BO with Meta‐Learning.**\\\n",
        "@zhang_pabbo_2025 develop PABBO, a transformer‐based meta‐learner that jointly models both the surrogate posterior and the duel acquisition policy. Once pre‐trained on a diverse corpus of BO problems, it requires *no* GP refitting or inner‐loop optimization at test time—simply mapping past duels to new pair scores in one forward pass, and achieving orders‐of‐magnitude speed‐ups while matching or exceeding GP dynamics :contentReference[oaicite:2]{index=\"2\"}.\n",
        "\n",
        "> *Why this matters for HED+PBO:* Although our HED+PairwiseGP already reduces input dimensionality, each iteration still fits a GP and samples dozens of candidates. A PABBO‐inspired extension could meta‐train on HED‐embedded scheduling duels, so that in a new clinic setting, the model instantly proposes top perturbations—unlocking truly interactive preferential optimization for clinician‐in‐the‐loop scheduling or adaptive appointment systems.\n",
        "\n",
        "By exploring these directions—LLM‐driven acquisition design, learned inner‐loop proposals, and fully amortized PBO—we can further improve the **scalability**, **runtime efficiency**, and **automation** of HED‐based preferential optimization for outpatient scheduling.\n",
        "\n",
        "## Timeline\n",
        "\n",
        "-   **Experiment Design**: 19-05-2025\n",
        "-   **Implementation**: 19-05-2025\n",
        "-   **Execution**: (to be filled after run)\n",
        "-   **Analysis**: (to be filled after run)\n",
        "\n",
        "## References\n",
        "\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "07b94e99"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}