# Bounds: Each value should be between 0 and 1
bounds = [(0, 1)] * T
# Initial guess: Random distribution that sums to 1
initial_guess = np.random.dirichlet(np.ones(T))
# Optimization: Minimize the objective function subject to the sum and bounds constraints
result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)
# Set the values in the array (index 0 remains 0)
values[1:] = result.x
# Now we need to reorder the values as per the new requirement
first_part = np.sort(values[1:i+1])  # Sort the first 'i' values in ascending order
second_part = np.sort(values[i+1:])[::-1]  # Sort the remaining 'T-i' values in descending order
# Combine the sorted parts back together
values[1:i+1] = first_part
values[i+1:] = second_part
return values
i = 5  # First 5 highest values in ascending order, rest in descending order
s = generate_weighted_list(max_s, l, i)
print(s)
print("Sum:", np.sum(s[1:]))  # This should be 1
print("Weighted service time:", np.dot(np.arange(len(s)), s))  # This should be close to l
initial_x = bailey_welch_schedule(T, d, N, s)
print(f"Initial schedule: {x_schedule}")
convolutions = compute_convolutions(s, N, q)
file_path_parameters = f"datasets/parameters_{N}_{T}_{l}.pkl"
with open(file_path_parameters, 'wb') as f:
pickle.dump({
'N': N,
'T': T,
'd': d,
'max_s': max_s,
'q': q,
'w': w,
'l': l,
'num_schedules': num_schedules,
'convolutions': convolutions
}, f)
print(f"Data saved successfully to '{file_path_parameters}'")
from functions import compute_convolutions, bailey_welch_schedule
N = 22 # Number of patients
T = 20 # Number of intervals
d = 5 # Length of each interval
max_s = 20 # Maximum service time
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.1 # Weight for the waiting time in objective function
l = 10
num_schedules = 100000 # Number of schedules to sample
# Create service time distribution
def generate_weighted_list(max_s, l, i):
# Initialize an array of T+1 values, starting with zero
values = np.zeros(T + 1)
# Objective function: Sum of squared differences between current weighted average and the desired l
def objective(x):
weighted_avg = np.dot(np.arange(1, T + 1), x) / np.sum(x)
return (weighted_avg - l) ** 2
# Constraint: The sum of the values from index 1 to T must be 1
constraints = ({
'type': 'eq',
'fun': lambda x: np.sum(x) - 1
})
# Bounds: Each value should be between 0 and 1
bounds = [(0, 1)] * T
# Initial guess: Random distribution that sums to 1
initial_guess = np.random.dirichlet(np.ones(T))
# Optimization: Minimize the objective function subject to the sum and bounds constraints
result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)
# Set the values in the array (index 0 remains 0)
values[1:] = result.x
# Now we need to reorder the values as per the new requirement
first_part = np.sort(values[1:i+1])  # Sort the first 'i' values in ascending order
second_part = np.sort(values[i+1:])[::-1]  # Sort the remaining 'T-i' values in descending order
# Combine the sorted parts back together
values[1:i+1] = first_part
values[i+1:] = second_part
return values
i = 5  # First 5 highest values in ascending order, rest in descending order
s = generate_weighted_list(max_s, l, i)
print(s)
print("Sum:", np.sum(s[1:]))  # This should be 1
print("Weighted service time:", np.dot(np.arange(len(s)), s))  # This should be close to l
initial_x = bailey_welch_schedule(T, d, N, s)
print(f"Initial schedule: {initial_xe}")
convolutions = compute_convolutions(s, N, q)
file_path_parameters = f"datasets/parameters_{N}_{T}_{l}.pkl"
with open(file_path_parameters, 'wb') as f:
pickle.dump({
'N': N,
'T': T,
'd': d,
'max_s': max_s,
'q': q,
'w': w,
'l': l,
'num_schedules': num_schedules,
'convolutions': convolutions
}, f)
print(f"Data saved successfully to '{file_path_parameters}'")
from functions import compute_convolutions, bailey_welch_schedule
N = 22 # Number of patients
T = 20 # Number of intervals
d = 5 # Length of each interval
max_s = 20 # Maximum service time
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.1 # Weight for the waiting time in objective function
l = 10
num_schedules = 100000 # Number of schedules to sample
# Create service time distribution
def generate_weighted_list(max_s, l, i):
# Initialize an array of T+1 values, starting with zero
values = np.zeros(T + 1)
# Objective function: Sum of squared differences between current weighted average and the desired l
def objective(x):
weighted_avg = np.dot(np.arange(1, T + 1), x) / np.sum(x)
return (weighted_avg - l) ** 2
# Constraint: The sum of the values from index 1 to T must be 1
constraints = ({
'type': 'eq',
'fun': lambda x: np.sum(x) - 1
})
# Bounds: Each value should be between 0 and 1
bounds = [(0, 1)] * T
# Initial guess: Random distribution that sums to 1
initial_guess = np.random.dirichlet(np.ones(T))
# Optimization: Minimize the objective function subject to the sum and bounds constraints
result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)
# Set the values in the array (index 0 remains 0)
values[1:] = result.x
# Now we need to reorder the values as per the new requirement
first_part = np.sort(values[1:i+1])  # Sort the first 'i' values in ascending order
second_part = np.sort(values[i+1:])[::-1]  # Sort the remaining 'T-i' values in descending order
# Combine the sorted parts back together
values[1:i+1] = first_part
values[i+1:] = second_part
return values
i = 5  # First 5 highest values in ascending order, rest in descending order
s = generate_weighted_list(max_s, l, i)
print(s)
print("Sum:", np.sum(s[1:]))  # This should be 1
print("Weighted service time:", np.dot(np.arange(len(s)), s))  # This should be close to l
initial_x = bailey_welch_schedule(T, d, N, s)
print(f"Initial schedule: {initial_x}")
convolutions = compute_convolutions(s, N, q)
file_path_parameters = f"datasets/parameters_{N}_{T}_{l}.pkl"
with open(file_path_parameters, 'wb') as f:
pickle.dump({
'N': N,
'T': T,
'd': d,
'max_s': max_s,
'q': q,
'w': w,
'l': l,
'num_schedules': num_schedules,
'convolutions': convolutions
}, f)
print(f"Data saved successfully to '{file_path_parameters}'")
from functions import get_v_star
def create_neighbors_list(x: list[int], v_star: np.ndarray) -> (list[int], list[int]):
"""
Create a set of pairs of schedules that are from the same neighborhood.
Parameters:
x (list[int]): A list of integers with |s| = T and sum N.
v_star (np.ndarray): Precomputed vectors V* of length T.
Returns:
tuple(list[int], list[int]): A pair of schedules.
"""
T = len(s)
# Precompute binomial coefficients (weights for random.choices)
binom_coeff = [math.comb(T, i) for i in range(1, T)]
# Choose a random value of i with the corresponding probability
i = random.choices(range(1, T), weights=binom_coeff)[0]
# Instead of generating the full list of combinations, sample one directly
j = random.sample(range(T), i)
x_p = x.copy()
for k in j:
x_temp = np.array(s_p) + v_star[k]
x_temp = s_temp.astype(int)
if np.all(s_temp >= 0):
s_p = s_temp.astype(int).tolist()
return x, x_p
start = time.time()
v_star = get_v_star(T)
neighbors_list = [create_neighbors_list(initial_x, v_star) for i in range(num_schedules)] # This can be done in parellel to improve speed
end = time.time()
h = random.choices(range(num_schedules), k=7)
print(f"Sampled schedules: {h}")
for i in h:
original_schedule = neighbors_list[i][0]
neighbor_schedule = neighbors_list[i][1]
difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]
print(f"Neighbors\n{original_schedule}\n{neighbor_schedule}\n{difference}")
training_set_feat_time = end - start
print(f"\nProcessing time: {training_set_feat_time} seconds\n")
from functions import get_v_star
def create_neighbors_list(x: list[int], v_star: np.ndarray) -> (list[int], list[int]):
"""
Create a set of pairs of schedules that are from the same neighborhood.
Parameters:
x (list[int]): A list of integers with |s| = T and sum N.
v_star (np.ndarray): Precomputed vectors V* of length T.
Returns:
tuple(list[int], list[int]): A pair of schedules.
"""
T = len(s)
# Precompute binomial coefficients (weights for random.choices)
binom_coeff = [math.comb(T, i) for i in range(1, T)]
# Choose a random value of i with the corresponding probability
i = random.choices(range(1, T), weights=binom_coeff)[0]
# Instead of generating the full list of combinations, sample one directly
j = random.sample(range(T), i)
x_p = x.copy()
for k in j:
x_temp = np.array(x_p) + v_star[k]
x_temp = x_temp.astype(int)
if np.all(x_temp >= 0):
x_p = s_temp.astype(int).tolist()
return x, x_p
start = time.time()
v_star = get_v_star(T)
neighbors_list = [create_neighbors_list(initial_x, v_star) for i in range(num_schedules)] # This can be done in parallel to improve speed
end = time.time()
h = random.choices(range(num_schedules), k=7)
print(f"Sampled schedules: {h}")
for i in h:
original_schedule = neighbors_list[i][0]
neighbor_schedule = neighbors_list[i][1]
difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]
print(f"Neighbors\n{original_schedule}\n{neighbor_schedule}\n{difference}")
training_set_feat_time = end - start
print(f"\nProcessing time: {training_set_feat_time} seconds\n")
from functions import get_v_star
def create_neighbors_list(x: list[int], v_star: np.ndarray) -> (list[int], list[int]):
"""
Create a set of pairs of schedules that are from the same neighborhood.
Parameters:
x (list[int]): A list of integers with |s| = T and sum N.
v_star (np.ndarray): Precomputed vectors V* of length T.
Returns:
tuple(list[int], list[int]): A pair of schedules.
"""
T = len(s)
# Precompute binomial coefficients (weights for random.choices)
binom_coeff = [math.comb(T, i) for i in range(1, T)]
# Choose a random value of i with the corresponding probability
i = random.choices(range(1, T), weights=binom_coeff)[0]
# Instead of generating the full list of combinations, sample one directly
j = random.sample(range(T), i)
x_p = x.copy()
for k in j:
x_temp = np.array(x_p) + v_star[k]
x_temp = x_temp.astype(int)
if np.all(x_temp >= 0):
x_p = x_temp.astype(int).tolist()
return x, x_p
start = time.time()
v_star = get_v_star(T)
neighbors_list = [create_neighbors_list(initial_x, v_star) for i in range(num_schedules)] # This can be done in parallel to improve speed
end = time.time()
h = random.choices(range(num_schedules), k=7)
print(f"Sampled schedules: {h}")
for i in h:
original_schedule = neighbors_list[i][0]
neighbor_schedule = neighbors_list[i][1]
difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]
print(f"Neighbors\n{original_schedule}\n{neighbor_schedule}\n{difference}")
training_set_feat_time = end - start
print(f"\nProcessing time: {training_set_feat_time} seconds\n")
from functions import get_v_star
def create_neighbors_list(x: list[int], v_star: np.ndarray) -> (list[int], list[int]):
"""
Create a set of pairs of schedules that are from the same neighborhood.
Parameters:
x (list[int]): A list of integers with |s| = T and sum N.
v_star (np.ndarray): Precomputed vectors V* of length T.
Returns:
tuple(list[int], list[int]): A pair of schedules.
"""
T = len(x)
# Precompute binomial coefficients (weights for random.choices)
binom_coeff = [math.comb(T, i) for i in range(1, T)]
# Choose a random value of i with the corresponding probability
i = random.choices(range(1, T), weights=binom_coeff)[0]
# Instead of generating the full list of combinations, sample one directly
j = random.sample(range(T), i)
x_p = x.copy()
for k in j:
x_temp = np.array(x_p) + v_star[k]
x_temp = x_temp.astype(int)
if np.all(x_temp >= 0):
x_p = x_temp.astype(int).tolist()
return x, x_p
start = time.time()
v_star = get_v_star(T)
neighbors_list = [create_neighbors_list(initial_x, v_star) for i in range(num_schedules)] # This can be done in parallel to improve speed
end = time.time()
h = random.choices(range(num_schedules), k=7)
print(f"Sampled schedules: {h}")
for i in h:
original_schedule = neighbors_list[i][0]
neighbor_schedule = neighbors_list[i][1]
difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]
print(f"Neighbors\n{original_schedule}\n{neighbor_schedule}\n{difference}")
training_set_feat_time = end - start
print(f"\nProcessing time: {training_set_feat_time} seconds\n")
from functions import calculate_objective_serv_time_lookup
objectives_schedule_1 = [
w * result[0] + (1 - w) * result[1]
for neighbor in neighbors_list
for result in [calculate_objective_serv_time_lookup(neighbor[0], d, convolutions)]
]
start = time.time()
objectives_schedule_2 = [
w * result[0] + (1 - w) * result[1]
for neighbor in neighbors_list
for result in [calculate_objective_serv_time_lookup(neighbor[1], d, convolutions)]
]
end = time.time()
training_set_lab_time = end - start
objectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]
rankings = np.argmin(objectives, axis=1).tolist()
for i in range(5):
print(f"Objectives: {objectives[i]}, Ranking: {rankings[i]}")
print(f"\nProcessing time: {training_set_lab_time} seconds\n")
# Saving neighbors_list and objectives to a pickle file
file_path_neighbors = f"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl"
with open(file_path_neighbors, 'wb') as f:
pickle.dump({'neighbors_list': neighbors_list, 'objectives': objectives, 'rankings': rankings}, f)
print(f"Data saved successfully to '{file_path_neighbors}'")
# Prepare the dataset
X = []
for neighbors in neighbors_list:
X.append(neighbors[0] + neighbors[1])
X = np.array(X)
y = np.array(rankings)
# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
class CustomCallback(TrainingCallback):
def __init__(self, period=10):
self.period = period
def after_iteration(self, model, epoch, evals_log):
if (epoch + 1) % self.period == 0:
print(f"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}")
return False
def fit_and_score(estimator, X_train, X_test, y_train, y_test):
"""Fit the estimator on the train set and score it on both sets"""
estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0
)
train_score = estimator.score(X_train, y_train)
test_score = estimator.score(X_test, y_test)
return estimator, train_score, test_score
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)
# Initialize the XGBClassifier without early stopping here
# Load the best trial parameters from a JSON file.
with open("model_params.json", "r") as f:
model_params = json.load(f)
# Initialize the EarlyStopping callback with validation dataset
early_stop = xgb.callback.EarlyStopping(
rounds=10, metric_name='logloss', data_name='validation_0', save_best=True
)
clf = xgb.XGBClassifier(
tree_method="hist",
max_depth=model_params["max_depth"],
min_child_weight=model_params["min_child_weight"],
gamma=model_params["gamma"],
subsample=model_params["subsample"],
colsample_bytree=model_params["colsample_bytree"],
learning_rate=model_params["learning_rate"],
n_estimators=model_params["n_estimators"],
early_stopping_rounds=9,
#callbacks=[CustomCallback(period=50), early_stop],
callbacks=[CustomCallback(period=50)],
)
print("Params: ")
for key, value in model_params.items():
print(f" {key}: {value}")
start = time.time()
results = []
for train_idx, test_idx in cv.split(X, y):
X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]
est, train_score, test_score = fit_and_score(
clone(clf), X_train, X_test, y_train, y_test
)
results.append((est, train_score, test_score))
end = time.time()
training_time = end - start
print(f"\nTraining time: {training_time} seconds\n")
# Print results
for i, (est, train_score, test_score) in enumerate(results):
print(f"Fold {i+1} - Train Score (Accuracy): {train_score:.4f}, Test Score (Accuracy): {test_score:.4f}")
# Fit the model on the entire dataset
# Initialize the XGBClassifier without early stopping here
start = time.time()
clf = xgb.XGBClassifier(
tree_method="hist",
max_depth=model_params["max_depth"],
min_child_weight=model_params["min_child_weight"],
gamma=model_params["gamma"],
subsample=model_params["subsample"],
colsample_bytree=model_params["colsample_bytree"],
learning_rate=model_params["learning_rate"],
n_estimators=model_params["n_estimators"],
)
clf.fit(X, y)
end= time.time()
modeling_time = end - start
clf.save_model('models/classifier_large_instance.json')
# Calculate and print the training accuracy
training_accuracy = clf.score(X, y)
print(f"Training accuracy: {training_accuracy * 100:.2f}%\n")
print(f"\nTraining time: {modeling_time} seconds\n")
num_test_schedules = 1000
#test_schedules = random_combination_with_replacement(T, N, num_test_schedules)
test_schedules = create_random_schedules(T, N, num_test_schedules)
test_neighbors = [create_neighbors_list(test_schedule, v_star) for test_schedule in test_schedules] # This can be done in parellel to improve speed
print(f"Sampled: {len(test_schedules)} schedules\n")
test_objectives_schedule_1 = [
w * result[0] + (1 - w) * result[1]
for test_neighbor in test_neighbors
for result in [calculate_objective_serv_time_lookup(test_neighbor[0], d, convolutions)]
]
# Start time measurement for the evaluation
start = time.time()
test_objectives_schedule_2 = [
w * result[0] + (1 - w) * result[1]
for test_neighbor in test_neighbors
for result in [calculate_objective_serv_time_lookup(test_neighbor[1], d, convolutions)]
]
test_rankings = [0 if test_obj < test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]
end = time.time()
evaluation_time = end - start
# Combine the objectives for each pair for later processing
test_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]
print(f"\nEvaluation time: {evaluation_time} seconds\n")
for i in range(6):
print(f"Neighbors: {test_neighbors[i]},\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\n")
from functions import get_v_star
def create_neighbors_list(x: list[int], v_star: np.ndarray) -> (list[int], list[int]):
"""
Create a set of pairs of schedules that are from the same neighborhood.
Parameters:
x (list[int]): A list of integers with |s| = T and sum N.
v_star (np.ndarray): Precomputed vectors V* of length T.
Returns:
tuple(list[int], list[int]): A pair of schedules.
"""
T = len(x)
# Precompute binomial coefficients (weights for random.choices)
binom_coeff = [math.comb(T, i) for i in range(1, T)]
# Choose a random value of i with the corresponding probability
i = random.choices(range(1, T), weights=binom_coeff)[0]
# Instead of generating the full list of combinations, sample one directly
j = random.sample(range(T), i)
x_p = x.copy()
for k in j:
x_temp = np.array(x_p) + v_star[k]
x_temp = x_temp.astype(int)
if np.all(x_temp >= 0):
x_p = x_temp.astype(int).tolist()
return x, x_p
start = time.time()
v_star = get_v_star(T)
neighbors_selection = [create_neighbors_list(initial_x, v_star) for i in range(num_schedules)][1] # This can be done in parallel to improve speed
neighbors_list = [create_neighbors_list(initial_x, v_star) for schedule in neighbors_selection]
end = time.time()
h = random.choices(range(num_schedules), k=7)
print(f"Sampled schedules: {h}")
for i in h:
original_schedule = neighbors_list[i][0]
neighbor_schedule = neighbors_list[i][1]
difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]
print(f"Neighbors\n{original_schedule}\n{neighbor_schedule}\n{difference}")
training_set_feat_time = end - start
print(f"\nProcessing time: {training_set_feat_time} seconds\n")
from functions import get_v_star
def create_neighbors_list(x: list[int], v_star: np.ndarray) -> (list[int], list[int]):
"""
Create a set of pairs of schedules that are from the same neighborhood.
Parameters:
x (list[int]): A list of integers with |s| = T and sum N.
v_star (np.ndarray): Precomputed vectors V* of length T.
Returns:
tuple(list[int], list[int]): A pair of schedules.
"""
T = len(x)
# Precompute binomial coefficients (weights for random.choices)
binom_coeff = [math.comb(T, i) for i in range(1, T)]
# Choose a random value of i with the corresponding probability
i = random.choices(range(1, T), weights=binom_coeff)[0]
# Instead of generating the full list of combinations, sample one directly
j = random.sample(range(T), i)
x_p = x.copy()
for k in j:
x_temp = np.array(x_p) + v_star[k]
x_temp = x_temp.astype(int)
if np.all(x_temp >= 0):
x_p = x_temp.astype(int).tolist()
return x, x_p
start = time.time()
v_star = get_v_star(T)
neighbors_selection = [create_neighbors_list(initial_x, v_star) for i in range(num_schedules)][1] # This can be done in parallel to improve speed
neighbors_list = [create_neighbors_list(schedule, v_star) for schedule in neighbors_selection]
end = time.time()
h = random.choices(range(num_schedules), k=7)
print(f"Sampled schedules: {h}")
for i in h:
original_schedule = neighbors_list[i][0]
neighbor_schedule = neighbors_list[i][1]
difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]
print(f"Neighbors\n{original_schedule}\n{neighbor_schedule}\n{difference}")
training_set_feat_time = end - start
print(f"\nProcessing time: {training_set_feat_time} seconds\n")
reticulate::repl_python()
