# Matern kernel is a common choice, nu=2.5 is smooth (twice differentiable)
# ConstantKernel handles the overall variance scaling
# WhiteKernel handles the observation noise
kernel = ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \
Matern(length_scale=np.ones(m), # Enable ARD, initialize length scales to 1
length_scale_bounds=(1e-2, 1e2),
nu=2.5) + \
WhiteKernel(noise_level=1e-10, # Small value for numerical stability
noise_level_bounds="fixed") # Bounds for noise optimization
# Instantiate the Gaussian Process Regressor
# alpha: Value added to the diagonal of the kernel matrix during fitting
#        for numerical stability (can also be seen as additional noise)
# n_restarts_optimizer: Restarts optimizer to find better hyperparameters
gp_model = GaussianProcessRegressor(
kernel=kernel,
alpha=1e-10, # Small value for numerical stability
n_restarts_optimizer=10, # More restarts -> better hyperparams but slower
random_state=42 # For reproducibility of optimizer restarts
)
# Fit the GP model
gp_model.fit(train_X_embedded_scaled, train_Y)
return gp_model
def expected_improvement(mu, sigma, f_best, xi=0.01):
"""
Computes the Expected Improvement acquisition function.
Assumes maximization (f_best is the current maximum observed value).
mu, sigma: Predicted mean and standard deviation (NumPy arrays).
f_best: Current best observed function value (scalar).
xi: Exploration-exploitation trade-off parameter.
"""
# Ensure sigma is positive and non-zero to avoid division errors
sigma = np.maximum(sigma, 1e-9)
Z = (mu - f_best - xi) / sigma
ei = (mu - f_best - xi) * norm.cdf(Z) + sigma * norm.pdf(Z)
# Set EI to 0 where variance is negligible
ei[sigma <= 1e-9] = 0.0
return ei
# MODIFIED: Accepts the scaler and uses scikit-learn GP + EI
def optimize_acqf_discrete_via_embedding(gp_model, scaler, dictionary_A, T, q, num_candidates, current_best_neg_f_val):
"""
Optimizes acquisition function (Expected Improvement) by sampling random
binary candidates, embedding, SCALING, predicting with GP, and calculating EI.
Selects the top q candidates based on EI.
Returns candidates as a numpy array (q x T).
"""
m = dictionary_A.shape[0]
# 1. Generate Random Binary Candidates
candidate_u_vectors_np = np.random.randint(0, 2, size=(num_candidates, T))
# Optional: Ensure unique candidates if needed (adds overhead)
# candidate_u_vectors_np = np.unique(candidate_u_vectors_np, axis=0)
# num_candidates = candidate_u_vectors_np.shape[0] # Update count
# 2. Embed the Candidates
embedded_candidates_np = embed_batch(candidate_u_vectors_np, dictionary_A)
# 3. Scale the Embedded Candidates
# Handle potential warning if scaler expects float64 (already float here)
# Use the *fitted* scaler from the training data
embedded_candidates_scaled_np = scaler.transform(embedded_candidates_np)
# 4. Predict Mean and Std Dev using the GP Model
mu, std = gp_model.predict(embedded_candidates_scaled_np, return_std=True)
# 5. Calculate Acquisition Function (Expected Improvement)
# current_best_neg_f_val is the maximum of the (negative) objectives seen so far
acq_values = expected_improvement(mu, std, current_best_neg_f_val, xi=0.01)
# 6. Select Top Candidates
# Use np.argsort to find indices that would sort the array (ascending)
# Select the last q indices for the highest EI values
# If q=1, np.argmax(acq_values) is simpler but argsort works generally
top_indices = np.argsort(acq_values)[-q:]
# Ensure indices are returned in descending order of acquisition value (optional but nice)
top_indices = top_indices[::-1]
return candidate_u_vectors_np[top_indices, :]
# --- BO Loop ---
# Parameters
N_INITIAL = 20
N_ITERATIONS = 3
BATCH_SIZE_q = 5
NUM_CANDIDATES_Acqf = T*1024 # Might need more for higher T
m = 128 # Dimension of the embedding space
# Store evaluated points (using NumPy arrays)
evaluated_U_np_list = [] # List to store evaluated U vectors (binary)
evaluated_f_vals = []    # List to store raw objective values (lower is better)
train_Y_list = []        # List to store NEGATED objective values for GP (higher is better)
# 1. Initialization
print(f"Generating {N_INITIAL} initial points...")
initial_candidates = []
while len(initial_candidates) < N_INITIAL:
U_init = np.random.randint(0, 2, size=T)
# Ensure unique initial points
is_duplicate = any(np.array_equal(U_init, u) for u in initial_candidates)
if not is_duplicate:
initial_candidates.append(U_init)
for U_init in initial_candidates:
f_val = evaluate_objective(U_init, X, v_star, convolutions, d, w)
neg_f_val = -f_val
evaluated_U_np_list.append(U_init)
evaluated_f_vals.append(f_val)
train_Y_list.append(neg_f_val)
# Convert lists to NumPy arrays for GP fitting
train_Y = np.array(train_Y_list).reshape(-1, 1) # Keep as column vector initially
best_obj_so_far = min(evaluated_f_vals) if evaluated_f_vals else float('inf')
print(f"Initial best objective value: {best_obj_so_far}")
if not np.isfinite(best_obj_so_far):
print("Warning: Initial best objective is infinite, possibly all initial points were infeasible.")
# 2. BO Iterations
for iteration in range(N_ITERATIONS):
start_time = time.time()
print(f"\n--- Iteration {iteration + 1}/{N_ITERATIONS} ---")
# a. Generate dictionary A for HED
current_dictionary_A = generate_diverse_random_dictionary(T, m)
# b. Embed ALL evaluated U vectors so far
if not evaluated_U_np_list:
print("Warning: No points evaluated yet. Skipping iteration.")
continue
evaluated_U_np_array = np.array(evaluated_U_np_list)
embedded_train_X = embed_batch(evaluated_U_np_array, current_dictionary_A)
# c. Scale the embedded training data
scaler = MinMaxScaler()
# Fit scaler only if there's data
if embedded_train_X.shape[0] > 0:
# Fit and transform
embedded_train_X_scaled = scaler.fit_transform(embedded_train_X)
else:
# Handle case with no data (shouldn't happen after init)
embedded_train_X_scaled = embedded_train_X # Will be empty
# Ensure train_Y is a NumPy array for fitting
train_Y_for_fit = np.array(train_Y_list) # Use the list directly
# d. Fit GP Model using SCALED data
print("Fitting GP model...")
if embedded_train_X_scaled.shape[0] > 0 and train_Y_for_fit.shape[0] == embedded_train_X_scaled.shape[0]:
gp_model = get_fitted_model(embedded_train_X_scaled, train_Y_for_fit, m)
print("GP model fitted.")
else:
print("Warning: Not enough data or data mismatch to fit GP model. Skipping iteration.")
continue # Skip if no data or mismatch
# e. Determine current best value for Acquisition Function
# We are maximizing the negative objective in the GP
current_best_neg_f_val = np.max(train_Y_for_fit) if train_Y_for_fit.size > 0 else -float('inf')
# Prevent potential issues if all points were infeasible (very large negative best_f)
if current_best_neg_f_val <= -LARGE_PENALTY / 2 and np.isfinite(current_best_neg_f_val):
print(f"Warning: Current best value ({current_best_neg_f_val:.2f}) is very low (likely from penalties). Acqf might behave unexpectedly.")
# f. Optimize Acquisition Function (Expected Improvement)
print("Optimizing acquisition function...")
next_U_candidates_np = optimize_acqf_discrete_via_embedding(
gp_model=gp_model,
scaler=scaler, # Pass the fitted scaler
dictionary_A=current_dictionary_A,
T=T,
q=BATCH_SIZE_q,
num_candidates=NUM_CANDIDATES_Acqf,
current_best_neg_f_val=current_best_neg_f_val
)
print(f"Selected {next_U_candidates_np.shape[0]} candidate(s).")
# g. Evaluate Objective for the selected candidate(s)
newly_evaluated_U = []
newly_evaluated_f = []
newly_evaluated_neg_f = []
for i in range(next_U_candidates_np.shape[0]):
next_U = next_U_candidates_np[i, :]
# Check if this candidate was already evaluated
# Use a tolerance for floating point comparisons if U were continuous
# For binary, exact comparison is fine
already_evaluated = any(np.array_equal(next_U, u) for u in evaluated_U_np_list)
if already_evaluated:
print(f"  Candidate {i} was already evaluated. Skipping re-evaluation.")
# TODO: Optionally, could try to generate a *different* candidate here
#       e.g., by running optimize_acqf again excluding this one,
#       or sampling randomly near it. For now, just skip.
continue # Skip to next candidate
# Evaluate the objective
next_f = evaluate_objective(next_U, X, v_star, convolutions, d, w)
next_neg_f = -next_f
print(f"  Candidate {i}: Obj = {next_f:.4f}")
# Add to temporary lists for this iteration
newly_evaluated_U.append(next_U)
newly_evaluated_f.append(next_f)
newly_evaluated_neg_f.append(next_neg_f)
# Update overall best objective found
if next_f < best_obj_so_far:
best_obj_so_far = next_f
# h. Augment Dataset for next iteration
evaluated_U_np_list.extend(newly_evaluated_U)
evaluated_f_vals.extend(newly_evaluated_f)
train_Y_list.extend(newly_evaluated_neg_f) # Add negative values for next GP fit
# Convert train_Y_list back to array for potential use (though we rebuild it next iter)
train_Y = np.array(train_Y_list).reshape(-1, 1)
iter_time = time.time() - start_time
print(f"Best objective value found so far: {best_obj_so_far:.4f}")
print(f"Total points evaluated: {len(evaluated_f_vals)}")
print(f"Iteration {iteration + 1} completed in {iter_time:.2f} seconds.")
# --- Results ---
print("\n--- Optimization Finished ---")
if not evaluated_f_vals:
print("No points were successfully evaluated.")
else:
# Find the best point among all evaluated points
final_best_idx = np.argmin(evaluated_f_vals) # Index of minimum raw objective
final_best_U = evaluated_U_np_list[final_best_idx]
final_best_f = evaluated_f_vals[final_best_idx]
print(f"Total evaluations: {len(evaluated_f_vals)}")
print(f"Best Objective Value Found: {final_best_f}")
# Ensure U is printed correctly if it's long
print(f"Best U vector Found: {final_best_U}")
# print(f"Best U vector Found (Indices of 1s): {np.where(final_best_U == 1)[0]}")
# Verification - Recalculate Y for the best U found
V_sum_best = np.sum(v_star[final_best_U == 1, :], axis=0)
Y_best = X + V_sum_best
is_feasible = np.all(Y_best >= 0)
if is_feasible:
ewt, esp = calculate_objective_serv_time_lookup(Y_best, d, convolutions)
recalculated_obj = w * ewt + (1 - w) * esp
else:
LARGE_PENALTY
print(f"\n--- Verification ---")
print(f"Is the best U feasible? {is_feasible}")
if is_feasible:
print(f"Resulting Y vector for best U: {Y_best}")
print(f"Objective value (recalculated): {recalculated_obj:.4f}")
if not np.isclose(final_best_f, recalculated_obj):
print(f"Warning: Stored best objective ({final_best_f}) does not match recalculation ({recalculated_obj})!")
elif final_best_f < LARGE_PENALTY:
print(f"Warning: Best objective ({final_best_f}) is not the penalty value, but feasibility check failed.")
print(f"Resulting Y vector (infeasible): {Y_best}")
else:
print("Best solution found corresponds to an infeasible penalty value.")
# --- BO Helper Functions ---
# --- get_fitted_model function remains the same ---
def get_fitted_model(train_X_embedded_scaled, train_Y, m):
# ... (implementation is unchanged) ...
if train_Y.ndim > 1 and train_Y.shape[1] == 1: train_Y = train_Y.ravel()
kernel = ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \
Matern(length_scale=np.ones(m), length_scale_bounds=(1e-2, 1e2), nu=2.5) + \
WhiteKernel(noise_level=1e-10, # Small value for numerical stability
noise_level_bounds="fixed") # Bounds for noise optimization
gp_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-10, n_restarts_optimizer=10, random_state=42)
gp_model.fit(train_X_embedded_scaled, train_Y)
return gp_model
def lower_confidence_bound(mu, sigma, kappa=2.576):
"""
Computes the Lower Confidence Bound (LCB) acquisition function.
Assumes maximization of this value guides the search (since mu is neg objective).
Higher LCB means lower predicted objective or lower penalty for uncertainty.
mu, sigma: Predicted mean and standard deviation (NumPy arrays).
kappa: Controls the balance between exploitation (high mu -> low original objective)
and exploration (low sigma).
"""
# Ensure sigma is non-negative
sigma = np.maximum(sigma, 0)
return mu - kappa * sigma # <<< Sign flipped from UCB
def optimize_acqf_discrete_via_embedding(gp_model, scaler, dictionary_A, T, q, num_candidates, kappa):
"""
Optimizes LCB acquisition function by sampling random binary candidates,
embedding, SCALING, predicting with GP, and calculating LCB.
Selects the top q candidates based on LCB.
Returns candidates as a numpy array (q x T).
"""
m = dictionary_A.shape[0]
# 1. Generate Random Binary Candidates
candidate_u_vectors_np = np.random.randint(0, 2, size=(num_candidates, T))
# 2. Embed the Candidates
embedded_candidates_np = embed_batch(candidate_u_vectors_np, dictionary_A)
# 3. Scale the Embedded Candidates
embedded_candidates_scaled_np = scaler.transform(embedded_candidates_np)
# 4. Predict Mean and Std Dev using the GP Model
mu, std = gp_model.predict(embedded_candidates_scaled_np, return_std=True)
# 5. Calculate Acquisition Function (Lower Confidence Bound) <<< CHANGED HERE
acq_values = lower_confidence_bound(mu, std, kappa=kappa) # Use LCB
# 6. Select Top Candidates (based on highest LCB) <<< COMMENT UPDATED
# We maximize LCB = mu - kappa*sigma, where mu is neg_objective
top_indices = np.argsort(acq_values)[-q:]
top_indices = top_indices[::-1] # Ensure descending order of LCB
return candidate_u_vectors_np[top_indices, :]
# --- BO Loop ---
# Parameters
KAPPA = 2.576 # Exploration parameter for LCB. Adjust as needed.
N_INITIAL = 20
N_ITERATIONS = 25
BATCH_SIZE_q = 5
NUM_CANDIDATES_Acqf = T*1024 # Might need more for higher T
m = 128 # Dimension of the embedding space
# Store evaluated points (using NumPy arrays)
evaluated_U_np_list = [] # List to store evaluated U vectors (binary)
evaluated_f_vals = []    # List to store raw objective values (lower is better)
train_Y_list = []        # List to store NEGATED objective values for GP (higher is better)
# 1. Initialization
for U_init in initial_candidates:
f_val = evaluate_objective(U_init, X, v_star, convolutions, d, w)
neg_f_val = -f_val
evaluated_U_np_list.append(U_init)
evaluated_f_vals.append(f_val)
train_Y_list.append(neg_f_val)
# Convert lists to NumPy arrays for GP fitting
train_Y = np.array(train_Y_list).reshape(-1, 1) # Keep as column vector initially
best_obj_so_far = min(evaluated_f_vals) if evaluated_f_vals else float('inf')
print(f"Initial best objective value: {best_obj_so_far}")
if not np.isfinite(best_obj_so_far):
print("Warning: Initial best objective is infinite, possibly all initial points were infeasible.")
# 2. BO Iterations
for iteration in range(N_ITERATIONS):
start_time = time.time()
print(f"\n--- Iteration {iteration + 1}/{N_ITERATIONS} ---")
# a. Generate dictionary A (remains the same)
current_dictionary_A = generate_diverse_random_dictionary(T, m)
# b. Embed ALL evaluated U vectors (remains the same)
if not evaluated_U_np_list: continue
evaluated_U_np_array = np.array(evaluated_U_np_list)
embedded_train_X = embed_batch(evaluated_U_np_array, current_dictionary_A)
# c. Scale the embedded training data (remains the same)
scaler = MinMaxScaler()
if embedded_train_X.shape[0] > 0: embedded_train_X_scaled = scaler.fit_transform(embedded_train_X)
else: embedded_train_X_scaled = embedded_train_X
# Ensure train_Y is NumPy array
train_Y_for_fit = np.array(train_Y_list)
# d. Fit GP Model (remains the same)
print("Fitting GP model...")
if embedded_train_X_scaled.shape[0] > 0 and train_Y_for_fit.shape[0] == embedded_train_X_scaled.shape[0]:
gp_model = get_fitted_model(embedded_train_X_scaled, train_Y_for_fit, m)
print("GP model fitted.")
else:
print("Warning: Not enough data or data mismatch to fit GP model. Skipping iteration.")
continue
# e. Determine current best neg value (useful for tracking, not directly used in LCB calculation)
current_best_neg_f_val = np.max(train_Y_for_fit) if train_Y_for_fit.size > 0 else -float('inf')
if current_best_neg_f_val <= -LARGE_PENALTY / 2 and np.isfinite(current_best_neg_f_val):
print(f"Warning: Current best NEGATIVE objective value ({current_best_neg_f_val:.2f}) is very low (likely from penalties).")
# f. Optimize Acquisition Function (LCB) <<< MODIFIED CALL & COMMENT
print("Optimizing acquisition function (LCB)...") # Comment updated
next_U_candidates_np = optimize_acqf_discrete_via_embedding(
gp_model=gp_model,
scaler=scaler,
dictionary_A=current_dictionary_A,
T=T,
q=BATCH_SIZE_q,
num_candidates=NUM_CANDIDATES_Acqf,
kappa=KAPPA # Pass kappa
)
print(f"Selected {next_U_candidates_np.shape[0]} candidate(s).")
# g. Evaluate Objective (remains the same)
newly_evaluated_U = []; newly_evaluated_f = []; newly_evaluated_neg_f = []
for i in range(next_U_candidates_np.shape[0]):
next_U = next_U_candidates_np[i, :]
already_evaluated = any(np.array_equal(next_U, u) for u in evaluated_U_np_list)
if already_evaluated: print(f"  Candidate {i} was already evaluated. Skipping."); continue
next_f = evaluate_objective(next_U, X, v_star, convolutions, d, w)
next_neg_f = -next_f
print(f"  Candidate {i}: Obj = {next_f:.4f}")
newly_evaluated_U.append(next_U); newly_evaluated_f.append(next_f); newly_evaluated_neg_f.append(next_neg_f)
if next_f < best_obj_so_far: best_obj_so_far = next_f
# h. Augment Dataset (remains the same)
evaluated_U_np_list.extend(newly_evaluated_U); evaluated_f_vals.extend(newly_evaluated_f); train_Y_list.extend(newly_evaluated_neg_f)
train_Y = np.array(train_Y_list).reshape(-1, 1)
iter_time = time.time() - start_time
print(f"Best objective value found so far: {best_obj_so_far:.4f}")
print(f"Total points evaluated: {len(evaluated_f_vals)}")
print(f"Iteration {iteration + 1} completed in {iter_time:.2f} seconds.")
# --- Results ---
print("\n--- Optimization Finished ---")
if not evaluated_f_vals: print("No points were successfully evaluated.")
else:
final_best_idx = np.argmin(evaluated_f_vals)
final_best_U = evaluated_U_np_list[final_best_idx]
final_best_f = evaluated_f_vals[final_best_idx]
print(f"Total evaluations: {len(evaluated_f_vals)}")
print(f"Best Objective Value Found: {final_best_f}")
print(f"Best U vector Found: {final_best_U}")
# Verification
V_sum_best = np.sum(v_star[final_best_U == 1, :], axis=0); Y_best = X + V_sum_best
is_feasible = np.all(Y_best >= 0); recalculated_obj = LARGE_PENALTY
if is_feasible:
ewt, esp = calculate_objective_serv_time_lookup(Y_best, d, convolutions)
recalculated_obj = w * ewt + (1 - w) * esp
print(f"\n--- Verification ---")
print(f"Is the best U feasible? {is_feasible}")
if is_feasible:
print(f"Resulting Y vector for best U: {Y_best}")
print(f"Objective value (recalculated): {recalculated_obj:.4f}")
if not np.isclose(final_best_f, recalculated_obj): print(f"Warning: Stored best objective ({final_best_f:.4f}) does not match recalculation ({recalculated_obj:.4f})!")
elif final_best_f < LARGE_PENALTY: print(f"Warning: Best objective ({final_best_f:.4f}) is not the penalty value, but feasibility check failed."); print(f"Resulting Y vector (infeasible): {Y_best}")
else: print("Best solution found corresponds to an infeasible penalty value.")
# --- BO Loop ---
# Parameters
# +++ KAPPA is now dynamic +++
INITIAL_KAPPA = 3.75 # Starting value for kappa (e.g., 1.96 corresponds to ~95% CI)
KAPPA_INCREASE_FACTOR = 1.3 # Factor to multiply kappa by on improvement (e.g., 10% increase)
# Optional: Add a maximum kappa to prevent excessive exploration
MAX_KAPPA = 15.0
N_INITIAL = 20
N_ITERATIONS = 25
BATCH_SIZE_q = 5
NUM_CANDIDATES_Acqf = T*1024 # Might need more for higher T
m = 128 # Dimension of the embedding space
# Store evaluated points (using NumPy arrays)
evaluated_U_np_list = [] # List to store evaluated U vectors (binary)
evaluated_f_vals = []    # List to store raw objective values (lower is better)
train_Y_list = []        # List to store NEGATED objective values for GP (higher is better)
# 1. Initialization
for U_init in initial_candidates:
f_val = evaluate_objective(U_init, X, v_star, convolutions, d, w)
neg_f_val = -f_val
evaluated_U_np_list.append(U_init)
evaluated_f_vals.append(f_val)
train_Y_list.append(neg_f_val)
# Convert lists to NumPy arrays for GP fitting
train_Y = np.array(train_Y_list).reshape(-1, 1) # Keep as column vector initially
best_obj_so_far = min(evaluated_f_vals) if evaluated_f_vals else float('inf')
print(f"Initial best objective value: {best_obj_so_far}")
if not np.isfinite(best_obj_so_far):
print("Warning: Initial best objective is infinite, possibly all initial points were infeasible.")
# +++ Initialize current_kappa +++
current_kappa = INITIAL_KAPPA
print(f"Initial Kappa: {current_kappa:.3f}")
# 2. BO Iterations
for iteration in range(N_ITERATIONS):
start_time = time.time()
print(f"\n--- Iteration {iteration + 1}/{N_ITERATIONS} ---")
# a. Generate dictionary A (remains the same)
current_dictionary_A = generate_diverse_random_dictionary(T, m)
# b. Embed ALL evaluated U vectors (remains the same)
if not evaluated_U_np_list: continue
evaluated_U_np_array = np.array(evaluated_U_np_list)
embedded_train_X = embed_batch(evaluated_U_np_array, current_dictionary_A)
# c. Scale the embedded training data (remains the same)
scaler = MinMaxScaler()
if embedded_train_X.shape[0] > 0: embedded_train_X_scaled = scaler.fit_transform(embedded_train_X)
else: embedded_train_X_scaled = embedded_train_X
# Ensure train_Y is NumPy array
train_Y_for_fit = np.array(train_Y_list)
# d. Fit GP Model (remains the same)
print("Fitting GP model...")
if embedded_train_X_scaled.shape[0] > 0 and train_Y_for_fit.shape[0] == embedded_train_X_scaled.shape[0]:
gp_model = get_fitted_model(embedded_train_X_scaled, train_Y_for_fit, m)
print("GP model fitted.")
else:
print("Warning: Not enough data or data mismatch to fit GP model. Skipping iteration.")
continue
# e. Determine current best neg value (useful for tracking)
current_best_neg_f_val = np.max(train_Y_for_fit) if train_Y_for_fit.size > 0 else -float('inf')
# (Warning about low neg_f_val is still relevant)
if current_best_neg_f_val <= -LARGE_PENALTY / 2 and np.isfinite(current_best_neg_f_val):
print(f"Warning: Current best NEGATIVE objective value ({current_best_neg_f_val:.2f}) is very low.")
# f. Optimize Acquisition Function (LCB) using current_kappa <<< MODIFIED CALL & COMMENT
print(f"Optimizing acquisition function (LCB) with Kappa = {current_kappa:.3f}...") # Show kappa being used
next_U_candidates_np = optimize_acqf_discrete_via_embedding(
gp_model=gp_model,
scaler=scaler,
dictionary_A=current_dictionary_A,
T=T,
q=BATCH_SIZE_q,
num_candidates=NUM_CANDIDATES_Acqf,
kappa=current_kappa # Pass the current (potentially updated) kappa
)
print(f"Selected {next_U_candidates_np.shape[0]} candidate(s).")
# g. Evaluate Objective AND UPDATE KAPPA ON IMPROVEMENT <<< MODIFIED HERE
newly_evaluated_U = []; newly_evaluated_f = []; newly_evaluated_neg_f = []
improvement_found_in_batch = False # Flag to track if kappa was updated in this batch
for i in range(next_U_candidates_np.shape[0]):
next_U = next_U_candidates_np[i, :]
already_evaluated = any(np.array_equal(next_U, u) for u in evaluated_U_np_list)
if already_evaluated: print(f"  Candidate {i} was already evaluated. Skipping."); continue
# Evaluate the objective
next_f = evaluate_objective(next_U, X, v_star, convolutions, d, w)
next_neg_f = -next_f
V_sum = np.sum(v_star[next_U == 1, :], axis=0)
Y = X + V_sum
print(f"  Candidate {i}: X = {Y}, Obj = {next_f:.4f}")
# Add to temporary lists for this iteration
newly_evaluated_U.append(next_U); newly_evaluated_f.append(next_f); newly_evaluated_neg_f.append(next_neg_f)
# --- Check for improvement and update kappa ---
if next_f < best_obj_so_far:
print(f"  ** Improvement found! Old best: {best_obj_so_far:.4f}, New best: {next_f:.4f}")
best_obj_so_far = next_f
# Increase Kappa
old_kappa = current_kappa
current_kappa *= KAPPA_INCREASE_FACTOR
# Optional: Cap Kappa
# current_kappa = min(current_kappa, MAX_KAPPA)
print(f"  ** Increasing Kappa from {old_kappa:.3f} to {current_kappa:.3f}")
improvement_found_in_batch = True # Set flag
# h. Augment Dataset (remains the same)
evaluated_U_np_list.extend(newly_evaluated_U); evaluated_f_vals.extend(newly_evaluated_f); train_Y_list.extend(newly_evaluated_neg_f)
train_Y = np.array(train_Y_list).reshape(-1, 1)
iter_time = time.time() - start_time
print(f"Best objective value found so far: {best_obj_so_far:.4f}")
print(f"Total points evaluated: {len(evaluated_f_vals)}")
# Also report the kappa value that will be used in the *next* iteration
print(f"Kappa for next iteration: {current_kappa:.3f}")
print(f"Iteration {iteration + 1} completed in {iter_time:.2f} seconds.")
# --- Results ---
# <<< Results section remains the same, including Verification >>>
print("\n--- Optimization Finished ---")
# ... (Rest of the results reporting and verification code is unchanged) ...
if not evaluated_f_vals: print("No points were successfully evaluated.")
else:
final_best_idx = np.argmin(evaluated_f_vals)
final_best_U = evaluated_U_np_list[final_best_idx]
final_best_f = evaluated_f_vals[final_best_idx]
print(f"Total evaluations: {len(evaluated_f_vals)}")
print(f"Best Objective Value Found: {final_best_f}")
print(f"Best U vector Found: {final_best_U}")
# Verification
V_sum_best = np.sum(v_star[final_best_U == 1, :], axis=0); Y_best = X + V_sum_best
is_feasible = np.all(Y_best >= 0); recalculated_obj = LARGE_PENALTY
if is_feasible:
ewt, esp = calculate_objective_serv_time_lookup(Y_best, d, convolutions)
recalculated_obj = w * ewt + (1 - w) * esp
print(f"\n--- Verification ---")
print(f"Is the best U feasible? {is_feasible}")
if is_feasible:
print(f"Resulting Y vector for best U: {Y_best}")
print(f"Objective value (recalculated): {recalculated_obj:.4f}")
if not np.isclose(final_best_f, recalculated_obj): print(f"Warning: Stored best objective ({final_best_f:.4f}) does not match recalculation ({recalculated_obj:.4f})!")
elif final_best_f < LARGE_PENALTY: print(f"Warning: Best objective ({final_best_f:.4f}) is not the penalty value, but feasibility check failed."); print(f"Resulting Y vector (infeasible): {Y_best}")
else: print("Best solution found corresponds to an infeasible penalty value.")
reticulate::repl_python()
