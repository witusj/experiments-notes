import optuna
import logging # Optional: To see pruned trial messages if desired
import sys
import math
import numpy as np
import time
from scipy.optimize import minimize
from itertools import combinations
from typing import List, Dict, Tuple, Callable, Optional, Union, Any, Iterable # Added type hints
import multiprocessing as mp
# Optional: Configure logging to see messages about pruned trials
# optuna.logging.get_logger("optuna").addHandler(logging.StreamHandler(sys.stdout))
from functions import compute_convolutions, bailey_welch_schedule, get_v_star
N = 12 # Number of patients
T = 8 # Number of intervals
d = 5 # Length of each interval
max_s = 20 # Maximum service time
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.1 # Weight for the waiting time in objective function
l = 10
v_star = get_v_star(T)
print("v_star: \n", v_star)
# Create service time distribution
def generate_weighted_list(max_s: int, l: float, i: int) -> Optional[np.ndarray]:
"""
Generates a service time probability distribution using optimization.
This function creates a discrete probability distribution over max_s possible
service times (from 1 to max_s). It uses optimization (SLSQP) to find a
distribution whose weighted average service time is as close as possible
to a target value 'l', subject to the constraint that the probabilities
sum to 1 and each probability is between 0 and 1.
After finding the distribution, it sorts the probabilities: the first 'i'
probabilities (corresponding to service times 1 to i) are sorted in
ascending order, and the remaining probabilities (service times i+1 to max_s)
are sorted in descending order.
Note:
- Requires NumPy and SciPy libraries (specifically scipy.optimize.minimize).
Args:
max_s (int): Maximum service time parameter (number of probability bins).
Must be a positive integer.
l (float): The target weighted average service time for the distribution.
Must be between 1 and max_s, inclusive.
i (int): The index determining the sorting split point. Probabilities
for service times 1 to 'i' are sorted ascendingly, and
probabilities for service times 'i+1' to 'max_s' are sorted
descendingly. Must be between 1 and max_s-1 for meaningful sorting.
Returns:
numpy.ndarray: An array of size max_s+1. The first element (index 0) is 0.
Elements from index 1 to max_s represent the calculated
and sorted probability distribution, summing to 1.
Returns None if optimization fails or inputs are invalid.
"""
# --- Input Validation ---
if not isinstance(max_s, int) or max_s <= 0:
print(f"Error: max_s must be a positive integer, but got {max_s}")
return None
if not isinstance(l, (int, float)) or not (1 <= l <= max_s):
print(f"Error: Target average 'l' ({l}) must be between 1 and max_s ({max_s}).")
return None
if not isinstance(i, int) or not (0 < i < max_s):
print(f"Error: Sorting index 'i' ({i}) must be between 1 and max_s-1 ({max_s-1}).")
# If clamping is desired instead of error:
# print(f"Warning: Index 'i' ({i}) is outside the valid range (1 to {max_s-1}). Clamping i.")
# i = max(1, min(i, max_s - 1))
return None # Strict check based on docstring requirement
# --- Inner helper function for optimization ---
def objective(x: np.ndarray) -> float:
"""Objective function: Squared difference between weighted average and target l."""
# x represents probabilities P(1) to P(max_s)
service_times = np.arange(1, max_s + 1)
weighted_avg = np.dot(service_times, x) # Equivalent to sum(k * P(k) for k=1 to max_s)
return (weighted_avg - l) ** 2
# --- Constraints for optimization ---
# Constraint 1: The sum of the probabilities must be 1
constraints = ({
'type': 'eq',
'fun': lambda x: np.sum(x) - 1.0 # Ensure float comparison
})
# Bounds: Each probability value x[k] must be between 0 and 1
# Creates a list of max_s tuples, e.g., [(0, 1), (0, 1), ..., (0, 1)]
bounds = [(0, 1)] * max_s
# Initial guess: Use Dirichlet distribution to get a random distribution that sums to 1.
# Provides a starting point for the optimizer. np.ones(max_s) gives equal weights initially.
initial_guess = np.random.dirichlet(np.ones(max_s))
# --- Perform Optimization ---
try:
result = minimize(
objective,
initial_guess,
method='SLSQP',
bounds=bounds,
constraints=constraints,
# options={'disp': False} # Set True for detailed optimizer output
)
# Check if optimization was successful
if not result.success:
print(f"Warning: Optimization failed! Message: {result.message}")
# Optionally print result object for more details: print(result)
return None # Indicate failure
# The optimized probabilities (P(1) to P(max_s))
optimized_probs = result.x
# --- Post-process: Correct potential floating point inaccuracies ---
# Ensure probabilities are non-negative and sum *exactly* to 1
optimized_probs[optimized_probs < 0] = 0 # Clamp small negatives to 0
current_sum = np.sum(optimized_probs)
if not np.isclose(current_sum, 1.0):
if current_sum > 0: # Avoid division by zero
optimized_probs /= current_sum # Normalize to sum to 1
else:
print("Warning: Optimization resulted in zero sum probabilities after clamping negatives.")
# Handle this case - maybe return uniform distribution or None
return None # Or return uniform: np.ones(max_s) / max_s
except Exception as e:
print(f"An error occurred during optimization: {e}")
return None
# --- Reorder the probabilities based on the index 'i' ---
# Split the probabilities P(1)...P(i) and P(i+1)...P(max_s)
# Note: Python slicing is exclusive of the end index, array indexing is 0-based.
# result.x[0] corresponds to P(1), result.x[i-1] to P(i).
# result.x[i] corresponds to P(i+1), result.x[max_s-1] to P(max_s).
first_part_probs = optimized_probs[:i]   # Probabilities P(1) to P(i)
second_part_probs = optimized_probs[i:]  # Probabilities P(i+1) to P(max_s)
# Sort the first part ascending, the second part descending
sorted_first_part = np.sort(first_part_probs)
sorted_second_part = np.sort(second_part_probs)[::-1] # [::-1] reverses
# --- Create final output array ---
# Array of size max_s + 1, initialized to zeros. Index 0 unused.
values = np.zeros(max_s + 1)
# Assign the sorted probabilities back into the correct slots (index 1 onwards)
values[1 : i + 1] = sorted_first_part      # Assign P(1)...P(i)
values[i + 1 : max_s + 1] = sorted_second_part # Assign P(i+1)...P(max_s)
# Final check on sum after potential normalization/sorting
if not np.isclose(np.sum(values[1:]), 1.0):
print(f"Warning: Final distribution sum is {np.sum(values[1:])}, not 1.0. Check logic.")
# Return the final array with the sorted probability distribution
return values
i = 5  # First 5 highest values in ascending order, rest in descending order
s = generate_weighted_list(max_s, l, i)
print("Service time distribution: ", s)
print("Sum: ", np.sum(s))  # This should be 1
print("Weighted service time:", np.dot(np.arange(len(s)), s))  # This should be close to l
x_star = bailey_welch_schedule(T, d, N, s)
print(f"Initial schedule: {x_star}")
convolutions = compute_convolutions(s, N, q)
from vns_logic import variable_neighborhood_search
# This guard is ESSENTIAL for multiprocessing in notebooks/scripts
if __name__ == "__main__":
# --- Configure Logging ---
log_file = 'vns_run.log'
logging.basicConfig(
level=logging.INFO,  # Log INFO level and above (INFO, WARNING, ERROR, CRITICAL)
format='%(asctime)s - %(levelname)s - %(processName)s - %(message)s',
filename=log_file,
filemode='w'  # 'w' clears the file each time, 'a' appends
)
# Optional: Add a handler to also print logs to console
# console_handler = logging.StreamHandler(sys.stdout)
# console_handler.setLevel(logging.INFO) # Or DEBUG
# console_handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s')) # Simpler format for console
# logging.getLogger().addHandler(console_handler)
# --- Handle potential import error from earlier ---
try:
# Check if the required functions were actually imported or defined
calculate_objective_serv_time_lookup
powerset
get_neighborhood_for_parallel
logging.info("Successfully imported or defined helper functions.")
except NameError:
logging.error("Essential helper functions are missing. Exiting.")
sys.exit("Exiting due to missing helper functions. Check imports or definitions.")
# --- Crucial for 'spawn' start method (Windows, macOS, Quarto/Jupyter) ---
mp.freeze_support() # Needed for frozen executables, good practice
logging.info("Script execution started.")
print("Starting VNS from Quarto...")
start_time = time.time()
# Call the imported function
best_solution, best_cost = variable_neighborhood_search(
x_init=x_star,
d=d,
convolutions=convolutions,
w=w,
v_star=v_star,
echo=True # Set to False for less output
)
end_time = time.time()
print("\n--- VNS Result ---")
print(f"Best solution found: {best_solution}")
print(f"Best cost found: {best_cost:.4f}")
print(f"Total execution time: {end_time - start_time:.2f} seconds")
from vns_logic import variable_neighborhood_search
from functions import calculate_objective_serv_time_lookup, powerset, get_neighborhood_for_parallel
# This guard is ESSENTIAL for multiprocessing in notebooks/scripts
if __name__ == "__main__":
# --- Configure Logging ---
log_file = 'vns_run.log'
logging.basicConfig(
level=logging.INFO,  # Log INFO level and above (INFO, WARNING, ERROR, CRITICAL)
format='%(asctime)s - %(levelname)s - %(processName)s - %(message)s',
filename=log_file,
filemode='w'  # 'w' clears the file each time, 'a' appends
)
# Optional: Add a handler to also print logs to console
# console_handler = logging.StreamHandler(sys.stdout)
# console_handler.setLevel(logging.INFO) # Or DEBUG
# console_handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s')) # Simpler format for console
# logging.getLogger().addHandler(console_handler)
# --- Handle potential import error from earlier ---
try:
# Check if the required functions were actually imported or defined
calculate_objective_serv_time_lookup
powerset
get_neighborhood_for_parallel
logging.info("Successfully imported or defined helper functions.")
except NameError:
logging.error("Essential helper functions are missing. Exiting.")
sys.exit("Exiting due to missing helper functions. Check imports or definitions.")
# --- Crucial for 'spawn' start method (Windows, macOS, Quarto/Jupyter) ---
mp.freeze_support() # Needed for frozen executables, good practice
logging.info("Script execution started.")
print("Starting VNS from Quarto...")
start_time = time.time()
# Call the imported function
best_solution, best_cost = variable_neighborhood_search(
x_init=x_star,
d=d,
convolutions=convolutions,
w=w,
v_star=v_star,
echo=True # Set to False for less output
)
end_time = time.time()
print("\n--- VNS Result ---")
print(f"Best solution found: {best_solution}")
print(f"Best cost found: {best_cost:.4f}")
print(f"Total execution time: {end_time - start_time:.2f} seconds")
reticulate::repl_python()
reticulate::repl_python()
