import numpy as np
import torch
import time
from botorch.models import SingleTaskGP
from botorch.fit import fit_gpytorch_mll
# CHANGE: Import LogExpectedImprovement instead of ExpectedImprovement
from botorch.acquisition import LogExpectedImprovement
from botorch.exceptions import BadInitialCandidatesWarning
from gpytorch.mlls import ExactMarginalLogLikelihood
from gpytorch.kernels import MaternKernel, ScaleKernel
from gpytorch.priors import GammaPrior
# ADD: Import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
import warnings
warnings.filterwarnings("ignore", category=BadInitialCandidatesWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
# Filter the specific NumericsWarning about LogEI recommendation if desired,
# but it's good practice to switch as recommended.
# warnings.filterwarnings("ignore", category=NumericsWarning)
# --- Problem Definition ---
# Fixed Data (Use your actual data)
X = np.array([2, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 4])
T = X.shape[0]
# Placeholder V_star (Replace with your actual V_star: T rows, T columns)
V_star = np.zeros((T, T))
for i in range(T):
if i < T: V_star[i, i] = 1
if i >= 0 and i < T: V_star[i, i-1] = -1
print(V_star)
# Objective Function Calculation
LARGE_PENALTY = 1e10 # Penalty for infeasible solutions
def evaluate_objective(U_np, X_vec, V_star_mat):
"""
Target function: Evaluates objective for a single binary numpy array U.
Returns a float.
"""
if not isinstance(U_np, np.ndarray):
raise TypeError("Input U must be a numpy array")
if U_np.ndim != 1:
raise ValueError("Input U must be 1-dimensional")
if U_np.shape[0] != V_star_mat.shape[0]:
raise ValueError(f"Dimension mismatch: U length {U_np.shape[0]} != V* rows {V_star_mat.shape[0]}.")
if X_vec.shape[0] != V_star_mat.shape[1]:
raise ValueError("Dimension mismatch: X length must match V* columns.")
if not np.all((U_np == 0) | (U_np == 1)):
raise ValueError("Input U must be binary (0s and 1s).")
V_sum = np.sum(V_star_mat[U_np == 1, :], axis=0)
Y = X_vec + V_sum
if np.all(Y >= 0):
objective_value = np.sum(Y**2)
return objective_value
else:
return LARGE_PENALTY
# --- HED Implementation ---
def hamming_distance(u1, u2):
return np.sum(u1 != u2)
def generate_diverse_random_dictionary(T, m):
dictionary_A = np.zeros((m, T), dtype=int)
for i in range(m):
theta = np.random.uniform(0, 1)
row = (np.random.rand(T) < theta).astype(int)
dictionary_A[i, :] = row
return dictionary_A
def embed_vector(U_np, dictionary_A):
m = dictionary_A.shape[0]
embedding_phi = np.zeros(m, dtype=int)
for i in range(m):
embedding_phi[i] = hamming_distance(U_np, dictionary_A[i, :])
return embedding_phi
def embed_batch(U_batch_tensor, dictionary_A):
U_batch_np = U_batch_tensor.cpu().numpy().astype(int)
m = dictionary_A.shape[0]
batch_size = U_batch_np.shape[0] if U_batch_np.ndim > 1 else 1 # Handle single vector case
embeddings_np = np.zeros((batch_size, m), dtype=int)
if U_batch_np.ndim == 1: # Single vector case
embeddings_np[0, :] = embed_vector(U_batch_np, dictionary_A)
else: # Batch of vectors
for j in range(batch_size):
embeddings_np[j, :] = embed_vector(U_batch_np[j, :], dictionary_A)
return torch.from_numpy(embeddings_np).to(dtype=torch.float64)
# --- BO Helper Functions ---
def get_fitted_model(train_X_embedded_scaled, train_Y, m): # Takes scaled input
"""Fits a SingleTaskGP model to the SCALED embedded data."""
if train_Y.ndim == 1:
train_Y = train_Y.unsqueeze(-1)
kernel = MaternKernel(
nu=2.5,
ard_num_dims=m, # ARD on the embedding dimension
batch_shape=torch.Size(),
lengthscale_prior=GammaPrior(3.0, 6.0),
)
covar_module = ScaleKernel(
kernel,
batch_shape=torch.Size(),
outputscale_prior=GammaPrior(2.0, 0.15),
)
model = SingleTaskGP(train_X=train_X_embedded_scaled, train_Y=train_Y, covar_module=covar_module)
mll = ExactMarginalLogLikelihood(model.likelihood, model)
fit_gpytorch_mll(mll)
return model
# MODIFIED: Accepts the scaler
def optimize_acqf_discrete_via_embedding(acq_function, dictionary_A, scaler, T, q, num_candidates):
"""
Optimizes acquisition function by sampling random binary candidates,
embedding, SCALING, and evaluating acqf. Selects the top q candidates.
Returns candidates as a numpy array (q x T).
"""
m = dictionary_A.shape[0]
candidate_u_vectors_np = np.random.randint(0, 2, size=(num_candidates, T))
# Ensure unique candidates if desired (optional, adds overhead)
# candidate_u_vectors_np = np.unique(candidate_u_vectors_np, axis=0)
# num_candidates = candidate_u_vectors_np.shape[0] # Update count if unique applied
candidate_u_vectors_torch = torch.from_numpy(candidate_u_vectors_np)
with torch.no_grad():
# Embed the candidates
embedded_candidates = embed_batch(candidate_u_vectors_torch, dictionary_A)
# SCALE the embedded candidates using the *fitted* scaler
embedded_candidates_np = embedded_candidates.cpu().numpy()
# Handle potential warning if scaler expects float64
embedded_candidates_np = embedded_candidates_np.astype(np.float64)
embedded_candidates_scaled_np = scaler.transform(embedded_candidates_np)
embedded_candidates_scaled = torch.from_numpy(embedded_candidates_scaled_np).to(dtype=torch.float64, device=acq_function.model.train_inputs[0].device) # Ensure same device
# Evaluate acquisition function on SCALED embedded candidates
acq_values = acq_function(embedded_candidates_scaled.unsqueeze(1)) # Add batch dim [num_cand, 1, m]
top_indices = torch.topk(acq_values, k=q).indices.cpu().numpy()
return candidate_u_vectors_np[top_indices.flatten(), :] # Ensure indices are flat
# --- BO Loop ---
dtype = torch.float64
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
# Parameters
N_INITIAL = 20
N_ITERATIONS = 40
BATCH_SIZE_q = 1
NUM_CANDIDATES_Acqf = 1024 # Might need more for higher T
m = 256
# Store evaluated points
evaluated_U_np = []
evaluated_f_vals = []
train_Y_torch = []
# 1. Initialization
print(f"Generating {N_INITIAL} initial points...")
for _ in range(N_INITIAL):
U_init = np.random.randint(0, 2, size=T)
# Ensure initial points are unique if possible (optional)
while any(np.array_equal(U_init, u) for u in evaluated_U_np):
U_init = np.random.randint(0, 2, size=T)
f_val = evaluate_objective(U_init, X, V_star)
evaluated_U_np.append(U_init)
evaluated_f_vals.append(f_val)
train_Y_torch.append(torch.tensor([-f_val], dtype=dtype, device=device))
train_Y = torch.cat(train_Y_torch).unsqueeze(-1)
best_obj_so_far = min(evaluated_f_vals) if evaluated_f_vals else float('inf')
print(f"Initial best objective value: {best_obj_so_far}")
# 2. BO Iterations
for iteration in range(N_ITERATIONS):
start_time = time.time()
print(f"\n--- Iteration {iteration + 1}/{N_ITERATIONS} ---")
# a. Generate dictionary
current_dictionary_A = generate_diverse_random_dictionary(T, m)
# b. Embed ALL evaluated U vectors so far
evaluated_U_tensor = torch.from_numpy(np.array(evaluated_U_np))
embedded_train_X = embed_batch(evaluated_U_tensor, current_dictionary_A).to(device)
# ADD: Scale the embedded training data
scaler = MinMaxScaler()
embedded_train_X_np = embedded_train_X.cpu().numpy().astype(np.float64) # Convert for scaler
# Fit scaler only if there's data
if embedded_train_X_np.shape[0] > 0:
scaler.fit(embedded_train_X_np)
embedded_train_X_scaled_np = scaler.transform(embedded_train_X_np)
embedded_train_X_scaled = torch.from_numpy(embedded_train_X_scaled_np).to(dtype=dtype, device=device)
else:
# Handle case with no data yet (shouldn't happen after init)
embedded_train_X_scaled = embedded_train_X
# c. Fit GP Model using SCALED data
print("Fitting GP model...")
# Ensure train_Y is on the same device as scaled X
train_Y = train_Y.to(device)
gp_model = get_fitted_model(embedded_train_X_scaled, train_Y, m)
print("GP model fitted.")
# d. Define Acquisition Function (CHANGE: Use LogExpectedImprovement)
current_best_f_for_acqf = train_Y.max().item() if train_Y.numel() > 0 else -float('inf') # Handle empty train_Y
# Prevent potential issues if all initial points were infeasible (very large negative best_f)
if current_best_f_for_acqf <= -LARGE_PENALTY / 2:
print("Warning: Current best value is very low (likely from penalties). Acqf might behave unexpectedly.")
# Optionally clamp best_f or use a different strategy if all points are infeasible
acq_function = LogExpectedImprovement( # CHANGED HERE
model=gp_model,
best_f=current_best_f_for_acqf,
maximize=True
)
# e. Optimize Acquisition Function (using the fitted scaler)
print("Optimizing acquisition function...")
next_U_candidates_np = optimize_acqf_discrete_via_embedding(
acq_function=acq_function,
dictionary_A=current_dictionary_A,
scaler=scaler, # Pass the fitted scaler
T=T,
q=BATCH_SIZE_q,
num_candidates=NUM_CANDIDATES_Acqf
)
print(f"Selected {next_U_candidates_np.shape[0]} candidate(s).")
# f. Evaluate Objective for the selected candidate(s)
next_Y_vals_torch = []
newly_evaluated_U = []
newly_evaluated_f = []
for i in range(next_U_candidates_np.shape[0]):
next_U = next_U_candidates_np[i, :]
# Check if this candidate was already evaluated
already_evaluated = any(np.array_equal(next_U, u) for u in evaluated_U_np)
if already_evaluated:
print(f"  Candidate {i} was already evaluated. Skipping re-evaluation.")
# Optionally, could try to generate a different candidate here
continue # Skip to next candidate if already evaluated
next_f = evaluate_objective(next_U, X, V_star)
print(f"  Candidate {i}: Obj = {next_f:.4f}")
# g. Augment Dataset (only add if not already evaluated)
newly_evaluated_U.append(next_U)
newly_evaluated_f.append(next_f)
next_Y_vals_torch.append(torch.tensor([-next_f], dtype=dtype, device=device))
# Update overall best
if next_f < best_obj_so_far:
best_obj_so_far = next_f
# Add newly evaluated points to main lists
evaluated_U_np.extend(newly_evaluated_U)
evaluated_f_vals.extend(newly_evaluated_f)
# Concatenate new observations for the next iteration's GP fit (if any new points were added)
if next_Y_vals_torch:
train_Y = torch.cat([train_Y, torch.cat(next_Y_vals_torch).unsqueeze(-1)])
iter_time = time.time() - start_time
print(f"Best objective value found so far: {best_obj_so_far:.4f}")
print(f"Total points evaluated: {len(evaluated_f_vals)}")
print(f"Iteration {iteration + 1} completed in {iter_time:.2f} seconds.")
# --- Results ---
print("\n--- Optimization Finished ---")
if not evaluated_f_vals:
print("No points were successfully evaluated.")
else:
final_best_idx = np.argmin(evaluated_f_vals)
final_best_U = evaluated_U_np[final_best_idx]
final_best_f = evaluated_f_vals[final_best_idx]
print(f"Total evaluations: {len(evaluated_f_vals)}")
print(f"Best Objective Value Found: {final_best_f}")
print(f"Best U vector Found: {final_best_U}")
# Verification
V_sum_best = np.sum(V_star[final_best_U == 1, :], axis=0)
Y_best = X + V_sum_best
is_feasible = np.all(Y_best >= 0)
print(f"Is the best solution feasible? {is_feasible}")
if is_feasible:
print(f"Resulting Y vector for best U: {Y_best}")
print(f"Sum of squares (recalculated): {np.sum(Y_best**2):.4f}")
elif final_best_f < LARGE_PENALTY:
print(f"Warning: Best objective ({final_best_f}) is not the penalty, but feasibility check failed.")
else:
print("Best solution found corresponds to an infeasible penalty value.")
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
