# Import necessary river components
import time
import math
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import pickle
import random
from scipy.optimize import minimize
from river import tree, metrics, evaluate, stream, drift
# Instantiate the Hoeffding Tree Classifier
# These parameters are illustrative; optimal values depend on the specific data stream
ht_model = tree.HoeffdingTreeClassifier(
grace_period=200,           # Number of samples to observe before considering a split
split_criterion='info_gain',# Criterion for choosing splits (Information Gain)
delta=1e-7,                 # Hoeffding bound confidence parameter (1 - delta)
nominal_attributes=None     # Specify if any features are nominal (categorical)
# Other parameters like split_confidence, tie_threshold can also be set
)
print("Hoeffding Tree Classifier instantiated.")
arf_model = tree.HoeffdingAdaptiveTreeClassifierhat_model = tree.HoeffdingAdaptiveTreeClassifier(
grace_period=200,         # Hoeffding Tree: Wait for 100 samples before first split check
delta=1e-7,               # Hoeffding Tree: Confidence level for Hoeffding bound
drift_detector=drift.ADWIN(delta=0.001), # Use ADWIN detector with its own delta (sensitivity)
switch_significance=0.05, # Significance level (beta) to switch trees after drift
# Other relevant HT parameters (e.g., split_criterion, max_depth) can be set here
seed=42                   # For reproducibility
)
print("Adaptive Random Forest Classifier instantiated.")
from functions import compute_convolutions, bailey_welch_schedule
N = 22 # Number of patients
T = 20 # Number of intervals
d = 5 # Length of each interval
max_s = 20 # Maximum service time
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.1 # Weight for the waiting time in objective function
l = 10
num_schedules = 60000 # Number of schedules to sample
# Create service time distribution
def generate_weighted_list(max_s, l, i):
"""
Generates a service time probability distribution using optimization.
This function creates a discrete probability distribution over T possible
service times (from 1 to T). It uses optimization (SLSQP) to find a
distribution whose weighted average service time is as close as possible
to a target value 'l', subject to the constraint that the probabilities
sum to 1 and each probability is between 0 and 1.
After finding the distribution, it sorts the probabilities: the first 'i'
probabilities (corresponding to service times 1 to i) are sorted in
ascending order, and the remaining probabilities (service times i+1 to T)
are sorted in descending order.
Note:
- This function relies on a globally defined integer 'T', representing
the maximum service time considered (or number of probability bins).
- The parameter 'max_s' is accepted but not used directly within this
function's optimization or sorting logic as shown. It might be
related to how 'T' is determined externally.
- Requires NumPy and SciPy libraries (specifically scipy.optimize.minimize).
Args:
max_s (any): Maximum service time parameter (currently unused in the
provided function body's core logic).
l (float): The target weighted average service time for the distribution.
i (int): The index determining the sorting split point. Probabilities
for service times 1 to 'i' are sorted ascendingly, and
probabilities for service times 'i+1' to 'T' are sorted
descendingly. Must be between 1 and T-1 for meaningful sorting.
Returns:
numpy.ndarray: An array of size T+1. The first element (index 0) is 0.
Elements from index 1 to T represent the calculated
and sorted probability distribution, summing to 1.
Returns None if optimization fails.
"""
# Initialize an array of T+1 values, starting with zero
# Index 0 is unused for probability, indices 1 to T hold the distribution
values = np.zeros(T + 1)
# --- Inner helper function for optimization ---
def objective(x):
"""Objective function: Squared difference between weighted average and target l."""
# Calculate weighted average: sum(index * probability) / sum(probability)
# Since sum(probability) is constrained to 1, it simplifies.
weighted_avg = np.dot(np.arange(1, T + 1), x) # Corresponds to sum(k * P(ServiceTime=k))
return (weighted_avg - l) ** 2
# --- Constraints for optimization ---
# Constraint 1: The sum of the probabilities (x[0] to x[T-1]) must be 1
constraints = ({
'type': 'eq',
'fun': lambda x: np.sum(x) - 1
})
# Bounds: Each probability value x[k] must be between 0 and 1
# Creates a list of T tuples, e.g., [(0, 1), (0, 1), ..., (0, 1)]
bounds = [(0, 1)] * T
# Initial guess: Use Dirichlet distribution to get a random distribution that sums to 1
# Provides a starting point for the optimizer. np.ones(T) gives equal weights initially.
initial_guess = np.random.dirichlet(np.ones(T))
# --- Perform Optimization ---
# Minimize the objective function subject to the sum and bounds constraints
# using the Sequential Least Squares Programming (SLSQP) method.
result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)
# Check if optimization was successful
if not result.success:
print(f"Warning: Optimization failed! Message: {result.message}")
# Handle failure case, e.g., return None or raise an error
return None # Or potentially return a default distribution
# Assign the optimized probabilities (result.x) to the correct slice of the values array
# result.x contains the T probabilities for service times 1 to T.
values[1:] = result.x
# --- Reorder the values based on the index 'i' ---
# Ensure 'i' is within a valid range for slicing and sorting
if not (0 < i < T):
print(f"Warning: Index 'i' ({i}) is outside the valid range (1 to {T-1}). Sorting might be trivial.")
# Adjust i or handle as an error depending on requirements
i = max(1, min(i, T - 1)) # Clamp i to a safe range for demonstration
# Sort the first 'i' probabilities (indices 1 to i) in ascending order
first_part = np.sort(values[1:i+1])
# Sort the remaining 'T-i' probabilities (indices i+1 to T) in descending order
second_part = np.sort(values[i+1:])[::-1] # [::-1] reverses the sorted array
# Combine the sorted parts back into the 'values' array
values[1:i+1] = first_part
values[i+1:] = second_part
# Return the final array with the sorted probability distribution
return values
i = 5  # First 5 highest values in ascending order, rest in descending order
s = generate_weighted_list(max_s, l, i)
print(s)
print("Sum:", np.sum(s[1:]))  # This should be 1
print("Weighted service time:", np.dot(np.arange(len(s)), s))  # This should be close to l
initial_x = bailey_welch_schedule(T, d, N, s)
print(f"Initial schedule: {initial_x}")
convolutions = compute_convolutions(s, N, q)
from functions import get_v_star, get_neighborhood
def sample_neighbors_list(x: list[int], v_star: np.ndarray, all = True) -> (list[int], list[int]):
"""
Create a set of pairs of schedules that are from the same neighborhood.
Parameters:
x (list[int]): A list of integers with |s| = T and sum N.
v_star (np.ndarray): Precomputed vectors V* of length T.
Returns:
tuple(list[int], list[int]): A pair of schedules.
"""
T = len(x)
# Precompute binomial coefficients (weights for random.choices)
binom_coeff = [math.comb(T, i) for i in range(1, T)]
# Choose a random value of i with the corresponding probability
i = random.choices(range(1, T), weights=binom_coeff)[0]
# Instead of generating the full list of combinations, sample one directly
j = random.sample(range(T), i)
x_p = x.copy()
for k in j:
x_temp = np.array(x_p) + v_star[k]
x_temp = x_temp.astype(int)
if np.all(x_temp >= 0):
x_p = x_temp.astype(int).tolist()
if all:
diff = [int(x - y) for x, y in zip(x, x_p)]
return x, x_p, diff
else:
return x_p
start = time.time()
v_star = get_v_star(T)
# Sample a set of schedules from the neighborhood of the initial schedule
neighbors_selection = [sample_neighbors_list(initial_x, v_star, all = False) for i in range(num_schedules)] # This can be done in parallel to improve speed
print(len(neighbors_selection))
end = time.time()
# For the sampled schedules, create the neighbors
neighbors_list = [sample_neighbors_list(schedule, v_star) for schedule in neighbors_selection]
# Randomly switch the order of the neighbors
# neighbors_list = [neighbor if random.random() < 0.5 else neighbor[::-1] for neighbor in neighbors_list]
end = time.time()
h = random.choices(range(num_schedules), k=7)
print(f"Sampled schedules: {h}")
for i in h:
original_schedule = neighbors_list[i][0]
neighbor_schedule = neighbors_list[i][1]
difference =neighbors_list[i][2]
print(f"Neighbors\n{original_schedule}\n{neighbor_schedule}\n{difference}")
training_set_feat_time = end - start
print(f"\nProcessing time: {training_set_feat_time} seconds\n")
from functions import calculate_objective_serv_time_lookup
objectives_schedule_1 = [
w * result[0] + (1 - w) * result[1]
for neighbor in neighbors_list
for result in [calculate_objective_serv_time_lookup(neighbor[0], d, convolutions)]
]
start = time.time()
objectives_schedule_2 = [
w * result[0] + (1 - w) * result[1]
for neighbor in neighbors_list
for result in [calculate_objective_serv_time_lookup(neighbor[1], d, convolutions)]
]
end = time.time()
training_set_lab_time = end - start
objectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]
rankings = np.argmin(objectives, axis=1).tolist()
for i in range(5):
print(f"Objectives: {objectives[i]}, Ranking: {rankings[i]}")
print(f"\nProcessing time: {training_set_lab_time} seconds\n")
# Step 1: Flatten the objectives into a 1D array
flattened_data = [value for sublist in objectives for value in sublist]
# Step 2: Find the index of the minimum value
min_index = np.argmin(flattened_data)
# Step 3: Convert that index back to the original 2D structure
row_index = min_index // 2  # Assuming each inner list has 2 values
col_index = min_index % 2
print(f"The minimum objective value is at index [{row_index}][{col_index}].\nThis is schedule: {neighbors_list[row_index][col_index]} with objective value {objectives[row_index][col_index]}.")
print(f"\nAverage ranking: {np.mean(rankings)}\n")
def prepare_features(schedule1, diff):
"""
Concatenates two schedule vectors and converts them into a
dictionary format suitable for river models.
"""
# Ensure schedules are numpy arrays for easier concatenation if needed
s1 = np.asarray(schedule1)
diff = np.asarray(diff)
# Concatenate the two schedule vectors
concatenated_schedule = np.concatenate((s1, diff))
# Create feature dictionary using indices as keys
# Example: {0: val_s1_0, 1: val_s1_1,..., N: val_s2_0,...}
features = {i: float(val) for i, val in enumerate(concatenated_schedule)}
return features
print("\nStarting incremental training simulation...")
# Use river's stream utilities for evaluation (optional but good practice)
metric = metrics.Accuracy()
for i, (sched_pair, rank) in enumerate(zip(neighbors_list, rankings)):
schedule1 = sched_pair[0]
diff = sched_pair[2]
features = prepare_features(schedule1, diff)
target = rank
# Optional: Make a prediction before learning (for prequential evaluation)
y_pred = ht_model.predict_one(x=features)
if y_pred is not None: # Model might not be ready to predict initially
metric.update(target, y_pred)
# Learn from the current instance (pair + rank)
ht_model.learn_one(x=features, y=target)
if (i + 1) % 1000 == 0: # Print progress for every 100th pair in this small example
print(f"Processed pair {i+1}. Current Accuracy: {metric.get():.4f}") # If using prequential evaluation
print(f"Processed pair {i+1}. Model updated.")
print("Incremental training simulation finished.")
print(f"Final Accuracy (Prequential): {metric.get():.4f}") # If using prequential evaluation
metric_arf = metrics.Accuracy()
for i, (sched_pair, rank) in enumerate(zip(neighbors_list, rankings)):
schedule1 = sched_pair[0]
diff = sched_pair[2]
features = prepare_features(schedule1, diff)
target = rank
# Optional: Make a prediction before learning (for prequential evaluation)
y_pred = arf_model.predict_one(x=features)
if y_pred is not None: # Model might not be ready to predict initially
metric_arf.update(target, y_pred)
# Learn from the current instance (pair + rank)
arf_model.learn_one(x=features, y=target)
if (i + 1) % 1000 == 0: # Print progress for every 100th pair in this small example
print(f"Processed pair {i+1}. Current Accuracy: {metric_arf.get():.4f}") # If using prequential evaluation
print(f"Processed pair {i+1}. Model updated.")
print("Incremental training simulation finished.")
print(f"Final Accuracy (Prequential): {metric_arf.get():.4f}") # If using prequential evaluation
