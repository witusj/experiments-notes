---
title: "Large instance local search with trained XGBoost regressor model"
jupyter: python3
---

## Objective

Test the working and performance of a [previously trained](xgboost-pairwise-ranking-large-w-bailey-welch.qmd) XGBoost Ranking model in a local search application.

## Background

In previous experiments, we trained an XGBoost Classifier model to predict the objective values of neighboring schedules. In this experiment, we will use the trained models to perform a local search to find the best schedule.

## Hypothesis

The XGBoost Classifier model will be able to efficiently guide the local search algorithm to find a schedule with a lower objective value than the initial schedule.

## Methodology

### Tools and Materials

```{python}
import numpy as np
import json
from itertools import chain, combinations
import sys
from math import comb  # Available in Python 3.8 and later
import xgboost as xgb
import pickle
from typing import List, Tuple, Dict, Iterable, TypeVar, Union, Any
```

### Load Parameters

```{python}
N = 22 # Number of patients
T = 20 # Number of intervals
l = 10

file_path_parameters = f"datasets/parameters_{N}_{T}_{l}.pkl"
# Load the data from the pickle file
with open(file_path_parameters, 'rb') as f:
    data_params = pickle.load(f)

N = data_params['N'] # Number of patients
T = data_params['T'] # Number of intervals
d = data_params['d'] # Length of each interval
max_s = data_params['max_s'] # Maximum service time
q = data_params['q'] # Probability of a scheduled patient not showing up
w = data_params['w'] # Weight for the waiting time in objective function
l = data_params['l']
  
num_schedules = data_params['num_schedules'] # Number of schedules to sample
convolutions = data_params['convolutions']
print(f"Parameters loaded: N={N}, T={T}, l={l}, d={d}, max_s={max_s}, q={q}, w={w}, num_schedules={num_schedules}")
```

### Experimental Design

We will use the trained XGBoost Classifier model to guide a local search algorithm to find the best schedule. The local search algorithm will start with an initial schedule and iteratively explore the neighborhood of the current schedule to find a better one. As an initial schedule, we will use the schedule with the lowest objective value from the training dataset that was used to train the XGBoost Classifier model.

### Variables

-   **Independent Variables**:
    -   Initial schedule, trained XGBoost Classifier
-   **Dependent Variables**:
    -   Speed, accuracy, and convergence of the local search algorithm.

### Data Collection

We will use the training dataset to initialize the local search algorithm.

### Sample Size and Selection

### Experimental Procedure

![Local search algorithm](images/local_search_algorithm.png){#fig-local-search-algorithm}

## Results

### Load the initial best schedule.

Start with the best solution found so far $\{x^*, C(x^*)\}$ from the training set.

```{python}
# Load the best solution from the training dataset
file_path_schedules = f"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl"
# Load the data from the pickle file
with open(file_path_schedules, 'rb') as f:
    data_sch = pickle.load(f)
    
print(f"The data has following keys: {[key for key in data_sch.keys()]}")

# Step 1: Flatten the objectives into a 1D array
flattened_data = [value for sublist in data_sch['objectives'] for value in sublist]

# Step 2: Find the index of the minimum value
min_index = np.argmin(flattened_data)

# Step 3: Convert that index back to the original 2D structure
row_index = min_index // 2  # Assuming each inner list has 2 values
col_index = min_index % 2

print(f"The minimum objective value is at index [{row_index}][{col_index}].\nThis is schedule: {data_sch['neighbors_list'][row_index][col_index]} with objective value {data_sch['objectives'][row_index][col_index]}.")

# Set the initial schedule to the best solution from the training dataset
initial_schedule = data_sch['neighbors_list'][row_index][col_index]
```

### Generate the neighborhood of $x^*$.

#### Define $V^*$.

Define the vectors $V^*$ as follows:

$$
\left\{
\begin{array}{c}
\vec{v_1}, \\
\vec{v_2}, \\
\vec{v_3}, \\
\vdots \\
\vec{v_{T-1}}, \\
\vec{v_T} \\
\end{array}
\right\} = 
\left\{
\begin{array}{c}
(-1, 0,...., 0, 1), \\
(1, -1, 0,...., 0), \\
(0, 1, -1,...., 0), \\
\vdots \\
(0,...., 1, -1, 0), \\
(0,...., 0, 1, -1) \\
\end{array}
\right\}
$$

#### Define $U_t$.

Define $U_t$ as the set of all possible subsets of $V^*$ such that each subset contains exactly $t$ elements, i.e.,

$$
U_t = \{ S \subsetneq V^* \mid |S| = t \}, \quad t \in \{1, 2, \dots, T\}.
$$

```{python}
from functions import get_v_star

def powerset(iterable, size=1):
    "powerset([1,2,3], 2) --> (1,2) (1,3) (2,3)"
    return [[i for i in item] for item in combinations(iterable, size)]
  
x = initial_schedule

# Generate a matrix 'v_star' using the 'get_v_star' function
v_star = get_v_star(T)

# Generate all possible non-empty subsets (powerset) of the set {0, 1, 2, ..., t-1}
# 'ids' will be a list of tuples, where each tuple is a subset of indices
size = 2
ids = powerset(range(T), size)
len(ids)
ids[:T]
```

#### Define the neighborhood of $x$

Define the neighborhood of $x$ as all vectors of the form $x + u_{tk}$ with $\forall \, u_{tk} \in U_t$.

```{python}
from functions import get_neighborhood
test_nh = get_neighborhood(x, v_star, ids)
print(f"All neighborhoods with {size} patients switched:\n x = {np.array(x)}: \n {test_nh}")
```

### Local search algorithm

```{python}
def local_search_predicted(x: List[int],w: float, v_star: np.ndarray, clf: xgb.XGBClassifier, size: int = 2, restarts: int = 3) -> Tuple[np.ndarray, int]:
    """
    Performs a local search around a given point in the feature space
    and predicts the class label using an XGBClassifier model.

    Args:
        x (List[int]): The starting point for the local search, represented as a list of integers.
        v_star (np.ndarray): The current best solution (e.g., a NumPy array representing a feature vector).
        clf (XGBClassifier): An XGBoost Classifier model that will be used for prediction.
        size (int, optional): The size of the neighborhood to explore around the starting point. Defaults to 2.
        restarts (int, optional): The number of restarts for the local search. Defaults to 3. This prevents infinite loops.

    Returns:
        Tuple[np.ndarray, int]: A tuple containing the best neighbor found (as a NumPy array)
                                 and its predicted class label (as an integer).
    """
    
    # Outer loop for the number of patients to switch
    max_restarts = restarts  # Or some other reasonable limit to prevent infinite loops when searching
    restart_count = 0
    x_star = x
    objectives_star = calculate_objective_serv_time_lookup(x_star, d, convolutions)
    cost_star = w * objectives_star[0] + (1 - w) * objectives_star[1]
    t = 1
    w = 0.1
    while t < size and restart_count < max_restarts:
        print(f'Running local search {t}')
        neighbor_temp = []
        ids_gen = powerset(range(T), t)
        neighborhood = get_neighborhood(x_star, v_star, ids_gen)
        print(f"Switching {t} patient(s). Size of neighborhood: {len(list(ids_gen))}")
        found_better_solution = False
        for neighbor in neighborhood:
            # This is just for debugging. The actual new method for local search will not use this, because the whole purpose is to improve computational efficiency.
            objectives_tuple = calculate_objective_serv_time_lookup(neighbor, d, convolutions)
            costs_list = [cost_star, w * objectives_tuple[0] + (1 - w) * objectives_tuple[1]]
            # Append the neighbor to the list of neighbors
            neighbor_temp.append(neighbor)
            schedule_pairs = x_star + neighbor.tolist()
            print(f"Schedule pairs: {schedule_pairs}")
            print(f"Costs: {costs_list}")
            # Predict the rank and ambiguousness using the trained model
            rank = clf.predict([schedule_pairs])
            ambiguousness = clf.predict_proba([schedule_pairs])
            print(f"Predicted rank: {rank}, ambiguousness: {ambiguousness}")
            if rank[0] == 1:
                # If the predicted rank is 1, it means the neighbor seems better
                # Check actual rank by calculating the objective value for neighbor and comparing it with x_star
                true_objectives = calculate_objective_serv_time_lookup(neighbor, d, convolutions)
                print(f"True objectives: {true_objectives}")
                true_cost = w * true_objectives[0] + (1 - w) * true_objectives[1]
                if true_cost > cost_star:
                    # If the true cost is greater, the model misranked and needs retraining
                    print(f"Model needs retraining. True cost: {true_cost}, Current cost: {cost_star}")
                    # Update the training data
                    true_cost_list = [
                        w * result[0] + (1 - w) * result[1]
                        for neighbor in neighbors_temp
                        for result in [calculate_objective_serv_time_lookup(neighbor, d, convolutions)]
                    ]
                    y_update = [0 if test_cost < cost_star else 1 for test_cost in true_cost_list]
                    X_update = []
                    for neighbor in neighbors_temp:
                        X_new.append(x_star + neighbor.tolist())
                    
                    # Increase n_estimators
                    clf.n_estimators += 5
                    # Fit with previous model
                    
                    print(f"Retraining on {len(neighbors_temp)} new schedules")
                    clf.fit(X_update, y_update, xgb_model=clf)
                x_star = neighbor.tolist()
                print(f"Found better solution: {x_star}")
                found_better_solution = True
                break
        if found_better_solution:
            t = 1
            restart_count += 1
            print(f"Restarting local search. Restart count: {restart_count}")
        else:
            t += 1
    return x_star
```


```{python}
def local_search_predict_update(
    x: List[int],
    w: float,
    v_star: np.ndarray,
    clf: xgb.XGBClassifier,
    obj_func_params: Dict[str, Any],
    size: int = 2,
    restarts: int = 3,
    check_proba_threshold: float = 0.7 # Check P(0) against this threshold
) -> Tuple[List[int], xgb.XGBClassifier]:
    """
    Performs local search guided by an XGBClassifier, minimizing expensive
    objective calls. Verifies prediction=0 if P(class=0) is below threshold.
    Updates the classifier incrementally when mispredictions (P=1/A=0) occur.
    T is inferred from len(x).

    Args:
        x (List[int]): Starting point (list representing a schedule/solution).
        w (float): Weight for combining objectives into a single cost.
        v_star (np.ndarray): Current best overall solution (usage depends on get_neighborhood).
        clf (xgb.XGBClassifier): Pre-trained XGBoost Classifier to be updated.
        obj_func_params (Dict[str, Any]): Dictionary containing parameters needed by
                                         calculate_objective_serv_time_lookup.
        size (int, optional): Max neighborhood size. Defaults to 2.
        restarts (int, optional): Max restarts if an improving move is found. Defaults to 3.
        check_proba_threshold (float, optional): Probability threshold for P(class=0).
                                                 If prediction is 0 and
                                                 P(class=0) < threshold, the prediction
                                                 will be verified. Defaults to 0.7.

    Returns:
        Tuple[List[int], xgb.XGBClassifier]: Best solution found and updated classifier.
    """
    if not isinstance(getattr(clf, 'n_estimators', None), int):
        raise ValueError(f"clf.n_estimators must be an integer, but found {getattr(clf, 'n_estimators', None)}")

    x_star = list(x) # Work with a copy
    T = len(x_star) # Infer T from the length

    if T <= 0:
        raise ValueError("Input schedule x cannot be empty (length must be positive).")

    restart_count = 0
    t = 1 # Start with neighborhood size 1

    # Calculate initial cost
    try:
        print("Calculating initial cost...")
        objectives_star = calculate_objective_serv_time_lookup(x_star, **obj_func_params)
        cost_star = w * objectives_star[0] + (1 - w) * objectives_star[1]
        print(f"Initial solution cost: {cost_star:.4f}")
    except Exception as e:
        print(f"Error calculating initial cost: {e}")
        return x_star, clf

    while t <= size and restart_count < restarts:
        print(f"\n--- Running local search level t={t} (Restart {restart_count}/{restarts}) ---")

        ids_gen_iterable = powerset(range(T), t)
        neighborhood_iter = get_neighborhood(x_star, v_star, ids_gen_iterable)

        found_better_solution_at_level_t = False
        neighbors_at_level_t: List[List[int]] = [] # Store neighbors explored at this level
        neighbors_processed_count = 0

        for neighbor_np in neighborhood_iter:
            neighbors_processed_count +=1
            neighbor = neighbor_np.tolist()
            neighbors_at_level_t.append(neighbor)

            # --- Check for consistent feature length before prediction ---
            expected_feature_len = getattr(clf, 'n_features_in_', None)
            current_feature_len = len(x_star) + len(neighbor)
            if expected_feature_len is not None and current_feature_len != expected_feature_len:
                 print(f"  Warn: Feature length mismatch for neighbor {neighbors_processed_count}. Expected {expected_feature_len}, got {current_feature_len}. Skipping.")
                 continue

            feature_pair = x_star + neighbor

            # 1. Predict using the CHEAP classifier (get both class and probability)
            try:
                prediction = clf.predict([feature_pair])[0]
                proba = clf.predict_proba([feature_pair])[0] # Get probabilities for [class 0, class 1]
                proba_class_0 = proba[0] # Probability of class 0 (not better)
            except Exception as e:
                print(f"  Warn: Error predicting for neighbor {neighbors_processed_count}: {e}. Assuming prediction=0.")
                prediction = 0
                proba_class_0 = 1.0 # Assume high confidence in prediction=0 if error occurs

            print(f"  Neighbor {neighbors_processed_count}: Predicted={prediction} (P(0)={proba_class_0:.3f}) ", end="")

            # 2. Decide whether to perform expensive check
            perform_expensive_check = False
            check_reason = ""

            if prediction == 1:
                # Always check if model predicts improvement
                perform_expensive_check = True
                check_reason = "Predicted 1"
            # *** Reverted Logic Below ***
            elif proba_class_0 < check_proba_threshold: # Check if P(0) is below threshold
                 # Verify even if prediction is 0, because confidence in P=0 is low
                 perform_expensive_check = True
                 check_reason = f"Borderline P(0) < {check_proba_threshold}"
            else: # prediction == 0 and proba_class_0 >= threshold
                 print("-> Skipping objective function call (Confident P=0).")

            # 3. Perform EXPENSIVE check if needed
            if perform_expensive_check:
                print(f"-> Verifying ({check_reason})...")
                try:
                    objectives_neighbor = calculate_objective_serv_time_lookup(neighbor, **obj_func_params)
                    cost_neighbor = w * objectives_neighbor[0] + (1 - w) * objectives_neighbor[1]
                    is_truly_better = cost_neighbor < cost_star
                    true_label = 1 if is_truly_better else 0
                    print(f"    True Cost={cost_neighbor:.4f} (Current Best={cost_star:.4f}) -> Actual Better={is_truly_better}")

                    # 4. Handle outcome of expensive check
                    if is_truly_better:
                        # Found an actual improvement (P=1/A=1 or P=0/A=1)
                        print(f"    Confirmed better solution. Updating x_star.")
                        x_star = list(neighbor)
                        cost_star = cost_neighbor
                        T = len(x_star) # Recalculate T
                        found_better_solution_at_level_t = True
                        t = 1 # Reset level
                        restart_count += 1
                        print(f"    Restarting search from t=1. Restart count: {restart_count}")
                        break # Exit inner loop (neighbors at level t) - First Improvement Strategy
                    else:
                        # No improvement found (P=1/A=0 or P=0/A=0 after check)
                        if prediction == 1 and not is_truly_better:
                            # Misprediction! (P=1/A=0) -> Trigger retraining
                            print(f"    Misprediction! (Predicted 1, Actual 0). Triggering retraining process.")

                            # --- Retraining Sub-routine (Remains the same) ---
                            features_for_update: List[List[int]] = []
                            labels_for_update: List[int] = []
                            found_better_during_retrain_calc = False

                            print(f"    Calculating true costs for {len(neighbors_at_level_t)} neighbors at level {t} for retraining...")
                            for n_idx, n_schedule in enumerate(neighbors_at_level_t):
                                try:
                                    # Check length consistency before calculating objective/features
                                    if expected_feature_len is not None and len(x_star) + len(n_schedule) != expected_feature_len:
                                        print(f"    Warn: Skipping neighbor {n_idx+1} during retraining due to length mismatch.")
                                        continue

                                    n_objectives = calculate_objective_serv_time_lookup(n_schedule, **obj_func_params)
                                    n_cost = w * n_objectives[0] + (1 - w) * n_objectives[1]
                                    n_is_better = n_cost < cost_star
                                    n_true_label = 1 if n_is_better else 0

                                    n_feature_pair = x_star + n_schedule
                                    features_for_update.append(n_feature_pair)
                                    labels_for_update.append(n_true_label)
                                    print(f"      Neighbor {n_idx+1}: Cost={n_cost:.4f}, True Label={n_true_label}")

                                    # OPPORTUNISTIC UPDATE check:
                                    if n_is_better:
                                        print(f"      Opportunistic Update! Found better neighbor ({n_idx+1}) during cost calculation.")
                                        x_star = list(n_schedule)
                                        cost_star = n_cost
                                        T = len(x_star) # Recalculate T
                                        found_better_during_retrain_calc = True
                                except Exception as e:
                                    print(f"      Warn: Error calculating cost for neighbor {n_idx+1} ({n_schedule}) during retraining: {e}. Skipping for training.")
                                    continue

                            # Perform incremental fit
                            if features_for_update:
                                print(f"    Fitting model incrementally with {len(labels_for_update)} data points...")
                                try:
                                    X_update = np.array(features_for_update)
                                    y_update = np.array(labels_for_update)
                                    clf.fit(X_update, y_update, xgb_model=clf)
                                    print("    Model update complete.")
                                except Exception as e:
                                    print(f"    Error during incremental model update: {e}")
                            else:
                                print("    No valid data gathered for retraining.")

                            # Handle opportunistic update
                            if found_better_during_retrain_calc:
                                 found_better_solution_at_level_t = True
                                 t = 1
                                 restart_count += 1
                                 print(f"    Restarting search from t=1 due to opportunistic update. Restart count: {restart_count}")
                                 break # Exit inner loop
                            # --- End Retraining Sub-routine ---
                        # else: (P=0 check showed actual 0)
                        #    print("    Borderline check confirmed no improvement.")
                        #    pass # Just continue to next neighbor

                except Exception as e:
                    print(f"  Warn: Error calculating objective for neighbor {neighbors_processed_count} ({neighbor}) during verification: {e}.")

        # --- End of neighbor loop for level t ---

        # Increment Level if No Better Solution was Found and Loop Didn't Break
        if not found_better_solution_at_level_t:
            if neighbors_processed_count > 0:
                 print(f"No improving solution found or confirmed at level t={t}.")
            else:
                 print(f"No neighbors generated or processed at level t={t}.")
            t += 1 # Move to the next neighborhood size level

    # --- End of outer while loop ---
    print(f"\nLocal search finished after {restart_count} restarts or reaching max size {size}.")
    print(f"Final solution: {x_star}")
    print(f"Final cost: {cost_star:.4f}")

    return x_star, clf
```

```{python}
from functions import calculate_objective_serv_time_lookup

# Define the path to the saved model
model_path = "models/classifier_large_instance.json" # Make sure this path is correct

with open("best_trial_params.json", "r") as f:
    best_trial_params = json.load(f)

clf = xgb.XGBClassifier(
    tree_method="hist",
    max_depth=best_trial_params["max_depth"],
    min_child_weight=best_trial_params["min_child_weight"],
    gamma=best_trial_params["gamma"],
    subsample=best_trial_params["subsample"],
    colsample_bytree=best_trial_params["colsample_bytree"],
    learning_rate=best_trial_params["learning_rate"],
    n_estimators=best_trial_params["n_estimators"],
)

# Load the model directly from the file path
clf.load_model(model_path)

intial_objectives = calculate_objective_serv_time_lookup(x, d, convolutions)
initial_c_star = w * intial_objectives[0] + (1 - w) * intial_objectives[1]
x_star = local_search_predict_update(x, w, v_star, clf, {'d': d, 'convolutions': convolutions}, size=T, restarts=T)[0]
final_objectives = calculate_objective_serv_time_lookup(x_star, d, convolutions)
final_c_star = w * final_objectives[0] + (1 - w) * final_objectives[1]
print(f"\nInitial schedule: {x}, with objective value: {initial_c_star}.\nFinal schedule: {x_star}, with objective value: {final_c_star}.")
```

### Run the local search algorithm

```{python}
from functions import local_search
# Computing optimal solution with real cost
print(f"Initial schedule: {x}")
test_x = local_search(x_star, d, convolutions, w, v_star, T, echo=True)
```

```{python}
print(f"Initial schedule: {x}\nFinal schedule: {test_x[0]}\nDifference: {test_x[0] - x}\nObjective value: {test_x[1]}")
test_res = calculate_objective_serv_time_lookup(test_x[0], d, convolutions)
```

## Discussion

Analyze your results in this section. Discuss whether your hypothesis was supported, what the results mean, and the implications for future work. Address any anomalies or unexpected findings, and consider the broader impact of your results.

## Timeline

Document the duration and key dates of the experiment. This helps in project management and reproducibility.

## References

Cite all sources that informed your experiment, including research papers, datasets, and tools. This section ensures that your work is properly grounded in existing research and that others can trace the origins of your methods and data.s
